\documentclass[final,twoside,spanish,a4paper,12pt]{book}
\usepackage{cclicenses}
\exhyphenpenalty=10000

\usepackage{cmll}
\usepackage{fancyhdr}
\usepackage{amssymb,amsmath,amsthm}
\usepackage[top=1in, bottom=1.5in, left=1in, right=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{needspace}
\usepackage{tikz}
\usetikzlibrary{cd,positioning,decorations.text}
\tikzcdset{scale cd/.style={every label/.append style={scale=#1},cells={nodes={scale=#1}}}}
\usepackage[spanish,es-nodecimaldot]{babel}
\usepackage{multicol}
\usepackage{url}
\usepackage{lastpage}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{nicefrac}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{cancel}
\usepackage[llbracket,rrbracket]{stmaryrd}
\usepackage{pifont}
\renewcommand\thefootnote{\ding{\numexpr171+\value{footnote}}}
\usepackage{soul,color}
\usepackage{proof}
\usepackage[square]{natbib}
\usepackage[colorlinks,
  urlcolor=blue,
  linkcolor=blue,
  citecolor=blue,
  filecolor=black,
bookmarks]
{hyperref}
\usepackage{adjustbox,varwidth,xparse}

\NewDocumentEnvironment{bracedrows}{m}
  {\begin{adjustbox}{valign=t}%
   $\kern-\nulldelimiterspace\left.
   \begin{tabular}{@{}l@{}}}
  {\end{tabular}\right\rbrace
   \begin{varwidth}{.5\linewidth}#1\end{varwidth}$%
   \end{adjustbox}}

\usepackage{qcircuit}

\allowdisplaybreaks

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3} 

\parindent 0pt


\newcommand{\boxedheader}[1]{%
  \tikz[baseline=(X.base)]{
    \node[
      fill=gray!20,
      draw=black,
      rounded corners=3pt,
      inner xsep=4pt,
      inner ysep=2pt
    ] (X) {#1};
  }%
}

\newtheoremstyle{ejercicioBox}% nombre del estilo
  {}{}% espacio arriba/abajo
  {}% fuente del cuerpo
  {}% indentación
  {\bfseries}% fuente del encabezado
  {}% puntuación después del encabezado
  { }% espacio después del encabezado
  {\boxedheader{\thmname{#1}\thmnumber{ #2}}\thmnote{ (#3)}}%

\newtheoremstyle{defStyle}{}{}{}{}{\color{blue!50!black}\bfseries}{}{ }{}
\theoremstyle{defStyle}
\newtheorem{definicion}{Definición}[chapter]
\newtheoremstyle{thmStyle}{}{}{}{}{\color{red!50!black}\bfseries}{}{ }{}
\theoremstyle{thmStyle}
\newtheorem{teorema}[definicion]{Teorema}
\newtheorem{lema}[definicion]{Lema}
\newtheorem{corolario}[definicion]{Corolario}
\newtheorem*{propiedades}{Propiedades}
\newtheorem*{propiedad}{Propiedad}
\declaretheorem[style=definition]{postulado}
\addtotheorempreheadhook[postulado]{%
  \moveright\dimexpr(\linewidth-35em)/2\vbox\bgroup
  \hsize=35em
  \linewidth=\hsize
}
\addtotheorempostfoothook[postulado]{\egroup}
\newenvironment{postuladobis}[1]
{\renewcommand{\thepostulado}{\ref{#1}$'$}%
  \addtocounter{postulado}{-1}%
  \begin{postulado}}
{\end{postulado}}
\newenvironment{postuladoAlt}[1]
{\renewcommand{\thepostulado}{\ref{#1}}%
  \addtocounter{postulado}{-4}%
  \begin{postulado}}
{\end{postulado}}
\theoremstyle{remark}
\newtheorem*{observacion}{Observación}
\newtheorem*{observaciones}{Observaciones}
\newtheoremstyle{ejemploStyle}{}{}{}{}{\color{green!50!black}\bfseries}{}{ }{}
\theoremstyle{ejemploStyle}
\newtheorem{ejemplo}[definicion]{Ejemplo}
\theoremstyle{ejercicioBox}
\newtheorem{ejercicio}{Ejercicio}[part]

\pagestyle{fancy}
\fancyhf{}
\fancyhead[RO,LE]{\thepage}
\fancyhead[LO]{\rmfamily\sc\nouppercase\leftmark}
\fancyhead[RE]{\rmfamily\sc\nouppercase\rightmark}
\setlength{\headheight}{27.16pt}


\newcommand\concat{\ensuremath{\mathop{+\!\!+}}}
\newcommand\Obj{\ensuremath{\mathbf{Obj}}}
\newcommand\Arr{\ensuremath{\mathbf{Arr}}}
\newcommand\Id{\ensuremath{\mathsf{Id}}}
\newcommand\Home[3][\mathsf{Set}]{\mathsf{Hom}_{#1}({#2},{#3})} 
\newcommand\home[2]{[{#1},{#2}]} 
\newcommand\coprodu[2]{\left[{#1},{#2}\right]} 
\newcommand\xlra[1]{\overset{#1}{\longrightarrow}}
\newcommand\inl{\mathsf{inl}}
\newcommand\inr{\mathsf{inr}}
\newcommand\elimtop[2]{\ensuremath{{#1};{#2}}}
\newcommand\elimbot[1]{\ensuremath{\mathsf{err}(#1)}}
\newcommand\elimandl[1]{\ensuremath{\pi_1{#1}}}
\newcommand\elimandr[1]{\ensuremath{\pi_2{#1}}}
\newcommand\elimor[5]{\ensuremath{\mathsf{match}(#1,#2.#3,#4.#5)}}
\newcommand\B{\mathbb B}
\newcommand\Q{\B}
\newcommand\tbasis{\ensuremath{\mathfrak B}}
\newcommand\types{\ensuremath{\mathcal T}}
\newcommand\qtypes{\ensuremath{\mathcal Q}}
\newcommand\values{\ensuremath{\mathcal V}}
\newcommand\One{\bm{1}}
\newcommand\Zero{\bm{0}}
\newcommand\SN{\ensuremath{\mathcal{SN}}}
\newcommand\Neu{\ensuremath{\mathcal N}}
\newcommand\Red[1]{\ensuremath{\mathsf{Red}(#1)}}
\newcommand\CR[1]{\ensuremath{(\mathsf{CR}_{#1})}}
\newcommand\RC{\ensuremath{\mathcal{RC}}}
\newcommand\conlista{\leavevmode\vspace{-0.4\baselineskip}}
\newcommand\prodi[2]{\langle #1, #2 \rangle}
\newcommand\norma[1]{\Vert #1 \Vert}
\newcommand\braket[2]{\langle{#1}|{#2}\rangle}
\newcommand\matriz[1]{\begin{pmatrix}#1\end{pmatrix}}
\newcommand\conj[1]{#1^*}
\newcommand\adj[1]{#1^\dagger}
\newcommand\trans[1]{#1^T}
\newcommand\inv[1]{#1^{-1}}
\newcommand\ket[1]{|#1\rangle}
\newcommand\bra[1]{\langle #1|}
\newcommand\tr{\ensuremath{\mathsf{tr}}}
\newcommand\ifz[3]{\ensuremath{\mathsf{isZ}({#1})?{#2}\!:\!{#3}}}
\newcommand\ite[3]{\ifz{#1}{#2}{#3}}
\newcommand\fun[2]{\ensuremath{\lambda #1.#2}}
\newcommand\fix[2]{\ensuremath{ \mu #1.#2}}
\newcommand\letl[3]{\ensuremath{\mathsf{let~}#1=#2\mathsf{~in~}#3}}
\newcommand\nat{\ensuremath{\mathsf{nat}}}
\newcommand\fst{\ensuremath{\pi_1}}
\newcommand\snd{\ensuremath{\pi_2}}
\newcommand\FV{\ensuremath{\mathsf{FV}}}
\newcommand\mgu{\ensuremath{\mathsf{mgu}}}
\newcommand\Gen{\ensuremath{\mathsf{Gen}}}
\newcommand\s[1]{\ensuremath{\mathsf{#1}}}
\newcommand\thunk[2]{\ensuremath{\langle{#1},{#2}\rangle}}
\newcommand\cierre[3]{\ensuremath{\thunk{{#1},{#2}}{#3}}}
\newcommand\hra{\ensuremath{\hookrightarrow}}
\newcommand\sem[1]{\ensuremath{\left\llbracket {#1}\right\rrbracket}}
\newcommand\semT[2][\theta]{\ensuremath{\sem{#2}_{#1}}}
\newcommand\promote[3]{\ensuremath{\mathsf{promote}\ #1\ \mathsf{for}\ #2\ \mathsf{in}\ #3}}
\newcommand\derelict[1]{\ensuremath{\mathsf{derelict}\ #1}}
\newcommand\discard[2]{\ensuremath{\mathsf{discard}\ #1\ \mathsf{in}\ #2}}
\newcommand\copyt[3]{\ensuremath{\mathsf{copy}\ #1\ \mathsf{as}\ #2\ \mathsf{in}\ #3}}
\newcommand\prog[2]{[{#1},{#2}]}
\newcommand\newq{\ensuremath{\mathsf{new}}}
\newcommand\meas{\ensuremath{\mathsf{meas}}}
\newcommand\bit{\ensuremath{\mathsf{bit}}}
\newcommand\qbit{\ensuremath{\mathsf{qbit}}}
\newcommand\te[1]{\ensuremath{\mathit{#1}}}
\newcommand\un[1]{\ensuremath{\mathcal{#1}}}
\newcommand\gen[1]{\ensuremath{\mathbb{#1}}}
\newcommand\z[1][A]{\vec 0_{S(#1)}}
\newcommand\pair[2]{\langle{#1},{#2}\rangle}
\newcommand\lra{\longrightarrow}
\newcommand\may[1][\alpha]{[{#1}.]}
\newcommand\titulo[3][\scriptsize]{\rotatebox[origin=c]{90}{\parbox[t]{#2}{\centering #1{#3}}}}
\newcommand\tax{\textsl{Ax}}
\newcommand\tif{\textsl{If}}
\newcommand\rbetab{(\s{\beta_b})}
\newcommand\rbetan{(\s{\beta_n})}
\newcommand\riftrue{(\s{if_{1}})}
\newcommand\riffalse{(\s{if_{0}})}
\newcommand\rlinr{(\s{lin^+_r})}
\newcommand\rlinscalr{(\s{lin^\alpha_r})}
\newcommand\rlinzr{(\s{lin^0_r})}
\newcommand\rlinl{(\s{lin^+_l})}
\newcommand\rlinscall{(\s{lin^\alpha_l})}
\newcommand\rlinzl{(\s{lin^0_l})}
\newcommand\rneut{(\s{neutral})}
\newcommand\runit{(\s{unit})}
\newcommand\rzeros{(\s{zero_\alpha})}
\newcommand\rzeroS{(\s{zero_S})}
\newcommand\rzero{(\s{zero})}
\newcommand\rprod{(\s{prod})}
\newcommand\rdists{(\s{\alpha dist})}
\newcommand\rdistcasum{(\s{dist^+_\Uparrow})}
\newcommand\rdistcascal{(\s{dist^\alpha_\Uparrow})}
\newcommand\rcaneutl{(\s{neut^\Uparrow_l})}
\newcommand\rcaneutr{(\s{neut^\Uparrow_r})}
\newcommand\rfact{(\s{fact})}
\newcommand\rfacto{(\s{fact^1})}
\newcommand\rfactt{(\s{fact^2})}
\newcommand\rproj{(\s{proj})}
\newcommand\rehead{(\s{head})}
\newcommand\rtail{(\s{tail})}
\newcommand\rdistzr{(\s{dist^0_r})}
\newcommand\rdistzl{(\s{dist^0_l})}
\newcommand\rdistscalr{(\s{dist^\alpha_r})}
\newcommand\rdistscall{(\s{dist^\alpha_l})}
\newcommand\rdistsumr{(\s{dist^+_r})}
\newcommand\rdistsuml{(\s{dist^+_l})}
\newcommand\rcomm{(\s{comm})}
\newcommand\rassoc{(\s{assoc})}
\newcommand\cas[2]{\Uparrow^{\scriptscriptstyle S({#2})}_{\scriptscriptstyle S({#1})}}
\newcommand\head{\text{\sl head}}
\newcommand\tail{\text{\sl tail}}
\newcommand\red[2][1]{\overset{\scriptscriptstyle\smash{#2}\vphantom{x}}{\lra[#1]}\ }
\newcommand\npair[2]{({#1}-{#2})}

\title{
  {\large Materia optativa}\\
  {\bf Fundamentos de lenguajes\\
  para computación cuántica}\\[1ex]
  \parbox{\textwidth}
  {\centering\large Licenciatura en Ciencias de la Computación\\
    Facultad de Ciencias Exactas y Naturales\\
  Universidad de Buenos Aires}
  \\[1em]
  {\sc\Large Apunte de clase} 
}
\author{Alejandro Díaz-Caro\\ 
  {\small Inria / LORIA (Francia)}
  \\
  {\small \& Universidad Nacional de Quilmes (Argentina)} 
}
\date{Versión (incompleta) del \today}

\begin{document}
\shorthandoff{"}
\maketitle

\pagestyle{empty} \ \vfill
\begin{center}
  \parbox{0.62\textwidth}{
    \begin{center}
      \textbf{Enfoque de este apunte}
    \end{center}

    Estas notas están basadas en cursos que dí en diferentes lugares: Escuela
    de Ciencias Informáticas (UBA), Congreso Argentino de Ciencias de la
    Computación, Escuela de Verano de Río Cuarto, y materias optativas de
    cuántica en la Licenciatura en Ciencias de la Computación (LCC) de la UNR y
    la Licenciatura en Informática (LI) de la UNQ, así como materias
    obligatorias para la LI de la UNQ (Características de Lenguajes de
    Programacion, Lógica y Programación). Están pensadas para estudiantes de
    grado y posgrado de computación, no de física. Es por ello que el enfoque
    que se da es casi puramente matemático, con algún comentario aquí y allá de
    la física que motiva el formalismo, pero todos los razonamientos se
    realizan exclusivamente desde el lado de la matemática. De todas maneras,
    {\color{red}el curso contendrá cosas no incluidas en estos apuntes, y no profundizará
    en otros temas que sí están tratados más profundamente aquí.} Se recomienda
    recurrir a la bibliografía sugerida.
  }
\end{center}
\vfill


Curso:\\
Fundamentos de lenguajes para computación cuántica\\
\\
Alejandro Díaz-Caro\\
\\
\cc 2025 Creative Commons Attibution 4.0 Internacional.\\
Podés ver una copia de la licencia en
\url{http://creativecommons.org/licenses/by/4.0/}.


\newpage

\tableofcontents

\setcounter{chapter}{-1}
\chapter{Organización de la materia}
\section*{Cronograma}
Esta materia se dictará en las siguientes fechas, de 14 a 18h, con el siguiente cronograma.
\renewcommand{\arraystretch}{1.5}

\begin{tabular}{l|c|p{.66\linewidth}}
  Fecha & Parte & Temas \\
  \hline\hline
  17 de noviembre & \ref{part:LC} & Cálculo lambda tipado, isomorfismo de Curry-Howard\\\hline
  18 de noviembre &  \ref{part:SemDen} & Semántica denotacional y teoría de categorías\\\hline
  19 de noviembre & \ref{part:CC} & Computación cuántica y algoritmos cuánticos\\\hline
  20 de noviembre & Papers & Cálculo lambda cuántico con control clásico\\\hline
  25 de noviembre &  Papers & Cálculo lambda cuántico con control cuántico\\\hline\hline
\end{tabular}
\renewcommand{\arraystretch}{1}

\section*{Bibliografía de referencia}
Las primeras tres clases se encuentran resumidas en este apunte. Sin embargo, si se quiere profundizar en alguno de los temas, se puede recurrir a la bibliografía de referencia que se sugiere en esta sección.
\subsubsection*{Parte \ref{part:LC}}
\begin{itemize}
  \item Gilles Dowek y Jean-Jacques Lévy. ``Introduction to the theory of programming languages''. Springer. 2011.
  \item Morten H. B. Sørensen y Paweł Urzyczyn. ``Lectures on the Curry-Howard isomorphism''. Elsevier. 2006.
  \item Jean-Yves Girard, Paul Taylor e Yves Lafont. ``Proof and types''. Cambridge University Press. 1989.
  \item Henk Barendregt, Wil Dekkers y Richard Statman. ``Lambda calculi with types''. Cambridge University Press. 2013.
  \item Mauricio Ayala-Rincón y Flávio L.~C.~de Moura. ``Applied logic for computer scientists: Computational deduction and formal proofs''. Springer. 2016.
  \item Frank Pfenning. ``Linear logic''. Apuntes de curso en la Carnegie Mellon University. 2002.
\end{itemize}
\subsubsection*{Parte~\ref{part:SemDen}}
\begin{itemize}
  \item Benjamin C. Pierce. ``Basic Category Theory for Computer Scientists''. MIT Press. 1991.
  \item Roy L.~Crole. ``Categories for types''. Cambridge University Press. 1993.
  \item Joachim Lambek y Philip J. Scott ``Introduction to higher order categorical logic''. Cambridge University Press. 1988.
\end{itemize}

\subsubsection*{Parte \ref{part:CC}}
\begin{itemize}
  \item Michael Nielsen e Isaac Chuang. ``Quantum computation and quantum information''. Cambridge University Press. 2a edición, 2010.
  \item Noson S.~Yanofsky y Mirco A.~Mannucci. ``Quantum computing for computer scientists''. Cabridge University Press. 2008.
\end{itemize}

\subsubsection*{Dos últimas clases}
\paragraph{Libros}
\begin{itemize}
  \item Bob Coecke y Aleks Kissinger. ``Picturing quantum processes: A first course in quantum theory and diagrammatic reasoning''. Cambridge University Press. 2017.
  \item  Simon Gay and Ian Mackie. ``Semantic Techniques in Quantum Computation''. Cambridge University Press. 2009.
\end{itemize}
\paragraph{Cálculo lambda cuántico con control clásico}
\begin{itemize}
  \item Peter Selinger. Towards a quantum programming language. Mathematical Structures in Computer Science 14(4):527-586, 2004.
  \item Peter Selinger y Benoît Valiron. A lambda calculus for quantum computation with classical control. Mathematical Structures in Computer Science 16(3):527--552, 2006.
\end{itemize}
\paragraph{Cálculo lambda cuántico con control cuántico}
\begin{itemize}
  \item Alejandro Díaz-Caro. A quick overview on the quantum control approach to the lambda calculus. Logical and Semantic Frameworks with Applications (LSFA'21). EPTCS, 357:1--17, 2021.
  \item Alejandro Díaz-Caro. Towards a Computational Quantum Logic: An Overview of an Ongoing Research Program. (CiE 2025, invited paper) - LNCS 15764:34--46, 2025.
\end{itemize}
\subparagraph{Cálculo lambda cuántico con control cuántico: Lambda-S}
\begin{itemize}
  \item Alejandro Díaz-Caro, Gilles Dowek y Juan Pablo Rinaldi. Two linearities for quantum computing in the lambda calculus. Biosystems 186:104012, 2019.
  \item Alejandro Díaz-Caro, Mauricio Guillermo, Alexandre Miquel y Benoît Valiron. Realizability in the unitary sphere. Proceedings of the 34th Annual ACM/IEEE Symposium on Logic in Computer Science (LICS 2019), 1--13, 2019.
  \item Alejandro Díaz-Caro y Octavio Malherbe. A categorical construction for the computational definition of vector spaces. Applied Categorical Structures 28(5):807--844, 2020.
  \item Alejandro Díaz-Caro y Octavio Malherbe. Quantum control in the unitary sphere: Lambda-S1 and its categorical model. Logical Methods in Computer Science 18(3:32), 2022.
  \item Alejandro Díaz-Caro y Octavio Malherbe. A concrete model for a typed linear algebraic lambda calculus. Mathematical Structures in Computer Science 34(1):1--44, 2023.
  \item Alejandro Díaz-Caro y Nicolás. A. Monzon. A Quantum-Control Lambda-Calculus with Multiple Measurement Bases. (APLAS 2025) - LNCS 16201:151-170, 2025.
  \item Alejandro Díaz-Caro, Octavio Malherbe, y Rafael Romero. Basis-Sensitive Quantum Typing via Realisability. arXiv:2510.18542, 2025.
\end{itemize}
\subparagraph{Cálculo lambda cuántico con control cuántico: $\mathcal L^{\mathcal S}$}
\begin{itemize}
  \item Alejandro Díaz-Caro y Gilles Dowek. A new connective in natural deduction, and its application to quantum computing. Theoretical Computer Science 957:113840, 2023. 
  \item Alejandro Díaz-Caro y Gilles Dowek. A linear linear lambda-calculus. Mathematical Structures in Computer Science, 34(10):1103-1137, 2024.
  \item Alejandro Díaz-Caro, Malena Ivnisky, y Octavio Malherbe. An Algebraic Extension of Intuitionistic Linear Logic: The $\mathcal L^{\mathcal S}_!$-Calculus and Its Categorical Model. Journal of Logic and Computation 34(8):exaf053, 2025.
  \item Kinnari Dave, Alejandro Díaz-Caro, y Vladimir Zamdzhiev. IMALL with a Mixed-State Modality: A Logical Approach to Quantum Computation. (APLAS 2025) - LNCS 16201:131-150, 2025.
  \item Alejandro Díaz-Caro y Octavio Malherbe. The sup connective in IMALL: A categorical semantics. arXiv:2205.02142, 2025.
\end{itemize}

\part{Lógica, cálculo lambda, y el isomorfismo de Curry-Howard}\label{part:LC}
\chapter{Lógica Proposicional Intuicionista en Deducción Natural}
\section{Gramática y pruebas}
Vamos a considerar proposiciones cuyo valor de verdad no depende de cómo se intepretan. Es decir, sólo consideramos tautologías de la lógica proposicional (aquellas proposiciones cuyas tablas de verdad tienen $T$ en todas las filas).

Con Deducción Natural daremos una caracterización sintáctica, es decir, un conjunto de fórmulas que se puedan probar en un sistema deductivo.

\begin{definicion}
  [Reglas de prueba]
  \label{def:proofrule}
  Una regla de prueba (\textit{proof rule}), es una regla que permite deducir una proposición (conclusión) a partir de otras (premisas). Si $A_1,A_2,\dots,A_n$ son las premisas y $B$ es la conclusión, la regla de prueba ``$r$'' se escribe
  \[
    \infer[r]{B}{A_1 & A_2 & \dots & A_n}
  \]
\end{definicion}
\begin{definicion}
  [Prueba]
  Una prueba de una proposicion lógica en deducción natural se construye aplicando sucesivamente reglas de prueba.
\end{definicion}
\begin{ejemplo}\label{ej:derivacion}
  A partir de asumir $A$, $B$ y $C$ como verdaderas, podemos derivar $(A\wedge B)\wedge C$ como sigue, utilizando las reglas $\textrm{Hip}$ y $\wedge_i$.
  \[
    \infer[\wedge_i]{(A\wedge B)\wedge C}{
      \infer[\wedge_i]{A\wedge B}
      {
	\infer[\textrm{Hip}]{A}{}
	&
	\infer[\textrm{Hip}]{B}{}
      }
      &
      \infer[\textrm{Hip}]{C}{}
    }
  \]
\end{ejemplo}

\begin{definicion}
  [Secuente]
  La notación $A_1,\dots,A_n\vdash B$ denota que a partir del conjunto de proposiciones $\{A_1,\dots,A_n\}$ podemos obtener una prueba de $B$.

  Un secuente es válido si podemos construir una prueba.
\end{definicion}

\begin{ejemplo}
    \label{ej:hip} A partir de la regla de hipótesis (Hip), vemos que el secuente $A\vdash A$ es válido para cualquier proposición $A$.
\end{ejemplo}
\begin{ejemplo}
    Continuando con el Ejemplo~\ref{ej:derivacion}, el secuente $A,B,C\vdash (A\wedge B)\wedge C$ es un secuente válido.
\end{ejemplo}
\begin{ejemplo}
    \label{ej:neg} $(A\wedge B)\Rightarrow C,A,\neg C\vdash\neg B$, es válido (¡pero aún no dimos las reglas de prueba para mostrarlo!).
    \item\label{ej:seqInvalido} $A,B\vdash A\wedge\neg B$, es un secuente inválido.
\end{ejemplo}

Las reglas de prueba deben permitir solo proposiciones válidas, impidiendo probar secuentes como el del Ejemplo~\ref{ej:seqInvalido}.

A partir del Ejemplo~\ref{ej:hip}, podemos cambiar las reglas de pruebas para que en lugar de tener proposiciones como premisas y conclusión, tengan secuentes.
\begin{ejemplo}
  El Ejemplo~\ref{ej:derivacion} puede ser reescrito con reglas de prueba de secuentes como sigue.
  \begin{equation}
    \label{der:seq}
    \infer[\wedge_i]{A,B,C\vdash (A\wedge B)\wedge C}{
      \infer[\wedge_i]{A,B\vdash A\wedge B}
      {
	\infer[\textrm{Hip}]{A\vdash A}{}
	&
	\infer[\textrm{Hip}]{B\vdash B}{}
      }
      &
      \infer[\textrm{Hip}]{C\vdash C}{}
    }
  \end{equation}
  Más generalmente, podemos cambiar la regla Hip por una regla que nos permita probar $A_1,\dots,A_n\vdash A_i$. En efecto, si asumimos que $A_1,\dots,A_n$ son válidas, es posible derivar $A_i$ simplemente con
  \[
    \infer[\textrm{Hip}]{A_i}{}
  \]
  A esta nueva regla que permite derivar $A_1,\dots,A_n\vdash A_i$ la llamamos ``ax'', por axioma:
  \[
    \infer[\textrm{ax}]{A_1,\dots,A_n\vdash A_i}{}
  \]
  Así, la derivación~\eqref{der:seq}, podemos generalizarla como
  \[
    \infer[\wedge_i]{A,B,C\vdash (A\wedge B)\wedge C}{
      \infer[\wedge_i]{A,B,C\vdash A\wedge B}
      {
	\infer[\textrm{ax}]{A,B,C\vdash A}{}
	&
	\infer[\textrm{ax}]{A,B,C\vdash B}{}
      }
      &
      \infer[\textrm{ax}]{A,B,C\vdash C}{}
    }
  \]
  con la ventaja de que cada secuente en la regla carga con el conjunto completo de hipótesis.
\end{ejemplo}

La regla que introduce la implicación es la siguiente: Si a partir de la hipótesis $A$ se puede derivar $B$, entonces $A\Rightarrow B$. 
\[
  \infer[\Rightarrow_i]{A\Rightarrow B}
  {
    \infer*{B}{\infer[\textrm{Hip}]{A}{}}
  }
\]
Aquí también se ve la ventaja de cargar con las hipótesis en cada premisa y conclusión, ya que esta regla, con secuentes, se puede notar como sigue
\[
  \infer[\Rightarrow_i]{\Gamma\vdash A\Rightarrow B}{\Gamma,A\vdash B}
\]
Donde $\Gamma$ es un conjunto de hipótesis extra.

\begin{definicion}
  [Gramática de la lógica proposicional]\label{def:gramLP}
El lenguaje de la lógica proposicional que consideramos en este curso es el que se obtiene a partir de la siguiente gramática:
\[
  A ::= \top\mid\bot\mid A\Rightarrow A\mid A\wedge A\mid A\vee A
\]
Donde $\top$ denota ``Verdadero'' y $\bot$ denota ``Falso''.
\end{definicion}

\begin{observacion}
  Si bien no consideramos $\neg A$ directamente en la gramática, es fácil ver que $\neg A$ es equivalente a $A\Rightarrow\bot$.
\end{observacion}


\begin{definicion}[Reglas de prueba]\label{def:reglas-de-prueba}
  Las reglas de prueba de la lógica proposicional intuicionista son las siguientes.
  Notar que para cada constructor de la gramática hay reglas de introducción ${}_i$ y de eliminación ${}_e$, excepto para $\bot$, donde sólo hay una regla de eliminación.
  \[
    \infer[\textrm{ax}]{\Gamma,A\vdash A}{}
    \qquad
    \infer[\top_i]{\Gamma\vdash\top}{}
    \qquad
    \infer[\top_e]{\Gamma\vdash A}{\Gamma\vdash\top & \Gamma\vdash A}
    \qquad
    \infer[\bot_e]{\Gamma\vdash C}{\Gamma\vdash\bot}
  \]
  \[
    \infer[\Rightarrow_i]{\Gamma\vdash A\Rightarrow B}{\Gamma,A\vdash B}
    \qquad
    \infer[\Rightarrow_e]{\Gamma\vdash B}{\Gamma\vdash A\Rightarrow B & \Gamma\vdash A}
  \]
  \[
    \infer[\wedge_i]{\Gamma\vdash A\wedge B}{\Gamma\vdash A & \Gamma\vdash B}
    \qquad
    \infer[\wedge_{e_1}]{\Gamma\vdash A}{\Gamma\vdash A\wedge B}
    \qquad
    \infer[\wedge_{e_2}]{\Gamma\vdash B}{\Gamma\vdash A\wedge B}
  \]
  \[
    \infer[\vee_{i_1}]{\Gamma\vdash A\vee B}{\Gamma\vdash A}
    \qquad
    \infer[\vee_{i_2}]{\Gamma\vdash A\vee B}{\Gamma\vdash B}
    \qquad
    \infer[\vee_e]{\Gamma\vdash C}{\Gamma\vdash A\vee B & \Gamma,A\vdash C & \Gamma,B\vdash C}
  \]


\end{definicion}

Descripción de las reglas:
\begin{itemize}
  \item La regla ax es la regla ya introducida de la hipótesis. Si en mi
    conjunto de hipótesis asumo que una proposición $A$ es verdadera, entonces
    puedo derivar que $A$ es verdadera.
  \item La regla $\top_i$ nos dice que bajo cualquier conjunto de hipótesis,
    $\top$ es verdadero.
  \item La regla $\top_e$ nos dice básicamente que podemos descartar la prueba $\Gamma\vdash\top$, o, dicho de otra manera, que en esa prueba no hay información relevante.
  \item La regla $\bot_e$ nos dice que si a partir de un conjunto de hipótesis
    podemos derivar que $\bot$ es verdadero, entonces podemos derivar cualquier
    cosa, ya que $\bot$ representa al falso, y esto constituiría una
    contradicción.
  \item La regla $\Rightarrow_i$ nos dice que si asumiendo $A$ como hipótesis
    podemos derivar $B$, entonces quitando esa hipótesis podemos derivar que
    $A$ implica $B$.
  \item La regla $\Rightarrow_e$ es el \textit{modus ponens}: Si a partir de un
    conjunto de hipótesis podemos derivar $A\Rightarrow B$ y también $A$,
    entonces podemos derivar $B$.
  \item La regla $\wedge_i$ nos dice que si a partir de un conjunto de hipótesis podemos derivar $A$ y también $B$, entonces podemos derivar $A\wedge B$.
  \item Las reglas $\wedge_{e_1}$ y $\wedge_{e_2}$ nos dicen que a si a partir de un conjunto de hipótesis podemos derivar $A\wedge B$, entonces podemos derivar tanto $A$ como $B$.
  \item La regla $\vee_{i_1}$ nos dice que si a partir de un conjunto de hipótesis podemos derivar $A$, entonces podemos derivar $A\vee B$. La regla $\vee_{i_2}$ nos dice lo mismo si podemos derivar $B$ en lugar de $A$.
  \item La regla $\vee_e$ nos dice que si a partir de un conjunto de hipótesis podemos derivar $A\vee B$, y además agregando la hipótesis $A$ o la hipótesis $B$, podemos derivar $C$, entonces podemos derivar $C$.
\end{itemize}

\begin{ejemplo}
  El secuente $\vdash A\Rightarrow (B\Rightarrow A)$ es válido. Es decir, la proposición $A\Rightarrow (B\Rightarrow A)$ es siempre válida sin asumir ninguna hipótesis para $A$, $B$ y $C$.
  La prueba es la siguiente.
  \[
    \infer[\Rightarrow_i]{\vdash A\Rightarrow(B\Rightarrow A)}
    {
      \infer[\Rightarrow_i]{A\vdash B\Rightarrow A}
      {
	\infer[\textrm{ax}]{A,B\vdash A}{}
      }
    }
  \]
\end{ejemplo}

\begin{ejemplo}
  En el Ejemlo~\ref{ej:neg} dijimos que el secuente
  $(A\wedge B)\Rightarrow C,A,\neg C\vdash\neg B$, es válido. Ahora podemos probarlo.

  Primero, debemos reescribir la negación con la implicación a botom. Por lo tanto, el secuente a probar es
  $(A\wedge B)\Rightarrow C,A,C\Rightarrow\bot\vdash B\Rightarrow\bot$.
  Una prueba es la siguiente.

  Escribimos $\Gamma$ en lugar de $(A\wedge B)\Rightarrow C,A,C\Rightarrow\bot$ por una cuestión de espacio.
  \[
    \infer[\Rightarrow_i]{\Gamma\vdash B\Rightarrow\bot}
    {
      \infer[\Rightarrow_e]{\Gamma,B\vdash\bot}
      {
	\infer[\textrm{ax}]{\Gamma,B\vdash C\Rightarrow\bot}{}
	&
	\infer[\Rightarrow_e]{\Gamma,B\vdash C}
	{
	  \infer[\textrm{ax}]{\Gamma,B\vdash (A\wedge B)\Rightarrow C}{}
	  &
	  \infer[\wedge_i]{\Gamma,B\vdash A\wedge B}
	  {
	    \infer[\textrm{ax}]{\Gamma,B\vdash A}{}
	    &
	    \infer[\textrm{ax}]{\Gamma,B\vdash B}{}
	  }
	}
      }
    }
  \]
\end{ejemplo}


\section{Reducción de pruebas: cut-elimination}
Naturalmente, existen muchas pruebas para una misma proposicion.  Por ejemplo,
una prueba de $A,B\vdash A$ puede ser derivarse simplemente a partir de la
regla de axioma, o con una derivación más compleja como
\[
  \infer[\wedge_{e_1}]{A,B\vdash A}
  {
    \infer[\wedge_i]{A,B\vdash A\wedge B}
    {
      \infer[\textrm{ax}]{A,B,\vdash A}{}
      &
      \infer[\textrm{ax}]{A,B,\vdash B}{}
    }
  }
\]
En este ejemplo se ve que al árbol de derivación más simple para obtener
$A,B\vdash A$, se le introduce una conjunción y luego se elimina la misma
conjunción.


Otro ejemplo, es una prueba de $A,B\vdash A\wedge A$ la que puede ser
simplemente la aplicación de la regla axioma, seguida de la introducción de la
conjunción,
\[
  \infer[\wedge_i]{A,B\vdash A\wedge B}
  {\infer[\textrm{ax}]{A,B\vdash A}{}
    &
  \infer[\textrm{ax}]{A,B\vdash B}{}}
\]
o algo más complejo como
\[
  \infer[\Rightarrow_e]{A,B\vdash A\wedge B}{
    \infer[\Rightarrow_i]{A,B\vdash (A\wedge B)\Rightarrow (A\wedge B)}
    {
      \infer[\textrm{ax}]{A,B,A\wedge B\vdash A\wedge B}{}
    }
    &
  \infer[\wedge_i]{A,B\vdash A\wedge B}
  {\infer[\textrm{ax}]{A,B\vdash A}{}
    &
  \infer[\textrm{ax}]{A,B\vdash B}{}}
  }
\]
Aquí también se puede ver que se introduce un conectivo, $\Rightarrow$ en este
caso, e inmediatamente después se elimina.

En general, se llama ``cut'' a la introducción seguida de la eliminación de
cualquier conectivo. El proceso de ``cut-elimination'' es el proceso de
eliminar los cuts, mediante un sistema de reescritura de derivaciones.

\begin{definicion}
  [Reglas de cut-elimination]\label{def:cut-elim}
  Vamos a nombrar las derivaciones con $\pi_1,\dots,\pi_n$, de manera que $\infer{\Gamma\vdash C}{\pi}$ es la derivación llamada $\pi$ que termina en la conclusión $\Gamma\vdash C$.

  También definiremos informalmente la substitución de derivaciones. Notamos $(\pi_2/A)\pi_1$ al proceso de substituir en la derivación de $\pi_1$ todas las utilizaciones de la proposición $A$ por la derivación $\pi_2$.

  Las reglas, entonces, son las siguientes (una regla para los cuts possibles para cada conectivo):
  \begin{align*}
    \vcenter{
      \infer[\top_e]{\Gamma\vdash A}{\infer[\top_i]{\Gamma\vdash\top}{} & 
      \infer{\Gamma\vdash A}{\pi}}
    }
    &\lra
    \pi & (\top)\\
    \vcenter{\infer[\Rightarrow_e]{\Gamma\vdash B}
      {
	\infer[\Rightarrow_i]{\Gamma\vdash A\Rightarrow B}{\infer{\Gamma,A\vdash B}{\pi_1}}
	&
	\infer{\Gamma\vdash A}{\pi_2}
      }
    }
    &\longrightarrow
    (\pi_2/A)\pi_1
    & (\Rightarrow)
    \\
    \vcenter{\infer[\wedge_{e_1}]{\Gamma\vdash A}
      {
	\infer[\wedge_i]{\Gamma\vdash A\wedge B}{\infer{\Gamma\vdash A}{\pi_1} & \infer{\Gamma\vdash B}{\pi_2}}
    }}
    &\longrightarrow
    \pi_1
    & (\wedge_1)
    \\
    \vcenter{\infer[\wedge_{e_2}]{\Gamma\vdash B}
      {
	\infer[\wedge_i]{\Gamma\vdash A\wedge B}{\infer{\Gamma\vdash A}{\pi_1} & \infer{\Gamma\vdash B}{\pi_2}}
    }}
    &\longrightarrow
    \pi_2
    & (\wedge_2)
    \\
    \vcenter{
      \infer[\vee_e]{\Gamma\vdash C}
      {
	\infer[\vee_{i_1}]{\Gamma\vdash A\vee B}{\infer{\Gamma\vdash A}{\pi_1}}
	&
	\infer{\Gamma,A\vdash C}{\pi_2}
	&
	\infer{\Gamma,B\vdash C}{\pi_3}
      }
    }
    &\longrightarrow
    (\pi_1/A)\pi_2
    & (\vee_1)
    \\
    \vcenter{
      \infer[\vee_e]{\Gamma\vdash C}
      {
	\infer[\vee_{i_2}]{\Gamma\vdash A\vee B}{\infer{\Gamma\vdash B}{\pi_1}}
	&
	\infer{\Gamma,A\vdash C}{\pi_2}
	&
	\infer{\Gamma,B\vdash C}{\pi_3}
      }
    }
    &\longrightarrow
    (\pi_1/A)\pi_3
    & (\vee_2)
  \end{align*}
\end{definicion}

\begin{ejemplo}
      En la derivación
      \[
	\infer[\wedge_{e_1}]{A,B\vdash A}
	{
	  \infer[\wedge_i]{A,B\vdash A\wedge B}
	  {
	    \infer[\textrm{ax}]{A,B,\vdash A}{}
	    &
	    \infer[\textrm{ax}]{A,B,\vdash B}{}
	  }
	}
      \]
      usando la regla $(\wedge_1)$ se obtiene la derivación
      \[
	\pi_1 = \vcenter{\infer[\textrm{ax}]{A,B,\vdash A}{}}
      \]
\end{ejemplo}
\begin{ejemplo}
    En la derivación
      \[
	\infer[\Rightarrow_e]{A,B\vdash A\wedge B}{
	  \infer[\Rightarrow_i]{A,B\vdash (A\wedge B)\Rightarrow (A\wedge B)}
	  {
	    \infer[\textrm{ax}]{A,B,A\wedge B\vdash A\wedge B}{}
	  }
	  &
	  \infer[\wedge_i]{A,B\vdash A\wedge B}
	  {\infer[\textrm{ax}]{A,B\vdash A}{}
	    &
	  \infer[\textrm{ax}]{A,B\vdash B}{}}
	}
      \]
      usando la regla $(\Rightarrow)$, donde 
      \[
	\pi_1 =\vcenter{
	  \infer[\textrm{ax}]{A,B,A\wedge B\vdash A\wedge B}{}
	}
	\qquad\textrm{ y }\qquad
	\pi_2=\vcenter{
	  \infer[\wedge_i]{A,B\vdash A\wedge B}
	  {\infer[\textrm{ax}]{A,B\vdash A}{}
	    &
	  \infer[\textrm{ax}]{A,B\vdash B}{}}
	}
      \]
      se obtiene
      \[
	(\pi_2/A\wedge B)\pi_1 = \pi_2 = \vcenter{\infer[\wedge_i]{A,B\vdash A\Rightarrow B}
	  {\infer[\textrm{ax}]{A,B\vdash A}{}
	    &
	\infer[\textrm{ax}]{A,B\vdash B}{}}}
      \]
\end{ejemplo}

\chapter{Cálculo lambda (extendido) simplemente tipado}\label{sec:calculo}
\section{Gramática}
En esta materia veremos una extensión del cálculo lambda simplemente tipado. 
El cálculo lambda (que se asume ya visto en materias anteriores), se construye a partir de variables, abstracciones y aplicaciones, es decir, la siguiente gramática
\[
  t::= x\mid\lambda x.t\mid tt
\]
Aquí extenderemos el cálculo lambda con algunas construcciones, las cuales no son necesarias a priori en el cálculo lambda sin tipos, ya que se pueden codificar en el cálculo lambda, sin embargo, son prácticas para no tener que codificarlas, y son necesarias en el caso del cálculo lambda simplemente tipado.
\begin{definicion}
  [Gramática del cálculo lambda extendido]\label{def:gramatica-lambda-calculo}
  El lenguaje del cálculo lambda extendido que consideraremos en este curso es el que se obtiene a partir de la siguiente gramática:
  \begin{align*}
    t =~& x\mid \star\mid\elimtop tt\mid \elimbot t \\
    &\mid \lambda x.t\mid tt\\
    &\mid \pair{t}{t} \mid \elimandl t \mid \elimandr t\\
    &\mid \inl(t)\mid \inr(t) \mid \elimor txtyt
  \end{align*}

  Llamamos términos a las palabras obtenidas mediante esta gramática.
\end{definicion}

Descripción informal de la gramática:
\begin{itemize}
  \item A las variables las anotaremos con las letras $x,y,z$.
  \item El símbolo $\star$ denota el par vacío, el fin de la ejecución.
  \item La construcción $\elimtop tr$ denota la secuencia: primero $t$, luego $r$.
  \item La construcción $\elimbot t$ denota que el término $t$ produce un error.
  \item La construcción $\lambda x.t$ denota la función cuya variable es $x$ y cuerpo es $t$.
  \item La construcción $tr$ denota la aplicación de $t$ al argumento $r$.
  \item La construcción $\pair tr$ denota el par.
  \item La construcción $\elimandl t$ denota la proyección de la primera componente de un par $t$.
  \item La construcción $\elimandr t$ denota la proyección de la segunda componente de un par $t$.
  \item La construcción $\inl(t)$ denota que $t$ es la componente izquieda de un tipo suma.
  \item La construcción $\inr(t)$ denota que $t$ es la componente derecha de un tipo suma.
  \item La construcción $\elimor txrys$ denota el match, el cual si machea $t$ con $\inl(t')$ devuelve el resultado de aplicar $\lambda x.r$ a $t'$ y si machea $t$ con $\inr(t')$ devuelve el resultado de aplicar $\lambda y.s$ a $t'$.
\end{itemize}

\begin{ejercicio}
Determinar cuáles de los siguientes son términos bien formados según la
gramática extendida, y justificar:
\begin{enumerate}
  \item $\lambda x.\pair{x}{\inl(\star)}$
  \item $\elimtop{\star}{}$
  \item $\pair{\lambda x.x}{yz}$
  \item $\elimor t x r y{}$
\end{enumerate}
En cada caso, indicar por qué es correcto o por qué no lo es.
\end{ejercicio}

\section{Semántica operacional}\label{sec:semOp}
\subsection{Reglas de reducción}
La gramática nos dice qué términos podemos escribir sintácticamente. La
semántica operacional nos da el significado de los términos, al definir como
operan.
\begin{definicion}
  [Reglas de reducción]\label{def:reglas-de-reduccion}
  Definimos una relación entre términos $t\lra r$, llamada reducción, como la relación que satisface las siguientes reglas:
  \begin{align*}
    \elimtop{\star}t &\lra t & (\mathsf{sec})\\
    (\lambda x.t)r &\lra (r/x)t & (\beta)\\
    \elimandl{\pair tr} &\lra t & (\pi_1)\\
    \elimandr{\pair tr} &\lra r & (\pi_2)\\
    \elimor{\inl(t)}xrys &\lra (t/x)r & (\mathsf{match}_l)\\
    \elimor{\inr(t)}xrys &\lra (t/y)s & (\mathsf{match}_r)
  \end{align*}
  así como las reglas de congruencia que permitirán reducir un subtérmino de un término:
  \[
    \infer{\elimtop ts\lra\elimtop rs}{t\lra r} 
    \qquad\infer{\elimtop st\lra\elimtop sr}{t\lra r} 
    \qquad \infer{\elimbot t\lra\elimbot r}{t\lra r} 
    \qquad \infer{\lambda xt\lra\lambda xr}{t\lra r}
    \qquad \infer{ts\lra rs}{t\lra r}
    \qquad \infer{st\lra sr}{t\lra r}
  \]
  \begin{ejercicio}
    Escribir las reglas de congruencia que faltan para que se pueda reducir dentro
    de cualquier término.
  \end{ejercicio}
\end{definicion}

\begin{observacion}
  La cuarta regla de congruencia, que permite reducir dentro de la función,
  corresponde a la posibilidad de optimizar programas.
\end{observacion}

\begin{ejercicio}
  Reducir, mostrando cada paso, los siguientes términos:
  \begin{enumerate}
    \item $\elimtop{\star}{\pair{\star}{\star}}$
    \item $(\lambda x.\elimandl x)\,\pair{\star}{\star}$
    \item $\elimor{\inr(\star)}x{(\lambda y.y)}y{(\lambda y.\star)}$
  \end{enumerate}
\end{ejercicio}

\subsection{Captura de variables}
Vamos a arrancar con este ejercicio motivador:
\begin{ejercicio}
  Reducir los siguientes términos
  \begin{enumerate}
    \item $(\lambda x.{\lambda x.x})\inl(\star)\inr(\star)$
    \item $(\lambda x.(\lambda y.(\lambda x.y;x))x)\star$
    \item $
	(\lambda x.(\lambda f.(\lambda x.f\star)\inr(\star))(\lambda y.y;x))\inl(\star)
      $
  \end{enumerate}
\end{ejercicio}
Tenemos que definir precisamente qué significa $(r/x)t$. Damos una definición
inductiva:
\begin{align*}
  (r/x)x &= r\\
  (r/x)y &= y\\
  (r/x)\star &=\star\\
  (r/x)\elimtop ts &=\elimtop{(r/x)t}{(r/x)s}\\
  (r/x)\elimbot t &=\elimbot{(r/x)t}\\
  (r/x)(\lambda x.t) &=\lambda x.t\\
  (r/x)(\lambda y.t) &=\lambda y.{(r/x)t} \quad\textrm{Si }y\notin \FV(r)\\
  (r/x)(\lambda y.t) &=\lambda z.{(r/x)(z/y)t} \quad\textrm{Si }y\in \FV(r)\\
  (r/x)(ts) &= (r/x)t(r/x)s\\
  (r/x)\pair ts &=\pair{(r/x)t}{(r/x)s}\\
  (r/x)\elimandl t &=\elimandl{(r/x)t}\\
  (r/x)\elimandr t &=\elimandr{(r/x)t}\\
  (r/x)\inl(t) &=\inl({(r/x)t})\\
  (r/x)\inr(t) &=\inr({(r/x)t})\\
  (r/x)\elimor tx{s_1}z{s_2} &=\elimor{(r/x)t}x{s_1}z{(r/x)s_2} \quad\textrm{Si } z\notin\FV(r)\\
  (r/x)\elimor tx{s_1}z{s_2} &=\elimor{(r/x)t}x{s_1}w{(r/x)(z/w)s_2} \quad\textrm{Si } z\in\FV(r)\\
  (r/x)\elimor ty{s_1}x{s_2} &=\elimor{(r/x)t}y{(r/x)s_1}x{s_2} \quad\textrm{Si } y\notin\FV(r)\\
  (r/x)\elimor ty{s_1}x{s_2} &=\elimor{(r/x)t}y{(r/x)(y/w)s_1}z{s_2} \quad\textrm{Si } y\in\FV(r)\\
  (r/x)\elimor ty{s_1}z{s_2} &=\elimor{(r/x)t}y{(r/x)s_1}z{(r/x)s_2} \quad\textrm{Si }\{y,z\}\cap\FV(r)=\emptyset\\
  (r/x)\elimor ty{s_1}z{s_2} &=\elimor{(r/x)t}w{(r/x)(w/y)s_1}z{(r/x)s_2} \quad\textrm{Si }\{y,z\}\cap\FV(r)=\{y\}\\
  (r/x)\elimor ty{s_1}z{s_2} &=\elimor{(r/x)t}y{(r/x)s_1}w{(r/x)(w/z)s_2} \quad\textrm{Si }\{y,z\}\cap\FV(r)=\{z\}\\
  (r/x)\elimor ty{s_1}z{s_2} &=\elimor{(r/x)t}{w_1}{(r/x)(w_1/y)s_1}{w_1}{(r/x)(w_2/z)s_2} \quad\textrm{Si }\{y,z\}\subseteq\FV(r)
\end{align*}
\begin{ejercicio}
  Calcular los siguientes reemplazos, explicando si ocurre renombrado de
  variables:
  \begin{enumerate}
    \item $(\star/x)(\lambda x.x)$
    \item $(y/x)(\lambda y.xy)$
    \item $(\inl(z)/x)\,\elimor{x}{u}{u}{v}{v}$
  \end{enumerate}
\end{ejercicio}
\begin{ejercicio}
  Definir, por inducción sobre $t$, $\FV(t)$.
\end{ejercicio}

\begin{ejercicio}
  Definir, por inducción sobre $t$, $BV(t)$, es decir, el conjunto de
  variables ligadas de $t$ (``bounded variables'').
\end{ejercicio}

\subsection{Estrategias de reducción}
\paragraph{Primeras definiciones}

\begin{definicion}
  Notamos $\lra^*$ al cierre reflexivo y transitivo de $\lra$.

  Es decir, si $t\lra^* r$, entonces, $t=s_0\lra s_1\lra s_2\lra\dots\lra s_n=r$, con $n\geq 0$.

  Notamos $\lra^+$ al cierre transitivo de $\lra$.

  Es decir, si $t\lra^* r$, $t=s_0\lra s_1\lra\dots\lra s_n=u$, con $n\geq 1$.
\end{definicion}

\begin{ejemplo}
  $(\lambda x.{x;\star})\star\lra^* \star$ porque $(\lambda x.{x;\star})\star\lra\star;\star\lra\star$.

  También, $(\lambda x.{x;\star})\star\lra^+\star$, ya que $(\lambda x.{x;\star})\star\neq \star$.
\end{ejemplo}

\needspace{4em}
\begin{definicion}
  ~
  \begin{enumerate}
    \item Un término $t$ está en forma normal si no existe $r$ tal que $t\lra r$.
    \item Un término $t$ es normalizable (o tiene forma normal) si existe $r$ en
      forma normal tal que $t\lra^*r$.
    \item Un término $t$ es fuertemente normalizable si no existe una secuencia
      infinita $s_0$, $s_1$, $\dots$ tal que $t\lra s_0\lra s_1\lra\dots$. Es decir,
      toda secuencia de reducción comenzada en $t$ debe ser finita y terminar en
      un término en forma normal.
  \end{enumerate}
\end{definicion}

\begin{definicion}
  Sea $\lra_R$ una relación binaria, y $\lra^*_R$ su cierre reflexivo y
  transitivo.
  \begin{itemize}
    \item
      \begin{tikzpicture}[baseline=-1.5ex]
	\node at (-7,-.4) {\parbox{0.7\textwidth}{ $\lra_R$ satisface la
	    \emph{propiedad del diamante} si $t\lra_R r_1$ y $t\lra_R r_2$ implica
	que $r_1\lra_R s$ y $r_2\lra_R s$ para algún $s$.} }; \node (t) at (0,0)
	{$t$}; \node (v1) at (-1,-1) {$r_1$}; \node (v2) at (1,-1) {$r_2$}; \node
	(u) at (0,-2) {$s$}; \draw[thick,->] (t) -- (v1); \draw[thick,->] (t) --
	(v2); \draw[thick,->] (v1) -- (u); \draw[thick,->] (v2) -- (u);
      \end{tikzpicture}
    \item $\lra_R$ es \emph{Church-Rosser} o \emph{confluente} si $\lra^*_R$
      satisface la propiedad del diamante. Es decir, si $t\lra^*_R r_1$ y $t\lra^*_R
      r_2$ implica que $r_1\lra^*_R s$ y $r_2\lra^*_R s$ para algún $s$.
    \item $\lra_R$ tiene \emph{formas normales únicas} si $t\lra^*_R r_1$ y
      $t\lra^*_R r_2$, para términos en forma normal $r_1$ y $r_2$, implica
      $r_1=r_2$.
  \end{itemize}
\end{definicion}

\needspace{3em}
\begin{lema}\label{lem:diamante-implies-CR-unf}
  ~
  \begin{enumerate}
  \item Si $\lra_R$ satisface la propiedad del diamante, entonces es
    Church-Rosser.
  \item Si $\lra_R$ es Church-Rosser, entonces tiene formas normales únicas.
  \end{enumerate}
\end{lema}
\begin{ejercicio}
  Demostrar el Lemma~\ref{lem:diamante-implies-CR-unf}
\end{ejercicio}

\begin{teorema}
  La relación definida en la Sección~\ref{sec:semOp} (semántica operacional) es Church-Rosser. \qed
\end{teorema}

\begin{ejemplo}
  \begin{tikzpicture}[baseline=0pt]
    \node (t) at (0,0) {$(\lambda x.\star;x)\star$}; \node (v1) at (-2,-1)
    {$\star;\star$}; \node (v2) at (2,-1) {$(\lambda x.x)\star$}; \node (u) at
    (0,-2) {$\star$}; \draw[thick,->] (t) -- (v1); \draw[thick,->] (t) -- (v2);
    \draw[thick,->] (v1) -- (u); \draw[thick,->] (v2) -- (u);
  \end{tikzpicture}
\end{ejemplo}

Pero esta propiedad, cuando hay términos que no terminan, no es suficiente, como veremos en los siguientes ejemplos.
\begin{ejemplo}
  Sea
  \[
    \Omega_\star = (\lambda x.xx\star)(\lambda x.xx\star)
  \]
  Es facil ver que $\Omega_\star\lra\Omega_\star \star\lra\Omega_\star \star\star\lra\Omega_\star \star\star\star\lra\cdots$.

  Entonces:
  \begin{align*}
    \elimor{\inl(\star)}xxy{\Omega_\star}
    &=
    \elimor{\inl(\star)}xxy{\Omega_\star\star}\\
    &\lra
    \elimor{\inl(\star)}xxy{\Omega_\star\star\star}\\
    &\lra
    \elimor{\inl(\star)}xxy{\Omega_\star\star\star\star}\\
    &\lra\cdots\lra\infty
  \end{align*}
  $\elimor{\inl(\star)}xxy{\Omega_\star}$ tiene un único resultado que es
  $\star$, pero no cualquier camino llega a él.

  Solución (en este caso): cuando hay un $\mathsf{match}$, reducir primero el
  $\mathsf{match}$ antes que sus ramas. Ésto, como veremos luego, es una
  \emph{estrategia}.
\end{ejemplo}
\begin{ejemplo}
  Sea $C_0 = \lambda x.0$ y $\Omega_\star^n = \Omega_\star\underbrace{\star\cdots\star}_n$.

  \begin{center}
    \begin{tikzpicture}
      \node (C0b1) at (0,0) {$C_0\Omega_\star^0$};
      \node (C0b2) at (0,-2) {$C_0\Omega_\star^1$};
      \node (cero) at (2,0) {$0$};
      \node (C0b12) at (2,-2) {$C_0\Omega_\star^2$};
      \node (C0b22) at (4,-2) {$C_0\Omega_\star^3$};
      \node (C0b13) at (6,-2) {$C_0\Omega_\star^4$};
      \node (dots) at (8,-2) {$\dots$};
      \draw[thick,->] (C0b1) -- (cero);
      \draw[thick,->] (C0b1) -- (C0b2);
      \draw[thick,->] (C0b2) -- (C0b12);
      \draw[thick,->] (C0b12) -- (cero);
      \draw[thick,->] (C0b12) -- (C0b22);
      \draw[thick,->] (C0b22) -- (C0b13);
      \draw[thick,->] (C0b22) -- (cero);
      \draw[thick,->] (C0b13) -- (cero);
      \draw[thick,->] (C0b13) -- (dots);
    \end{tikzpicture}
  \end{center}
\end{ejemplo}
La noción de \emph{estrategia de reducción} permite definir el orden en el cual
se debe reducir un término.

\begin{definicion}
  Llamamos \emph{redex} (por \emph{Red}ucible \emph{Ex}pression) a un subtérmino de un término
  que puede reducir.
\end{definicion}

\paragraph{Reducción débil}
Ejemplo motivador:
\[
  \begin{tikzcd}
    {(\lambda x.\elimor{\inl(\star)}y{y;x}zz)\star}\ar[r,red]\ar[d,blue] & {\elimor{\inl(\star)}y{y;\star}zz} \ar[r,red] &{\star;\star}\ar[r,red] & \star\\
    (\lambda x.\star;x)\star\ar[d,blue]\ar[rru,dashed] & \\
    (\lambda x.x)\star\ar[rrruu,blue] 
  \end{tikzcd}
\]
\begin{itemize}
  \item La dirección {\color{red}$\to$} dice qué sucede cuando se ejecuta el programa.
  \item La dirección {\color{blue}$\downarrow$} comienza a ejecutar el programa antes de recibir
  los argumentos, es decir, no ejecuta el programa sino que lo optimiza.
\end{itemize}

\begin{definicion}
  Una estrategia de reducción es \emph{débil} si no reduce nunca el cuerpo de
  una función, es decir, si no reduce bajo $\lambda$.
\end{definicion}

\begin{observacion}
  La estrategia débil no optimiza programa, los ejecuta. Sólo hace falta para
  ésto eliminar la regla
  \[
    \infer {\lambda x.t\lra\lambda x.u} {t\lra u}
  \]
\end{observacion}

\paragraph{Call-by-name}
\begin{center}
  \begin{tikzpicture}
    \draw[dashed] (-.7,.5) -- (2.5,.5);
    \draw[dashed] (-.7,.5) -- (-.7,-.5);
    \draw[dashed] (-.7,-.5) -- (2.5,-.5);
    \draw[dashed] (2.5,-.5) -- (2.5,.5);
    \node (C0b1) at (0,0) {$C_0\Omega_\star^0$};
    \node (C0b2) at (0,-2) {$C_0\Omega_\star^1$};
    \node (cero) at (2,0) {$0$};
    \node (C0b12) at (2,-2) {$C_0\Omega_\star^2$};
    \node (C0b22) at (4,-2) {$C_0\Omega_\star^3$};
    \node (C0b13) at (6,-2) {$C_0\Omega_\star^4$};
    \node (dots) at (8,-2) {$\dots$};
    \draw[thick,->] (C0b1) -- (cero);
    \draw[thick,->] (C0b1) -- (C0b2);
    \draw[thick,->] (C0b2) -- (C0b12);
    \draw[thick,->] (C0b12) -- (cero);
    \draw[thick,->] (C0b12) -- (C0b22);
    \draw[thick,->] (C0b22) -- (C0b13);
    \draw[thick,->] (C0b22) -- (cero);
    \draw[thick,->] (C0b13) -- (cero);
    \draw[thick,->] (C0b13) -- (dots);
  \end{tikzpicture}
\end{center}

\begin{definicion}
  La estrategia \emph{call-by-name} reduce siempre el redex de más a la
  izquierda. En caso de ser además débil, será el más a la izquierda que no esté
  debajo de un $\lambda$.
\end{definicion}

\begin{teorema}
  [Estandarización] Si un término reduce a un término en forma normal, entonces
  la estrategia call-by-name termina. \qed
\end{teorema}

Una ventaja de ésta estrategia es el teorema de estandarización. Otra ventaja
es que si tenemos, por ejemplo $(\lambda x.\star)(\mathit{Fact}\ 10)$ no
necesitamos calcular el factorial de $10$. Por otro lado, si tenemos $(\lambda
x.\pair xx)(\mathit{Fact}\ 10)$, tendremos que calcular el factorial de $10$ dos
veces.

De todas maneras, la mayoría de los lenguajes que usan call-by-name usan alguna
manera de ``compartir'' información (por ejemplo, con punteros que dicen que
  $(\lambda x.\pair xx)(\mathit{Fact}\ 10)$ reduce a $\pair xx$, donde $x$ es un
  puntero a $\mathit{Fact}\ 10$). A eso se le llama \emph{reducción lazy}.



\begin{ejercicio}
Escribir las reglas de reducción y congruencia que implementan call-by-name.
\end{ejercicio}

\paragraph{Call-by-value}
\begin{center}
  \begin{tikzpicture}
    \draw[dashed] (-.7,-1.5) -- (8.5,-1.5);
    \draw[dashed] (-.7,-1.5) -- (-.7,-2.5);
    \draw[dashed] (-.7,-2.5) -- (8.5,-2.5);
    \draw[dashed] (8.5,-2.5) -- (8.5,-1.5);
    \node (C0b1) at (0,0) {$C_0\Omega_\star^0$};
    \node (C0b2) at (0,-2) {$C_0\Omega_\star^1$};
    \node (cero) at (2,0) {$0$};
    \node (C0b12) at (2,-2) {$C_0\Omega_\star^2$};
    \node (C0b22) at (4,-2) {$C_0\Omega_\star^3$};
    \node (C0b13) at (6,-2) {$C_0\Omega_\star^4$};
    \node (dots) at (8,-2) {$\dots$};
    \draw[thick,->] (C0b1) -- (cero);
    \draw[thick,->] (C0b1) -- (C0b2);
    \draw[thick,->] (C0b2) -- (C0b12);
    \draw[thick,->] (C0b12) -- (cero);
    \draw[thick,->] (C0b12) -- (C0b22);
    \draw[thick,->] (C0b22) -- (C0b13);
    \draw[thick,->] (C0b22) -- (cero);
    \draw[thick,->] (C0b13) -- (cero);
    \draw[thick,->] (C0b13) -- (dots);
  \end{tikzpicture}
\end{center}


\begin{definicion}
  A los términos $t$ tales que $\FV(t)=\emptyset$ y que $t$ esté en forma
  normal, se les llaman \emph{valores}.
\end{definicion}
\begin{ejercicio}
Indicar cuáles de los siguientes términos son valores (términos cerrados y en
forma normal) y justificar:
\begin{enumerate}
  \item $\lambda x.\pair{x}{x}$
  \item $\inl(y)$
  \item $\elimandl{\pair{\star}{\star}}$
  \item $\pair{\star}{\lambda x.x}$
\end{enumerate}
\end{ejercicio}


\begin{definicion}
  La estrategia \emph{call-by-value} consiste en evaluar siempre los argumentos
  antes de pasarlos a la función. La idea es que
  \[
    (\lambda x.t)v
  \]
  reduce sólo cuando $v$ esté en forma normal (si la estrategia es débil, y sólo
  reducimos términos cerrados, $v$ es un valor).
\end{definicion}

En $(\lambda x.\pair xx)(\mathit{Fact}\ 10)$ comenzamos por reducir el factorial,
obtenemos $3628800$ y recién ahí lo pasamos a la función. De esa manera el
factorial es calculado una vez. 

\begin{ejercicio}
  Escribir las reglas que implementan call-by-value.
\end{ejercicio}

\begin{observacion}
  Un poco de pereza es necesaria: $\mathsf{match}$ \emph{siempre} debe evaluar
  primero la condición, estemos en call-by-name o call-by-value.
\end{observacion}

\section{Tipos simples}
\subsection{Introducción}
Ejemplos motivadores:
\begin{align*}
  (\lambda x.{\pi_1 x})\lambda x.x &\lra\pi_1\lambda x.x\\
  \elimor{\lambda x.x}xxyy &\not\lra\\
  (\lambda x.x)\star\lambda x.x&\lra \star\lambda x.x
\end{align*}
¡Todo es aplicable a todo! Sin restricciones. Proyectar una función no tiene
sentido. Hacer un $\mathsf{match}$ sobre una función o pasarle un argumento a
un $\star$, tampoco.

\textbf{Idea:} detectar este tipo de errores sintácticamente. Por ejemplo:
\[
  \infer {(\fun xx)\star\textrm{ es una constante}} {\fun xx\textrm{ recive un
      argumento y devuelve lo mismo} &\star\textrm{ es una constante}}
\]
Es decir, deducimos que no tiene sentido pasarle un argumento a $(\fun xx)\star$, ya
que es una constante, y lo dedujimos sin tener que \emph{ejecutar} el programa.

\paragraph{En matemáticas:}
\begin{center}
  \begin{tikzpicture}
    \node at (0.3,0) {Función:}; \node (D) at (2,0) {Dominio}; \node (C) at
    (4.5,0) {Codominio}; \node (T) at (3.25,-1) {Cualquier conjunto}; \draw[very
    thick,->] (D) -- (C); \draw[->] (T) -- (D); \draw[->] (T) -- (C);
  \end{tikzpicture}
\end{center}
Ejemplo:
\begin{align*}
  f :\mathit{Pares}&\to\mathbb N\\
  f(x) &\mapsto \frac x2
\end{align*}

¿Está bien definido $f(3+(4+1))$? Hay que determinar si $3+(4+1)$ pertenece al
dominio, es decir, si es par. \medskip

En general, determinar si un objeto cualquiera pertenece a un conjunto
cualquiera es un problema \emph{indecidible}. \medskip

De todas maneras, $\frac x2$ lo podemos calcular si $x$ es un número (y no, por
ejemplo, una función), y poco importa si es par o no. Así que vamos a restringir
las clases de conjuntos que se pueden utilizar como dominios. A estos conjuntos
los llamamos \textbf{tipos}.

\subsection{Gramática}
\begin{definicion}
  [Gramática de tipos]\label{def:gramatica-de-tipos}
  El lenguaje de tipos simples que consideramos en este curso es el que se obtiene a partir de la siguiente gramática:
  \[
    A ::= \top\mid\bot\mid A\Rightarrow A\mid A\wedge A\mid A\vee A
  \]
  Donde $\top$ y $\bot$ son dos tipos de base.
\end{definicion}

\subsection{La relación de tipado}
\begin{definicion}
  [Contexto de tipado]\label{def:contexto-de-tipado}
  Un contexto de tipado es un conjunto finito de pares de variables con tipos: $\{(x_1,A_1),\dots,(x_n,A_n)\}$. Usualmente escribimos los contextos como $x_1:A_1,\dots,x_n:A_n$, y los notamos genéricamente con letras griegas mayúsculas como $\Gamma,\Delta,\Xi$.
\end{definicion}

\begin{definicion}[Reglas de tipado]\label{def:reglas-de-tipado}
  La relación de tipado es una relación entre un contexto de tipado $\Gamma$, un término $t$ y un tipo $A$ (notación $\Gamma\vdash t:A$), que se define por medio de las siguientes reglas de tipado.
  Notar que para cada constructor de la gramática hay reglas de introducción ${}_i$ y de eliminación ${}_e$, excepto para $\bot$, donde sólo hay una regla de eliminación.
  \[
    \infer[\textrm{ax}]{\Gamma,x:A\vdash x:A}{}
    \qquad
    \infer[\top_i]{\Gamma\vdash\star:\top}{}
    \qquad
    \infer[\top_e]{\Gamma\vdash\elimtop tr:A}{\Gamma\vdash t:\top & \Gamma\vdash r:A}
    \qquad
    \infer[\bot_e]{\Gamma\vdash\elimbot t:C}{\Gamma\vdash t:\bot}
  \]
  \[
    \infer[\Rightarrow_i]{\Gamma\vdash\lambda x.t:A\Rightarrow B}{\Gamma,x:A\vdash t:B}
    \qquad
    \infer[\Rightarrow_e]{\Gamma\vdash tr:B}{\Gamma\vdash t:A\Rightarrow B & \Gamma\vdash r:A}
  \]
  \[
    \infer[\wedge_i]{\Gamma\vdash\pair tr:A\wedge B}{\Gamma\vdash t:A & \Gamma\vdash r:B}
    \qquad
    \infer[\wedge_{e_1}]{\Gamma\vdash\elimandl t:A}{\Gamma\vdash t:A\wedge B}
    \qquad
    \infer[\wedge_{e_2}]{\Gamma\vdash\elimandr t:B}{\Gamma\vdash t:A\wedge B}
  \]
  \[
    \infer[\vee_{i_1}]{\Gamma\vdash\inl(t):A\vee B}{\Gamma\vdash t:A}
    \qquad
    \infer[\vee_{i_2}]{\Gamma\vdash\inr(t):A\vee B}{\Gamma\vdash t:B}
  \]
  \[
    \infer[\vee_e]{\Gamma\vdash\elimor txrys:C}{\Gamma\vdash t:A\vee B & \Gamma,x:A\vdash r:C & \Gamma,y:B\vdash s:C}
  \]
\end{definicion}

Descripción de las reglas:
\begin{itemize}
  \item La regla ax dice que si en el contexto de tipado se tiene $x:A$, entonces 
    puedo derivar que $x:A$.
  \item La regla $\top_i$ nos dice que en cualquier contexto de tipado, $\star:\top$.
  \item La regla $\top_e$ nos dice que si en un contexto de tipado $t:\top$, entonces $\elimtop tr$ tendrá el tipo de $A$, la reducción de $t$ terminará en $\star$, el cual será descartado luego.
  \item La regla $\bot_e$ nos dice que si en un contexto de tipado podemos derivar $t:\bot$, entonces $t$ es un error y podemos tiparlo con cualquier tipo, márcandolo como error.
  \item La regla $\Rightarrow_i$ nos dice que si en un contexto de tipado tengo $x:A$ y podemos derivar $t:B$, entonces quitando esa variable del contexto, podemos derivar
    que $\lambda x.T:A\Rightarrow B$.
  \item La regla $\Rightarrow_e$ dice que si en un contexto de tipado podemos derivar que $t$ tiene el tipo de función $A\Rightarrow B$, y que $r:A$, entonces la aplicación de $t$ a $r$ tendrá el tipo $B$.
  \item La regla $\wedge_i$ nos dice que si en un contexto de tipado podemos derivar $t:A$ y también $r:B$, entonces podemos derivar $\pair tr:A\wedge B$.
  \item Las reglas $\wedge_{e_1}$ y $\wedge_{e_2}$ nos dicen que a si en un contexto de tipado podemos derivar $t:A\wedge B$, entonces podemos derivar tanto $\elimandl t:A$ como $\elimandr t:B$.
  \item La regla $\vee_{i_1}$ nos dice que si en un contexto de tipado podemos derivar $t:A$, entonces podemos derivar $\inl(t):A\vee B$. La regla $\vee_{i_2}$ nos dice lo mismo si podemos derivar $t:B$ en lugar de $t:A$.
  \item La regla $\vee_e$ nos dice que si en un contexto de tipado podemos derivar $t:A\vee B$, y además agregando la variable $x:A$ o la $y:B$, podemos derivar $r:C$ y $s:C$ respectrivamente, entonces podemos derivar $\elimor txys:C$.
\end{itemize}

\begin{ejemplo}
  La derivación $\vdash\lambda x.\lambda y.x:A\Rightarrow (B\Rightarrow A)$ es válida. 
  La prueba es la siguiente.
  \[
    \infer[\Rightarrow_i]{\vdash\lambda x.\lambda y.x:A\Rightarrow(B\Rightarrow A)}
    {
      \infer[\Rightarrow_i]{x:A\vdash\lambda y.x:B\Rightarrow A}
      {
	\infer[\textrm{ax}]{x:A,y:B\vdash x:A}{}
      }
    }
  \]
\end{ejemplo}

\begin{ejemplo}
  Sea $\Gamma=x:(A\wedge B)\Rightarrow C,y:A,z:C\Rightarrow\bot$ y $\Delta=\Gamma,w:B$.
  \[
    \infer[\Rightarrow_i]{\Gamma\vdash\lambda w.z(x\pair yw):B\Rightarrow\bot}
    {
      \infer[\Rightarrow_e]{\Delta\vdash z(x\pair yw):\bot}
      {
	\infer[\textrm{ax}]{\Delta\vdash z:C\Rightarrow\bot}{}
	&
	\infer[\Rightarrow_e]{\Delta\vdash x\pair yw:C}
	{
	  \infer[\textrm{ax}]{\Delta\vdash x:(A\wedge B)\Rightarrow C}{}
	  &
	  \infer[\wedge_i]{\Delta\vdash\pair yw:A\wedge B}
	  {
	    \infer[\textrm{ax}]{\Delta\vdash y:A}{}
	    &
	    \infer[\textrm{ax}]{\Delta\vdash w:B}{}
	  }
	}
      }
    }
  \]
\end{ejemplo}

\begin{ejemplo}
  Sean $\Delta=x:(\top\vee\top)\Rightarrow\top$, y $\Gamma=\Delta,y:\top$. Entonces,
  \[
    \infer[\Rightarrow_i]{\vdash\lambda x.{x((\lambda y.{\elimtop y{\inl(\star)}})\star)}:((\top\vee\top)\Rightarrow\top)\Rightarrow\top}
    {
      \infer[\Rightarrow_e]{\Delta\vdash x((\lambda y.{\elimtop y{\inl(\star)}})\star):\top}
      {
	\infer[\textrm{ax}]{\Delta\vdash x:(\top\vee\top)\Rightarrow\top}{}
	&
	\infer[\Rightarrow_e]{\Delta\vdash(\lambda y.{\elimtop y{\inl(\star)}})\star:\top\vee\top} 
	{
	  \infer[\Rightarrow_i]{\Delta\vdash\lambda y.{\elimtop y{\inl(\star)}}:\top\Rightarrow(\top\vee\top)}
	  {
	    \infer[\top_e]{\Gamma\vdash \elimtop y{\inl(\star)}:\top\vee\top}
	    {
	      \infer[\textrm{ax}]{\Gamma\vdash y:\top}{} 
	      &
	      \infer[\vee_i]{\inl(\star):\top\vee\top}{
		\infer[\top_i]{\Gamma\vdash \star:\top}{} 
	      }
	    } 
	  }
	  &
	  \infer[\top_i]{\Delta\vdash \star:\top}{}
	} 
      } 
    }
  \]
\end{ejemplo}

\begin{ejercicio}
  Determinar si las siguientes juicios de tipado pueden derivarse usando las
  reglas dadas. En caso negativo, explicar qué regla falla:
  \begin{enumerate}
    \item $x:A \vdash \pair{x}{x} : A\wedge A$
    \item $\vdash \lambda x.\elimandl x : A\Rightarrow B$
    \item $x:A\vee B \vdash \elimandr x : B$
  \end{enumerate}
\end{ejercicio}

\begin{ejercicio}
Dar un juicio de tipado para el siguiente término: $\lambda x.{xx}$.
\end{ejercicio}

\begin{teorema}
  [Subject reduction]
  \label{thm:SR}
  Si $\Gamma\vdash t:A$ y $t\lra r$ entonces $\Gamma\vdash r:A$.
  \qed
\end{teorema}

Es decir: si deducimos el tipo $A$ para un término, con las reglas de tipado
(sin ``ejecutar'' el programa), y luego ejecutamos el programa obteniendo $r$,
entonces el término $r$ tiene el mismo tipo. ¡Es exactamente lo que queríamos!
La intención fue desde el principio saber qué \emph{tipo} de resultado voy a
tener al ejecutar un programa ($\top$, una función, etc), y este teorema nos
dice que el sistema de tipos que definimos hace eso.

\begin{teorema}
  [Normalización fuerte]
  \label{thm:SN}
  Todo término tipado, termina.
  \qed
\end{teorema}

¿Qué sucede con $\Omega_\star = (\lambda x.{xx\star})(\lambda x.{xx\star})$? No es tipable. Es decir, no
existe un tipo $A$ tal que $\vdash\Omega_\star:A$. \medskip


\begin{observacion}
  En este lambda cálculo extendido podemos codificar $\mathsf{true}=\inl(\star)$, $\mathsf{false}=\inr(\star)$ y tendremos
  \[
    \mathsf{if}\ t\ \mathsf{then}\ r\ \mathsf{else}\ s:=\elimor txrys
  \]
  En efecto,
  \begin{align*}
    \mathsf{if}\ \mathsf{true}\ \mathsf{then}\ r\ \mathsf{else}\ s
    &=\elimor{\inl(\star)}xrys\lra r \\
    \mathsf{if}\ \mathsf{false}\ \mathsf{then}\ r\ \mathsf{else}\ s
    &=\elimor{\inr(\star)}xrys\lra r
  \end{align*}

  Por lo tanto, podemos identificar el tipo $\mathsf{Bool}$ con $\top\vee\top$.

  La compuerta $\mathsf{Not}$ que a $\mathsf{true}$ le asocia $\mathsf{false}$ y viceversa, la podemos codificar como
  \[
    \mathsf{Not}:=\lambda x.\elimor xy{\inr(\star)}z{\inl(\star)}
  \]
\end{observacion}

\begin{ejercicio}
  Mostrar que $\vdash\mathsf{Not}:(\top\vee\top)\Rightarrow(\top\vee\top)$.
\end{ejercicio}
\begin{ejercicio}
  Definir las compuertas $\mathsf{And}$ y $\mathsf{Or}$.
\end{ejercicio}
\begin{ejercicio}
  Dar el tipo de los términos del ejercicio anterior.
\end{ejercicio}

\section{El isomorfismo}
Tan sólo viendo las Definiciones~\ref{def:reglas-de-prueba} (reglas de prueba de la lógica proposicional) y \ref{def:reglas-de-tipado} (reglas de tipado del cálculo lambda extendido), es evidente de que estamos hablando de lo mismo. 

\subsection{El cálculo lambda como un lenguaje de pruebas}
Si consideramos el secuente $\vdash \top \Rightarrow \top $, podemos probarlo mediante la siguiente derivación de prueba:
\begin{equation}
  \label{eq:Id}
  \infer[\Rightarrow_i]{\vdash \top \Rightarrow \top }{
    \infer[\textrm{ax}]{\top \vdash \top }{}
  }
\end{equation}
pero también podemos derivarlo con
\begin{equation}
  \label{eq:IdId}
  \infer[\Rightarrow_e]{\vdash \top \Rightarrow \top }
  {
    \infer[\Rightarrow_i]{\vdash (\top \Rightarrow \top )\Rightarrow (\top \Rightarrow \top )}{
      \infer[\textrm{ax}]{\top\Rightarrow\top \vdash \top\Rightarrow\top }{}
      &
      \infer[\Rightarrow_i]{\vdash \top \Rightarrow \top }{
	\infer[\textrm{ax}]{\top \vdash \top }{}
      }
    }
  }
\end{equation}
La primer derivación, corresponde al término $\lambda x.x$, mientras que la segunda corresponde al término $(\lambda y.y)\lambda x.x$. Más aún, el secuente 
\begin{equation}
  \label{eq:Idt}
  \vdash\lambda x.x:\top\Rightarrow\top
\end{equation}
está en correspondencia biunívoca con la prueba \eqref{eq:Id} y 
\begin{equation}
  \label{eq:IdIdt}
  \vdash(\lambda y.y)\lambda x.x:\top\Rightarrow\top
\end{equation}
con la prueba \eqref{eq:IdId}. En general, tenemos que $\Gamma\vdash t:A$ está en correspondencia con una prueba y sólo una de $\Gamma\vdash A$, y por eso podemos acuñar el slogan
\begin{quote}\centering
  \textit{Los términos tipados del cálculo lambda son las pruebas de las proposiciones de la lógica proposicional}
\end{quote}

\subsection{La semántica operacional y el cut-elimination}
La Definición~\ref{def:cut-elim} (cut-elimination) también coincide con la Definición~\ref{def:reglas-de-reduccion} (reglas de reducción). Por ejemplo, podemos ver que la derivación~\eqref{eq:IdId} reduce, mediante las reglas de cut-elimination, a la derivación~\eqref{eq:Id}, de la misma manera que el término~\eqref{eq:Idt} reduce al término~\eqref{eq:IdIdt} usando las reglas de reducción.

\begin{itemize}
  \item La regla $(\top)$ coincide con la regla $(\mathsf{sec})$: Se trata de la introducción de $\top$ ($\star$) seguido de su eliminación (la secuencia).
    \[
      \vcenter{
	\infer[\top_e]{\Gamma\vdash A}{\infer[\top_i]{\Gamma\vdash\top}{} & 
	\infer{\Gamma\vdash A}{\pi}}
      }
      \lra
      \pi
      \qquad\qquad
      \elimtop{\star}t \lra t
    \]
  \item La regla $(\Rightarrow)$ coincide con la regla $(\beta)$: Se trata de la introducción de $\Rightarrow$ (una lambda abstracción) seguida de su eliminación (la aplicación).
    \[
      \vcenter{\infer[\Rightarrow_e]{\Gamma\vdash B}
	{
	  \infer[\Rightarrow_i]{\Gamma\vdash A\Rightarrow B}{\infer{\Gamma,A\vdash B}{\pi_1}}
	  &
	  \infer{\Gamma\vdash A}{\pi_2}
	}
      }
      \longrightarrow
      (\pi_2/A)\pi_1
      \qquad\qquad
      (\lambda x.t)r \lra (r/x)t
    \]
  \item Las reglas $(\wedge_1)$ y $(\wedge_2)$ coinciden con las reglas $(\pi_1)$ y $(\pi_2)$: Se trata de la introducción de $\wedge$ (un par) seguido de su eliminación (la proyección).
    \[
      \vcenter{\infer[\wedge_{e_1}]{\Gamma\vdash A}
	{
	  \infer[\wedge_i]{\Gamma\vdash A\wedge B}{\infer{\Gamma\vdash A}{\pi_1} & \infer{\Gamma\vdash B}{\pi_2}}
      }}
      \longrightarrow
      \pi_1
      \qquad\qquad
      \elimandl{\pair tr} \lra t
    \]
    \[
      \vcenter{\infer[\wedge_{e_2}]{\Gamma\vdash B}
	{
	  \infer[\wedge_i]{\Gamma\vdash A\wedge B}{\infer{\Gamma\vdash A}{\pi_1} & \infer{\Gamma\vdash B}{\pi_2}}
      }}
      \longrightarrow
      \pi_2
      \qquad\qquad
      \elimandr{\pair tr} \lra r 
    \]
  \item Las reglas $(\vee_1)$ y $(\vee_2)$ coinciden con las reglas $(\mathsf{match}_l)$ y $(\mathsf{match}_r)$: Se trata de la introducción de $\vee$ ($\inl$ o $\inr$) seguido de su eliminación (el match).
    \[
    \vcenter{
      \infer[\vee_e]{\Gamma\vdash C}
      {
	\infer[\vee_{i_1}]{\Gamma\vdash A\vee B}{\infer{\Gamma\vdash A}{\pi_1}}
	&
	\infer{\Gamma,A\vdash C}{\pi_2}
	&
	\infer{\Gamma,B\vdash C}{\pi_3}
      }
    }
    \longrightarrow
    \begin{array}[t]{l}
      (\pi_1/A)\pi_2\\[1ex]
      \qquad\elimor{\inl(t)}xrys \lra (t/x)r 
    \end{array}
    \]
    \[
    \vcenter{
      \infer[\vee_e]{\Gamma\vdash C}
      {
	\infer[\vee_{i_2}]{\Gamma\vdash A\vee B}{\infer{\Gamma\vdash B}{\pi_1}}
	&
	\infer{\Gamma,A\vdash C}{\pi_2}
	&
	\infer{\Gamma,B\vdash C}{\pi_3}
      }
    }
    \longrightarrow
    \begin{array}[t]{l}
      (\pi_1/A)\pi_3\\[1ex]
      \qquad\elimor{\inr(t)}xrys \lra (t/y)s 
    \end{array}
    \]
\end{itemize}


\chapter{Rapid(ísim)a descripción de la lógica lineal (MALL)}\label{chap:LL}
Esta sección es una adaptación libre de la Sección 3.1 del artículo de
\cite{DiCosmoMillerTSEP16}.  Para más detalles, se sugiere recurrir a dicha
fuente.

\section{Introducción}
La lógica lineal fue introducida por \cite{GirardTCS87}. Aquí daremos una
presentación por medio de cálculo de secuentes, ya que es muy similar a las
reglas de tipado que hemos visto en las secciones anteriores.

La idea principal a retener es que la lógica lineal es una lógica de recursos:
la fórmula $A\Rightarrow B$ normalmente se entiende como ``Si me das $A$, te
devuelvo $B$'', pero, en la práctica, significa más bien ``Si me das tantas $A$
como necesite, te devuelvo $B$''. Por ejemplo, el término
\[
  \lambda x.x;x
\]
tiene tipo $\top\Rightarrow\top$, pero para calcular $x;x$, se necesitan dos
copias de $x$. Es decir, el recurso ($x$), fue duplicado para poder calcular el
resultado. Si, por ejemplo, el recurso fuese un programa complejo que devuelve
$\star$, entonces duplicar el recurso tiene un costo.  En lógica lineal no
podemos duplicar recursos. Así, el tipo $\top\multimap\top$ significa: ``Si me
das un $\top$, te devuelvo un $\top$ usándolo exactamente una vez''.

Ésta lógica nos será de utilidad para definir cálculos cuánticos, ya que el
teorema de no clonado (Teorema~\ref{thm:no-cloning}) nos impide clonar recursos
cuánticos.

De la misma manera, la función $\lambda x.\star$, que descarta su argumento, no
podríamos decir que tiene tipo $\top\multimap\top$, ya que no utiliza una vez
su argumento.

\section{Cálculo de secuentes para MELL}
Los conectivos de la lógica lineal se dividen en multiplicativos (que no
permiten duplicar recursos) y aditivos (que lo permiten), y los conectivos
clásicos tienen su paralelo en ambos:
\begin{center}
  \begin{tabular}{|cc||cc|cc|}\hline
    \multicolumn{2}{|c||}{Clásico} & \multicolumn{2}{c}{Multiplicativo} & \multicolumn{2}{|c|}{Aditivo}\\\hline\hline
    $\wedge$ & (conjunción) & $\otimes$ & (tensor) & $\with$ & (with)\\\hline
    $\top$ & (verdadero) & $\One$ & (uno) & $\top$ & (top)\\\hline\hline
    $\vee$ & (disjunción) & $\parr$ & (par) & $\oplus$ & (oplus)\\\hline
    $\bot$ & (falso) & $\bot$ & (bottom) & $\Zero$ & (cero)\\\hline\hline
  \end{tabular}
\end{center}
Implicación lineal: $A\multimap B := \neg A\parr B$

A continuación se detallan las reglas en el formato $\Delta\vdash\Gamma$, que
significa que la conjunción (multiplicativa) de las fórmulas en $\Delta$,
implican la disjunción (multiplicativa) de las fórmulas en $\Gamma$.

\paragraph{Gramática}
\[
  A:= p\mid \neg A\mid A\otimes A\mid A\parr A
  \mid A\with A\mid A\oplus A
  \mid\One\mid\bot
  \mid \top\mid \Zero
\]
Donde $p$ representa una fórmula atómica.

\paragraph{Reglas de indentidad y negación}
\[
  \infer[ax]{A\vdash A}{}
  \qquad
  \infer[cut]{\Delta,\Delta'\vdash\Gamma,\Gamma'}{\Delta\vdash A,\Gamma &
  \Delta',A\vdash\Gamma'}
  \qquad
  \infer[\neg_l]{\Delta,\neg A\vdash\Gamma}{\Delta\vdash A,\Gamma}
  \qquad
  \infer[\neg_r]{\Delta\vdash \neg A,\Gamma}{\Delta,A\vdash\Gamma}
\]
\paragraph{Reglas multiplicativas}
\[
  \infer[\One_l]{\Delta,\One\vdash\Gamma}{\Delta\vdash\Gamma}
  \qquad
  \infer[\One_r]{\vdash\One}{}
  \qquad
  \infer[\otimes_l]{\Delta,A\otimes B\vdash\Gamma}{\Delta,A,B\vdash\Gamma}
  \qquad
  \infer[\otimes_r]{\Delta,\Delta'\vdash A\otimes B,\Gamma,\Gamma'}{\Delta\vdash
  A,\Gamma & \Delta'\vdash B,\Gamma'}
\]
\[
  \infer[\bot_l]{\bot\vdash}{}
  \qquad
  \infer[\bot_r]{\Delta\vdash\bot,\Gamma}{\Delta\vdash\Gamma}
  \qquad
  \infer[\parr_l]{\Delta,\Delta',A\parr B\vdash\Gamma,\Gamma'}{\Delta,A\vdash\Gamma &
  \Delta',B\vdash\Gamma'}
  \qquad
  \infer[\parr_r]{\Delta\vdash A\parr B,\Gamma}{\Delta\vdash A,B,\Gamma}
\]
\paragraph{Reglas aditivas}
\[
  \infer[\Zero_l]{\Delta,\Zero \vdash\Gamma}{}
  \qquad
  \infer[\with_{l1}]{\Delta,A\with B\vdash\Gamma}{\Delta,A\vdash\Gamma}
  \qquad
  \infer[\with_{l2}]{\Delta,A\with B\vdash\Gamma}{\Delta,B\vdash\Gamma}
  \qquad
  \infer[\with_r]{\Delta\vdash A\with B,\Gamma}{\Delta\vdash A,\Gamma & \Delta\vdash B,\Gamma}
\]
\[
  \infer[\top_r]{\Delta\vdash\top,\Gamma}{}
  \qquad
  \infer[\oplus_l]{\Delta,A\oplus B\vdash\Gamma}{\Delta,A\vdash\Gamma
  &\Delta,B\vdash\Gamma}
  \qquad
  \infer[\oplus_{r1}]{\Delta\vdash A\oplus B,\Gamma}{\Delta\vdash A,\Gamma}
  \qquad
  \infer[\oplus_{r2}]{\Delta\vdash A\oplus B,\Gamma}{\Delta\vdash B,\Gamma}
\]

\section{Un ejemplo simple de sistema de tipos lineal}
Supongamos que queremos redefinir lambda cálculo extendido de la
Sección~\ref{sec:calculo}, de manera que los tipos usen la lógica lineal.  Aquí
tenemos algunas deciciones a hacer: ¿usamos aditivos o multiplicativos? O una
mezcla de ambos?  Un resultado conocido es que en lógica intuicionista, no
existe  la disjunción multiplicativa ni el falso multiplicativo, y no existe la
implicación es siempre multiplicativa, por lo que ahí no hay elección posible:
la disjunción y el falso deben ser el aditivos ($\oplus$ y $\Zero$
respectivamente), y la implicación será $\multimap$.  En cambio es posible
tener una conjunción multiplicativa y una aditiva. Si queremos seguir
utilizando el mismo cálculo, deberemos elegir la aditiva, ya que las
proyecciones $\pi_1$ y $\pi_2$ que tenemos en el cálculo no pueden ser
multiplicativas. Por lo tanto, utilizaremos $\&$ para la conjunción.
Finalmente, es posible tener tanto el verdadero aditivo como el multiplicativo.
Elegiremos el multiplicativo.

Con todo esto, definimos el siguiente sistema de tipos (ver la diferencia con
la relación de tipado de la Definición~\ref{def:reglas-de-tipado}).  Es posible
comprobar (y un buen ejercicio), que cada una de las reglas es derivable en el
cálculo de secuentes dado en la sección anterior.
\[
  \infer[\textrm{ax}]{x:A\vdash x:A}{}
  \qquad
  \infer[\One_i]{\vdash\star:\One}{}
  \qquad
  \infer[\One_e]{\Gamma,\Delta\vdash\elimtop tr:A}{\Gamma\vdash t:\One & \Delta\vdash r:A}
  \qquad
  \infer[\Zero_e]{\Gamma\vdash\elimbot t:C}{\Gamma\vdash t:\Zero}
\]
\[
  \infer[\multimap_i]{\Gamma\vdash\lambda x.t:A\multimap B}{\Gamma,x:A\vdash t:B}
  \qquad
  \infer[\multimap_e]{\Gamma,\Delta\vdash tr:B}{\Gamma\vdash t:A\multimap B & \Delta\vdash r:A}
\]
\[
  \infer[\&_i]{\Gamma\vdash\pair tr:A\& B}{\Gamma\vdash t:A & \Gamma\vdash r:B}
  \qquad
  \infer[\&_{e_1}]{\Gamma\vdash\elimandl t:A}{\Gamma\vdash t:A\& B}
  \qquad
  \infer[\&_{e_2}]{\Gamma\vdash\elimandr t:B}{\Gamma\vdash t:A\& B}
\]
\[
  \infer[\oplus_{i_1}]{\Gamma\vdash\inl(t):A\oplus B}{\Gamma\vdash t:A}
  \qquad
  \infer[\oplus_{i_2}]{\Gamma\vdash\inr(t):A\oplus B}{\Gamma\vdash t:B}
\]
\[
  \infer[\vee_e]{\Gamma,\Delta\vdash\elimor txrys:C}{\Gamma\vdash t:A\vee B & \Delta,x:A\vdash r:C & \Delta,y:B\vdash s:C}
\]
Donde $\Gamma\cap\Delta=\emptyset$.

Podríamos agregar términos para los conectivos faltantes. Por ejemplo, podemos agregar $t\otimes r$ para la conjunción multiplicativa, y un término para la eliminación que podría ser
$\mathsf{let}\ t=x\otimes y\ \mathsf{in}\ u$, con la regla de reducción siguiente:
\[
\mathsf{let}\ t\otimes r=x\otimes y\ \mathsf{in}\ u
\lra
(t/x,r/y)u
\]
y sus reglas de tipado:
\[
  \infer[\otimes_i]{\Gamma,\Delta\vdash t\otimes r:A\otimes B}{\Gamma\vdash t:A & \Delta\vdash r:B}
  \qquad
  \infer[\otimes_e]{\Gamma,\Delta\vdash\mathsf{let}\ t=x\otimes y\ \mathsf{in}\ u:C}{\Gamma\vdash t:A\otimes B & \Delta,x:A,y:B\vdash u:C}
\]
 
\begin{ejemplo}
  Dado que la conjunción $\&$ es aditiva, puede duplicar su argumento:
  \[
    \infer[\multimap_i]{\vdash\lambda x.\pair xx:\One\multimap\One\&\One}
    {
      \infer[\&_i]{x:\One\vdash\pair xx:\One\&\One}
      {
	\infer[\One_i]{x:\One\vdash x:\One}{}
	&
	\infer[\One_i]{x:\One\vdash x:\One}{}
      }
    }
  \]
  En cambio, con la conjunción multiplicativa $\otimes$, no es posible, $\lambda x. x\otimes x$ no tiene un tipo, ya que $x:\One\nvdash x\otimes x:\One\otimes\One$.
\end{ejemplo}

\begin{ejercicio}
  Mostrar que las reglas dadas son lógicamente derivables en cálculo de secuentes.
\end{ejercicio}
\begin{ejercicio}
  Completar el lenguaje con términos que se correspondan con el conectivo que dejamos afuera: el verdadero aditivo.
  Dar sus reglas de tipado.
\end{ejercicio}



\part{Semántica denotacional y teoría de categorías}\label{part:SemDen}
\chapter{Teoría de categorías}
\section{Primeras definiciones}
\begin{definicion}\label{def:cat}
  Una categoría $\mathbf C$ se compone de:
  \begin{enumerate}
    \item Una colección de objetos $\Obj(\mathbf C)$.
    \item Una colección de flechas o morfismos $\Arr(\mathbf C)$.
    \item Operaciones que asignan a cada flecha $f$ un objeto $\mathit{dom}\  f$ (su dominio) y un objeto $\mathit{cod}\  f$ (su codominio) (escribimos $f:A\to B$ o $A\xlra f B$ para indicar que $\mathit{dom}\  f=A$ y $\mathit{cod}\  f=B$). A la colección de todas las flechas con dominio $A$ y codominio $B$ la escribimos $\Home[\mathbf C] AB$.
    \item Un operador de composición que asigna a cada par de flechas $A\xlra f B$ y $B\xlra g C$, la flecha composición $A\xlra{f\circ g} C$ que satisface la siguiente ley de asociatividad:
      \[
	\textrm{Para todas las flechas }A\xlra f B, B\xlra g C, C\xlra h D,\qquad h\circ(g\circ f) = (h\circ g)\circ f
      \]
    \item Para cada objeto $A$ existe una flecha identidad $A\xlra{\Id_A} A$ que satisface la ley de indentidad:

      \[
	\textrm{Para toda flecha }A\xlra f B,\qquad 
	\Id_B\circ f=f\textrm{ y }f\circ\Id_A=f
      \]
  \end{enumerate}
\end{definicion}

\begin{ejemplo}\label{ex:Set}
  La categoría $\mathbf{Set}$ tiene como objetos a los conjuntos y como flechas a las funciones totales entre conjuntos. La composición es la composición de funciones, y las flechas identidad son las funciones identidad.
\end{ejemplo}

\begin{ejercicio}
  Verificar que $\mathbf{Set}$ es una categoría de acuerdo a la
  Definición~\ref{def:cat}.
\end{ejercicio}

\begin{ejemplo}\label{ex:Poset}
  Un orden parcial $\leq_P$ en sobre un conjunto $P$ es una relación reflexiva, transitiva y antisimétrica en los elementos de $P$, es decir, es una relación para la cual, para todo $p,p',p''\in P$,
  \begin{enumerate}
    \item $p\leq_P p$,
    \item Si $p\leq_P p'$ y $p'\leq_P p''$, entonces $p\leq_P p''$,
    \item Si $p\leq_P p'$ y $p'\leq_P p$, entonces $p=p'$
  \end{enumerate}
  Si existe un orden parcial $\leq_P$ para un conjunto $P$, decimos que $(P,\leq_P)$ es un conjunto parcialmente ordenado.

  Sean $(P,\leq_P)$ a $(Q,\leq_Q)$ dos conjuntos parcialmente ordenados. Una función $f:P\to Q$ preserva el orden (o es monónona) si $p\leq_P p'$ implica $f(p)\leq_Q f(p')$.

  La categoría $\mathbf{Poset}$ tiene como objetos a los conjuntos parcialmente ordenados y como flechas a las funciones totales que preservan el orden.

  Verificamos que esto satisface cada punto de la Definición~\ref{def:cat}:
  \begin{enumerate}
    \item $\Obj(\mathbf{Poset})$ es la colección de los conjuntos parcialmente ordenados.
    \item $\Arr(\mathbf C)$ es la colección de funciones totales $(P,\leq_P)\xlra f(Q,\leq_Q)$ que preservan el orden.
    \item Para cada función total que preserva el orden $f$ con dominio $P$ y codominio $Q$, tenemos $\mathit{dom}\ f=(P,\leq_P)$, $\mathit{cod}\ f=(Q,\leq_Q)$, y $f\in\Home[\mathbf{Poset}]{(P,\leq_P)}{(Q,\leq_Q)}$.
    \item La composición de dos funciones totales $P\xlra f Q$ y $Q\xlra g R$ es una función total $g\circ f$ de $P$ a $R$. Más aún, si $p\leq_P p'$, como $f$ preserva el orden, tenemos $f(p)\leq_Q f(p')$, y, como $g$ preserva el orden, tenemos $g(f(p))\leq_R g(f(p'))$, por lo tanto, $g\circ f$ preserva el orden y entonces $g\circ f\in\Arr(\mathbf{Poset})$.
      Finalmente, la composición de funciones es asociativa.
    \item Para cada orden parcial $(P,\leq_P)$, tenemos que la función identidad $\Id_P$ preserva el órden y satisface la ley de indentidad.
  \end{enumerate}
\end{ejemplo}

\begin{ejercicio}
  Sea $\mathbf{Poset}$ la categoría del Ejemplo~\ref{ex:Poset}.  Probar que la
  composición de funciones que preservan el orden también preserva el orden, y
  que la identidad de cada conjunto parcialmente ordenado preserva el orden.
  Concluir que $\mathbf{Poset}$ es una categoría de acuerdo a la
  Definición~\ref{def:cat}.
\end{ejercicio}


\begin{ejemplo}
  Un monoide $(M,*,e)$ es un conjunto $M$ equipado con una operación binaria $*$ de un par de elementos de de $M$ en $M$ tal que $(x* y)* z = x* (y* z)$ para todo $x,y,z\in M$, y un elemento distinguido $e$ tal que $e* x=x=x* e$ para todo $x\in M$.
 
  Un homomorfismo de monoides de $(M,*_M,e_M)$ a $(N,*_N,e_N)$ es una función $f:M\to N$ tal que $f(e_M)=e_N$ y $f(x*_M y)=f(x)*_N f(y)$. La composición de dos homomorfismos de monoides es la misma que la composición de funciones en conjuntos.

  La categoría $\mathbf{Mon}$ tiene a los monoides como objetos y  a los homomorfismos de monoides como flechas.
\end{ejemplo}
\begin{ejercicio}
  Verificar que $\mathbf{Mon}$ es una categoría de acuerdo a la definición~\ref{def:cat}.
\end{ejercicio}

\begin{ejemplo}\label{ex:VecCat}
  La categoría $\mathbf{Vec}$ es la categoría cuyos objetos son espacios vectoriales y cuyas flechas son las transformaciones lineales.
\end{ejemplo}
\begin{ejercicio}
  Verificar que $\mathbf{Vec}$ es una categoría de acuerdo a la definición~\ref{def:cat}.
\end{ejercicio}

\begin{ejemplo}\label{ex:ZeroCat}
  La categoría $\mathbf{0}$ es la categoría tal que $\Obj(\mathbf{0})=\emptyset$ y $\Arr(\mathbf{0})=\emptyset$.
\end{ejemplo}

\section{Diagramas}
\begin{definicion}
  Un diagrama en una categoría $\mathbf{C}$ es una colección de vértices y aristas dirigidas, etiquetados consistentemente con objetos y flechas de la categoría $\mathbf{C}$.

  Un diagrama en una categoría $\mathbf{C}$ se dice conmutativo si, para cualquier par de vértices $X$ e $Y$, todos los caminos en el diagrama desde $X$ hasta $Y$ son iguales, en el sentido de que cada camino del diagrama determina una flecha y esas flechas son iguales en $\mathbf{C}$.

  Por ejemplo, en lugar de decir $f\circ g'=g\circ f'$, puedo decir que el siguiente diagrama conmuta:
  \[
    \begin{tikzcd}[labels=description,row sep=1.5cm,column sep=1.5cm]
      X\ar[d,"g'"]\ar[r,"f'"] & Z\ar[d,"g"]\\
      W\ar[r,"f"] & Y
    \end{tikzcd}
  \]
\end{definicion}

\begin{teorema}\label{thm:composicionDeDiagramas}
  Si los dos cuadros internos del siguiente diagrama conmutan, entonces el rectángulo exterior también conmuta.
  \[
    \begin{tikzcd}[labels=description,row sep=1.5cm,column sep=1.5cm]
      A\ar[d,"a"]\ar[r,"f"] & B\ar[d,"b"]\ar[r,"f'"] & C\ar[d,"c"]\\
      A'\ar[r,"g"] & B'\ar[r,"g'"] & C'
    \end{tikzcd}
  \]
\end{teorema}
\begin{proof}
  \begin{align*}
    &(g'\circ g)\circ a\\
    \textrm{(asociatividad)} &=g'\circ (g\circ a)\\
    \textrm{(conmutatividad del primer cuadrado)} &=g'\circ(b\circ f)\\
    \textrm{(asociatividad)} &=(g'\circ b)\circ f\\
    \textrm{(conmutatividad del segundo cuadrado)} &=(c\circ f')\circ f\\
    \textrm{(asociatividad)} &= c\circ (f'\circ f)
  \end{align*}
\end{proof}

\section{Monomorfismos, epimorfismos e isomorfismos}
\begin{definicion}
  \label{def:monomorfismo}
  Una flecha $B\xlra f C$ en una categoría $\mathbf C$ es un monomorfismo si, para cualquier par de flechas $A\xlra g B$ y $A\xlra h B$ de la categoría, la igualdad $f\circ g=f\circ h$ implica que $g=h$.
\end{definicion}
\begin{teorema}
  En $\mathbf{Set}$, los monomorfismos son exactamente las funciones inyectivas (las funciones para las cuales $f(x)=f(y)$ implica $x=y$).
\end{teorema}
\begin{proof}
  Sea $B\xlra f C$ una función inyectiva, y sean $A\xlra g B$, $A\xlra h B$ tales que $f\circ g=f\circ h$, pero $g\neq h$. Entonces hay algún elemento $a\in A$ para el cual $g(a)\neq h(a)$. Pero como $f$ es inyectiva, $f(g(a))\neq f(h(a))$, lo que contradice la suposición de que $f\circ g=f\circ h$. Esto muestra que una función inyectiva es un monomorfismo.

  Sea $B\xlra f C$ un monomorfismo. Si $f$ no es inyectiva, entonces existen dos elementos $b,b'\in B$ tales que $f(b)=f(b')$. Sea $A=\{a\}$ un conjunto de un solo elemento, y $A\xlra g B$ la función $g(a)=b$, mientras que $A\xlra h B$ es la función $h(a)=b'$. Entonces $f(g(a))=f(b)=f(b')=f(h(a))$, lo cual contradice la supocisión de que $f$ es un monomorfismo.
\end{proof}

\begin{definicion}
  \label{def:epimorfismo}
  Una flecha $A\xlra f B$ en una categoría $\mathbf C$ es un epimorfismo si, para cualquier par de flechas $B\xlra g C$ y $B\xlra h C$, la igualdad $g\circ f=h\circ f$.
\end{definicion}
\begin{teorema}\label{thm:epis}
  En $\mathbf{Set}$, los epimorfismos son exactamente las funciones sobreyectivas (las funciones $f:A\to B$ para las cuales para cada $b\in B$ existe un $a\in A$ tal que $f(a)=b$).
  \qed
\end{teorema}
\begin{ejercicio}
Probar el Teorema~\ref{thm:epis}, es decir, que en $\mathbf{Set}$ los
epimorfismos son exactamente las funciones sobreyectivas.
\end{ejercicio}

\begin{definicion}
  \label{def:isomorfismo}
  Una flecha $A\xlra f B$ es un isomorfismo si existe una flecha $B\xlra{f^{-1}} A$, llamada inversa de $f$, tal que $f^{-1}\circ f=\Id_A$ y $f\circ f^{-1}=\Id_B$. Los objetos $A$ y $B$ se dicen isomorfos si hay un isomorfismo entre ellos.
\end{definicion}
\begin{teorema}
  En $\mathbf{Set}$, los isomorfismos son exactamente las funciones biyectivas (eso es, inyectivas y sobreyectivas a la vez).\qed
\end{teorema}

\section{Algunas construcciones universales a todas las categorías}
\subsection{Objetos iniciales y terminales}
\begin{definicion}
  Un objeto $0$ es llamado objeto inicial si, para todo objeto $A$, existe exactamente una flecha desde $0$ a $A$.
\end{definicion}
\begin{definicion}
  De forma dual, un objeto $1$ es llamado objeto terminal si, para todo objeto $A$, existe exactamente una flecha desde $A$ en $1$.
\end{definicion}

\begin{ejemplo}
  En $\mathbf{Set}$ el objeto inicial es el conjunto vacío $\emptyset$. Para todo conjunto $A$, la función vacía es la única función de $\emptyset$ en $A$. Todo conjunto de un sólo elemento es un objeto terminal, ya que para todo conjunto $A$ existe una única función de $A$ a $\{x\}$ que mapea todos los elementos de $A$ en $x$.
\end{ejemplo}

\begin{ejemplo}
  Los objetos terminales pueden ser usados para proveer un análogo en categorías al concepto de elementos de conjuntos. La observación que motiva esto es que, en la categoría $\mathbf{Set}$, las funciones desde un conjunto de un sólo elemento a un conjunto $A$ están en correspondencia uno-a-uno con los elementos de $A$. Más aún, si $x\in A$, considerada como una flecha $1\xlra x A$ desde algún conjunto de un sólo elemento $1$, y $f$ es una función de $A$ a algún otro conjunto $B$, entonces el elemento $f(x)$ es el único elemento de $B$ tal que es la imagen de la composición $f\circ x$.

  En términos categóricos, una flecha desde un objeto terminal a un objeto $A$ se llama elemento global o constante de $A$.
\end{ejemplo}

\subsection{Productos}
\begin{definicion}
  Un producto de dos objetos $A$ y $B$ es un objeto $A\times B$, que viene con dos flechas proyección $A\times B\xlra{\pi_1} A$ y $A\times B\xlra{\pi_2}B$ tales que para todo objeto $C$ y par de flechas $C\xlra f A$ y $C\xlra g B$, existe exactamente una flecha $C\xlra{\pair fg} A\times B$ que hace que el siguiente diagrama conmute.
  \[
    \begin{tikzcd}[labels=description,row sep=2cm,column sep=3cm]
      A & A\times B\ar[l,"\pi_1"]\ar[r,"\pi_2"] & B\\
      & C\ar[lu,"f",sloped]\ar[u,"\pair fg",dashed]\ar[ru,"g",sloped]
    \end{tikzcd}
  \]
  Es decir, $\pi_1\circ\pair fg=f$ y $\pi_2\circ\pair fg=g$.
\end{definicion}
Las aristas punteadas en un diagrama se usan para representar flechas que se afirma que existen cuando el resto del diagrama se completa apropiadamente.

\begin{ejercicio}
  Mostrar que cualquier objeto $X$ con flechas $X\xlra{\pi_A} A$ y
  $X\xlra{\pi_B} B$ que satisface la definición de ``$X$ es un producto de $A$ y
  $B$'' es isomorfo a $A\times B$.  De la misma manera, mostrar que cualquier
  objeto isomorfo al objeto $A\times B$ es un producto de $A$ y $B$.
\end{ejercicio}

\begin{definicion}
Si $A\times C$ y $B\times D$ son productos, entonces para todo par de flechas $A\xlra f B$ y $C\xlra g D$, el mapa producto $A\times C\xlra{f\times g} B\times D$ es la flecha $\pair{f\circ\pi_1}{g\circ\pi_2}$.
\end{definicion}
\begin{observacion}
Notar que hay dos productos en juego en la definición anterior y sin embargo hablamos de $\pi_1$ y $\pi_2$ sin decir a cuál nos referíamos. Esto se puede hacer ya que la teoría de categorías es un formalismo ``fuertemente tipado'', en el sentido de que sólo un par de proyecciones puede ir allí, para que la flecha tenga sentido.
\end{observacion}

La noción dual al producto, el coproducto, corresponde, en teoría de conjuntos, a la unión disjunta.
\begin{definicion}
  Un coproducto de dos objetos $A$ y $B$ es un objeto $A+B$, que viene con dos flechas de inyección $A\xlra{i_1}A+B$ y $B\xlra{i_2}A+B$ tales que para todo objeto $C$ y par de flechas $A\xlra f C$ y $B\xlra g C$, existe exactamente una flecha $A+B\xlra{\coprodu fg} C$ que hace que el siguiente diagrama conmute.
  \[
    \begin{tikzcd}[labels=description,row sep=2cm,column sep=3cm]
      A\ar[r,"i_1"]\ar[rd,"f",sloped] & A+B\ar[d,"\coprodu fg"] & B\ar[l,"i_2"]\ar[ld,"g",sloped]\\
      & C
    \end{tikzcd}
  \]
  Es decir, $\coprodu fg\circ i_1=f$ y $\coprodu fg\circ i_2=g$.
\end{definicion}
\begin{ejercicio}
  Sean $f,g,h,k$ flechas en una categoría con productos.  Probar las siguientes
  igualdades:
  \begin{enumerate}
    \item $\pair{f\circ h}{g\circ h} = \pair f g \circ h$.
    \item $(f\times h)\circ\pair gk = \pair{f\circ g}{h\circ k}$.
    \item $(f\times h)\circ(g\times k) = (f\circ g)\times(h\circ k)$.
  \end{enumerate}
\end{ejercicio}

\subsection{Curryficación}
Dado que si $A$ y $B$ son conjuntos, $\Home AB$ es un conjunto, tenemos que $\Home AB\in\Obj(\mathbf{Set})$.
Sin embargo, no sucede en toda categoría $\mathbb C$ que dados dos objetos $A$ y $B$ en la categoría, $\Home[\mathbb C] AB$ sea un objeto de la categoría. Por ejemplo, eso no sucede en la categoría $\mathbf{Mon}$.

En las categorías que sí admiten considerar a $\Home[\mathbb C] AB$ como un objeto de la categoría, vamos a identificar ase objeto como $\home AB$, y vamos a dar una caracterización categórica del objeto (lo cual permitirá probar que el objeto pertenece a la categoría).

\begin{definicion}[Objeto exponencial]\label{def:objetoExpo}
  Sea $\mathbb C$ una categoría con productos y sean $A$ y $B$ objetos de la categoría.
  Un objeto $\home AB$ es un objeto exponencial si existe una flecha $\home AB\times A\xlra{\mathsf{eval}_{AB}} B$ tal que para todo objeto $C$ y flecha $C\times A\xlra g B$ hay una única flecha $\mathsf{curry}(g):C\to\home AB$ que hace que el siguiente diagrama conmute.
  \[
    \begin{tikzcd}[labels=description,row sep=2cm,column sep=3cm]
      \home AB\times A\ar[r,"\mathsf{eval}_{AB}"] & B\\
      C\times A\ar[ru,"g",sloped]\ar[u,dashed,"\mathsf{curry}(g)\times\Id_A"]
    \end{tikzcd}
  \]
Es decir, una única flecha $\mathsf{curry}(g)$ tal que
\[
  \mathsf{eval}_{AB}\circ(\mathsf{curry}(g)\times\Id_A) = g
\]
\end{definicion}
Una forma intuitiva de entender la definición anterior, es evaluando los mapas. En este caso quedaría de la siguiente manera:
\[
  \begin{tikzcd}[labels=description,column sep=3cm]
    (\mathsf{curry}(g)(c),a)\ar[r,mapsto,"\mathsf{eval}_{AB}"] & (\mathsf{curry}(g)(c))(a)\\
    & g(c,a)\ar[u,equal]\\
    (c,a)\ar[ru,mapsto,"g",sloped]\ar[uu,dashed,mapsto,"\mathsf{curry}(g)\times\Id"]
  \end{tikzcd}
\]

\begin{ejercicio}
  En la categoría $\mathbf{Set}$, considerar el conjunto $\home AB$ de todas las
  funciones de $A$ en $B$, y la aplicación de evaluación
  $\mathsf{eval}_{AB} : \home AB\times A \to B$, dada por
  $\mathsf{eval}_{AB}(f,a) = f(a)$.
  \begin{enumerate}
    \item Probar que $\home AB$ con $\mathsf{eval}_{AB}$ satisface la
      Definición~\ref{def:objetoExpo} de objeto exponencial.
    \item Describir explícitamente, para un conjunto $C$ y una función
      $g : C\times A \to B$, cuál es la función $\mathsf{curry}(g):C\to\home AB$.
  \end{enumerate}
\end{ejercicio}

\begin{definicion}[CCC]
  Una categoría cartesiana cerrada (CCC) es una categoría con objeto terminal, productos y exponenciación.
\end{definicion}

\begin{ejercicio}
  Mostrar que la categoría $\mathbf{Set}$, con objeto terminal cualquier conjunto
  de un solo elemento, producto cartesiano usual y objetos exponenciales
  $\home AB$, es una categoría cartesiana cerrada.
\end{ejercicio}
\section{Functores, transformaciones naturales y adjunciones}
\subsection{Functores}
\begin{definicion}
  Sean $\mathbf C$ y $\mathbf D$ dos categorías. Un functor $F:\mathbf C\to\mathbf D$ es un mapa que lleva cada objeto $A\in\Obj(\mathbf C)$ a un objeto $F(A)\in\Obj(\mathbf D)$, y cada flecha $A\xlra f B\in\Arr(\mathbf C)$ a una flecha $F(A)\xlra{F(f)}F(B)\in\Arr(\mathbf D)$ de manera tal que para todos los objetos $A\in\Obj(\mathbf C)$ y todas las flechas componibles $f,g\in\Arr(\mathbf C)$ se tiene
  \begin{enumerate}
    \item $F(\Id_A) = \Id_{F(A)}$
    \item $F(g\circ f) = F(g)\circ F(f)$
  \end{enumerate}
\end{definicion}

\begin{ejemplo}
  Dado un conjunto $S$ podemos armar el conjunto $\mathsf{List}(S)$ de listas finitas de elementos de $S$. 

  Esto define el mapa $\mathsf{List}$ que es la parte que actúa sobre objetos de un functor de $\mathbf{Set}$ en $\mathbf{Set}$.

  La parte que actúa sobre las flechas lleva la flecha $S\xlra f S'$ a una función $\mathsf{List}(S)\xlra{\mathsf{List}(f)}\mathsf{List}(S')$ que, dada una lista $L = {[s_1,\dots,s_n]}$ mapea a la $f$ sobre cada uno de los elementos:
  \[
    \mathsf{List}(f)(L) = {[f(s_1),\dots,f(s_n)]}
  \]
\end{ejemplo}

\begin{ejercicio}
  Sea $S\xlra f S'$ una función entre conjuntos.  Probar que $\mathsf{List}(f)$
  satisface las dos condiciones de functor:
  \begin{enumerate}
    \item $\mathsf{List}(\Id_S) = \Id_{\mathsf{List}(S)}$.
    \item $\mathsf{List}(g\circ f) = \mathsf{List}(g)\circ\mathsf{List}(f)$ para
      toda función $S'\xrightarrow g S''$.
  \end{enumerate}
\end{ejercicio}

\begin{ejemplo}\label{ej:FunctorListMon}
  Es facil ver que para cualquier conjunto $S$, $(\mathsf{List}(S),\concat,[])$ es un monoide, donde $\concat$ es la concatenación de listas y $[]$ es la lista vacía. En efecto, $\concat$ es cerrada en listas, es asociativa, y $[]$ es su elemento neutro.

  Por lo tanto, podemos considerar a $\mathsf{List}$ como un fucntor de $\mathbf{Set}$ en $\mathbf{Mon}$. La parte que actúa sobre los elementos lleva cada conjunto $S$ al monoide de listas con elementos de $S$. La parte que actúa sobre las flechas lleva una función $f$ a un homomorfismo de monoides $\mathsf{List}(f) = \mathit{maplist}(f)$, cuya definición recursiva es
  \begin{align*}
    \mathit{maplist}(f)([]) &= []\\
    \mathit{maplist}(f)(L\concat L') &= \mathit{maplist}(f)(L)\concat\mathit{maplist}(f)(L')\\
    \mathit{maplist}(f)([s]) &= [f(s)]
  \end{align*}
  Notar que las dos primeras líneas de esta definición, prueban que $\mathit{maplist}(f)$ es un homomorfismo de monoides.

  $\mathsf{List}(S)$ es usualmente llamado el monoide libre generado por $S$.
\end{ejemplo}
\begin{ejercicio}
  Mostrar que la definición de $\mathit{maplist}$ dada en el Ejemplo~\ref{ej:FunctorListMon}
  es equivalente a la definición más usual
  \begin{align*}
    \mathit{maplist}(f)([]) &= []\\
    \mathit{maplist}(f)(s:L) &= [f(s)]\concat\mathit{maplist}(f)(L).
  \end{align*}
\end{ejercicio}

\begin{ejercicio}
  Sea $\mathcal P(S)$ el conjunto de todas las partes (subconjuntos) de un
  conjunto $S$.  Mostrar que el operador de partes se puede extender a un
  functor $\mathcal P:\mathbf{Set}\to\mathbf{Set}$, definiendo para cada
  función $S\xlra f T$ una función
  $\mathcal P(f):\mathcal P(S)\to\mathcal P(T)$ de manera adecuada, y
  verificando las leyes de functor.
\end{ejercicio}

\begin{ejemplo}\label{ej:FunctorProd}
  Sea $\mathbf{C}$ una categoría con producto $X\times Y$ para cada par de objetos $X$ e $Y$.
  Entonces, cada objeto $A\in\Obj(\mathbf C)$ determina un functor $(\mbox{---}\times A):\mathbf C\to\mathbf C$ que lleva cada objeto $B$ a $B\times A$ y cada flecha $B\xlra f C$ a $f\times\Id_A$.

  El $\mbox{---}$ se usa para decir a dónde se coloca el argumento. Es lo que en cálculo lambda escribiríamos $\lambda x.x\times A$.
\end{ejemplo}

\begin{ejercicio}
  Sea $\mathbf C$ una categoría con productos y $A\in\Obj(\mathbf C)$.  Probar
  que el mapeo $(\mbox{---}\times A):\mathbf C\to\mathbf C$ del
  Ejemplo~\ref{ej:FunctorProd} satisface las leyes de functor:
  \[
    (\mbox{---}\times A)(\Id_B) = \Id_{B\times A},\qquad
    (\mbox{---}\times A)(g\circ f) = (\mbox{---}\times A)(g)\circ(\mbox{---}\times A)(f).
  \]
\end{ejercicio}

\subsection{Transformaciones naturales}
En la sección anterior definimos mapas de una categoría a otra (functores). Ahora vamos a definir mapas que preservan estructuras, llamadas transformaciones naturales, de un functor a otro.

Intuitivamente, la idea es la siguiente. Dados dos functores $F:\mathbf{C}\to\mathbf{D}$ y $G:\mathbf{C}\to\mathbf{D}$, podemos pensarlos como que cada uno de ellos proyecta un retrato de $\mathbf C$ en $\mathbf D$. Las transformaciones naturales surgen cuando imaginamos desplazar el retrato definido por $F$ al retrato definido por $G$.
Para cada objeto $A\in\Obj(\mathbf C)$, definimos una flecha $\eta_A$ de la imagen-$F$ en la imagen-$G$ de $A$. Para asegurar que la estructura de $F$ es preservada por esta transformación, requerimos que para cada flecha $A\xlra f B$ en $\mathbf C$, las transformaciones $\eta_A$ y $\eta_B$ lleven los puntos de la imagen-$F$ de $f$ a los puntos de la imagen-$G$ de $f$.
Luego de esta intuición, damos la definición formal:

\begin{definicion}
  Sean $\mathbf C$ y $\mathbf D$ dos categorías y sean $F$ y $G$ functores de $\mathbf C$ a $\mathbf D$. Una transformación natural $\eta$ de $F$ a $G$, notada $\eta:F\xlra\cdot G$, es una función que asigna a cada elemento $A\in\Obj(\mathbf C)$ una flecha $ F(A)\xlra{\eta_A}G(A)\in\Arr(\mathbf D)$ tal que para cada flecha $A\xlra f B\in\Arr(\mathbf C)$, el siguiente diagrama conmute en $\mathbf D$:
  \[
    \begin{tikzcd}[labels=description,column sep=2cm,row sep=2cm]
	F(A)\ar[r,"\eta_A"]\ar[d,"F(f)"] & G(A)\ar[d,"G(f)"]\\
	F(B)\ar[r,"\eta_B"] & G(B)
    \end{tikzcd}
  \]
  Si cada componente $\eta_A$ de $\eta$ es un isomorfismo en $\mathbf D$, entonces a $\eta$ se le llama isomorfismo natural.
\end{definicion}

\begin{ejemplo} 
  Para cada functor $F$, las componentes de la transformación natural identidad $\iota_F:F\xlra{\cdot} F$ son las flechas identidad de los objetos en la imagen de $F$, es decir, $\iota_F = \Id_{F(A)}$. Más aún, $\iota_F$ es un isomorfismo natural.
\end{ejemplo}

\begin{ejercicio}
  Demostrar que efectivamente
  $\iota_F:F\xlra\cdot F$
  es un isomorfismo natural.
\end{ejercicio}

\begin{ejemplo}
  Sea $\mathit{rev}$ la función que da vuelta listas, es decir, $\mathit{rev}_S:\mathsf{List}(S)\to\mathsf{List}(S)$ toma cualquier lista de $S$ y la da vuelta. Por ejemplo, 
  \[
    \mathit{rev}_{\mathbb N}[5,6,7] = [7,6,5]
  \]

  Este es un ejemplo de función polimórfica: opera exactamente igual, sin importar los elementos que componen la lista. Si le damos otros tres números, $\mathit{rev}_{\mathbb N}$ hace exactamente lo mismo que antes:
  \[
    \mathit{rev}_{\mathbb N}[6,7,8] = [8,7,6]
  \]
  En efecto, podemos aplicar cualquier mapeo a los elementos individuales del argumento de $\mathit{rev}$, incluso uno que cambie su tipo. Si $\mathbb N\xlra f S$, entonces
  \[
    \mathit{maplist}(f)(\mathit{rev}_{\mathbb N}[5,6,7]) = [f(7),f(6),f(5)]
  \]
  En general, si $S\xlra f T$, entonces
  \[
    \mathit{rev}_T\circ\mathit{maplist}(f) = \mathit{maplist}(f)\circ\mathit{rev}_S
  \]
  Y esto es precisamente lo que dice que $\mathit{rev}$ es una transformación natural.
\end{ejemplo}

\begin{ejercicio}
  Sea $\mathsf{List}:\mathbf{Set}\to\mathbf{Set}$ el functor de listas finitas y
  sea, para cada conjunto $S$, la función
  $\mathit{rev}_S:\mathsf{List}(S)\to\mathsf{List}(S)$ que da vuelta una lista.
  Mostrar que la familia $(\mathit{rev}_S)_S$ define una transformación natural
  $\mathit{rev}:\mathsf{List}\xlra\cdot\mathsf{List}$, es decir, que para toda
  función $S\xlra f T$ el siguiente diagrama conmuta:
  \[
    \begin{tikzcd}[labels=description,column sep=2cm,row sep=2cm]
      \mathsf{List}(S)\ar[r,"\mathit{rev}_S"]\ar[d,"\mathsf{List}(f)"'] &
      \mathsf{List}(S)\ar[d,"\mathsf{List}(f)"]\\
      \mathsf{List}(T)\ar[r,"\mathit{rev}_T"'] & \mathsf{List}(T)
    \end{tikzcd}
  \]
\end{ejercicio}

\begin{ejemplo}\label{ej:FunctorHom}
  Dado un conjunto fijo $A$, el mapa que lleva $B$ a $\home AB\times A$ puede ser extendido a un functor $F_A:\mathbf{Set}\to\mathbf{Set}$ como sigue:
  \begin{align*}
    F_A:\mathbf{Set}&\to\mathbf{Set}\\
    B &\mapsto\home AB\times A\\
    (B\xlra f C) &\mapsto(f\circ\mbox{---})\times\Id_A
  \end{align*}
  Para que se entienda el $F_A(f)$ consideremos el siguiente diagrama
  \[
    \begin{tikzcd}[labels=description,row sep=2cm,column sep=2cm]
      B\ar[d,"f"]\ar[r,"F_A"] & \home AB\times A\ar[d,"F_A(f) = (f\circ\mbox{---})\times\Id_A"]\\
      C\ar[r,"F_A"] & \home AC\times A 
    \end{tikzcd}
  \]
  A nivel de elementos el mapeo queda como sigue:
  \[
    \begin{tikzcd}[labels=description,row sep=2cm,column sep=2cm]
      b\ar[d,"f",mapsto]\ar[r,"F_A",mapsto] & (a\mapsto b,a)\ar[d,"F_A(f) = (f\circ\mbox{---})\times\Id_A",mapsto]\\
      f(b)\ar[r,"F_A"] & (a\mapsto f(b),a)
    \end{tikzcd}
  \]

  El hecho de que $\mathsf{eval}:F_A\xlra{\cdot} I_{\mathbf{Set}}$ es una transformación natural se demuestra con el siguiente diagrama conmutativo.
  \[
    \begin{tikzcd}[labels=description,row sep=2cm,column sep=2cm]
    F_A(C) = \home AC\times A\ar[r,"\mathsf{eval}_{AC}"]\ar[d,"F_A(g) = (g\circ\mbox{---})\times\Id_A"] & C=I_{\mathbf{Set}}(C)\ar[d,"g=I_{\mathbf{Set}}(g)"]\\
    F_A(B) = \home AB\times A\ar[r,"\mathsf{eval}_{AB}"] & B=I_{\mathbf{Set}}(B)
    \end{tikzcd}
  \]
\end{ejemplo}

\begin{ejercicio}
  En el Ejemplo~\ref{ej:FunctorHom} se afirma que el siguiente diagrama
  conmuta, mostrando que $\mathsf{eval}:F_A\xlra\cdot I_{\mathbf{Set}}$ es una
  transformación natural:
  \[
    \begin{tikzcd}[labels=description,row sep=2cm,column sep=2cm]
      F_A(C) = \home AC\times A\ar[r,"\mathsf{eval}_{AC}"]\ar[d,"F_A(g)"'] &
      C = I_{\mathbf{Set}}(C)\ar[d,"g"]\\
      F_A(B) = \home AB\times A\ar[r,"\mathsf{eval}_{AB}"'] &
      B = I_{\mathbf{Set}}(B)
    \end{tikzcd}
  \]
  Completar la demostración verificando explícitamente la conmutatividad del
  diagrama, es decir, que $g\circ\mathsf{eval}_{AC} =
  \mathsf{eval}_{AB}\circ F_A(g)$.
\end{ejercicio}


\begin{teorema}
  La composición de dos transformaciones naturales es una transformación natural.
\end{teorema}
\begin{proof}
  Sean $\mathbf C$ y $\mathbf D$ dos categorías, y sean $F$, $G$ y $H$ functores de $\mathbf C$ a $\mathbf D$.
  Sea $\sigma:F\xlra\cdot G$ y $\tau:G\xlra\cdot H$ dos transformaciones naturales.

  Entonces, para cada flecha $A\xlra f B\in\Arr(\mathbf C)$ podemos hacer el siguiente diagrama
  \[
    \begin{tikzcd}[labels=description,row sep=2cm,column sep=2cm]
      F(A)\ar[d,"F(f)"] \ar[r,"\sigma_A"] & G(A)\ar[r,"\tau_A"]\ar[d,"G(f)"] & H(A)\ar[d,"H(f)"]\\
      F(B)\ar[r,"\sigma_B"] & G(B)\ar[r,"\tau_B"] & H(B)
    \end{tikzcd}
  \]
  Ambos diagramas conmutan, ya que $\sigma$ y $\tau$ son transformaciones naturales, por lo cual el rectángulo exterior conmuta (Teorema~\ref{thm:composicionDeDiagramas}). Esto muestra que la transformación $(\tau\circ\sigma):F\xlra\cdot H$ definida como $(\tau\circ\sigma)_A=\tau_A\circ\sigma_A$ es natural.
\end{proof}

\begin{ejercicio}
  Sean $F,G,H:\mathbf C\to\mathbf D$ functores y sean
  $\sigma:F\xlra\cdot G$ y $\tau:G\xlra\cdot H$ transformaciones naturales.
  Describir explícitamente la componente $(\tau\circ\sigma)_A$ de la
  composición, para un objeto $A\in\Obj(\mathbf C)$, y verificar que la familia
  $(\tau\circ\sigma)_A$ cumple el diagrama de naturalidad.
\end{ejercicio}

\subsection{Adjunciones}
\begin{definicion}[Adjunción]\label{def:adjuncion}
  Una adjunción consiste en
%  \begin{itemize}
   % \item 
  un par de categorías $\mathbf C$ y $\mathbf D$ y
%    \item
  un par de functores $F:\mathbf C\to\mathbf D$ y $G:\mathbf D\to\mathbf C$,
%    \item una transformación natural $\eta:I_{\mathbf C}\xlra\cdot(G\circ F)$, llamada unidad, y
%    \item una transformación natural $\varepsilon:(F\circ G)\xlra\cdot I_{\mathbf D}$, llamada co-unidad;
%  \end{itemize}
  tales que para cada objeto $X\in\Obj(\mathbf C)$ 
  %y flecha $X\xlra f G(Y)\in\Arr(\mathbf C)$ hay una única flecha $F(X)\xlra{f^\sharp}Y$ tal que el siguiente diagrama conmuta:
%  \[
%    \begin{tikzcd}[labels=description,column sep=2cm,row sep=2cm]
%      X\ar[dr,"f"]\ar[r,"\eta_X"] & G(F(X))\ar[d,"G(f^\sharp)",dashed]\\
%      & G(Y)
%    \end{tikzcd}
%  \]
  y para cada objeto $Y\in\Obj(\mathbf D)$
%  y flecha $F(X)\xlra g Y\in\Arr(\mathbf D)$ hay una única flecha $X\xlra{g^*} G(Y)$ tal que el siguiente diagrama conmuta:
%  \[
%    \begin{tikzcd}[labels=description,column sep=2cm,row sep=2cm]
%      F(G(Y))\ar[r,"\varepsilon_Y"] & Y\\
%      F(X)\ar[u,"F(g^*)",dashed]\ar[ur,"g"]
%    \end{tikzcd}
%  \]
%  Decimos que $(F,G)$ es un par adjunto de functores, $F$ es el adjunto a izquierda y $G$ el adjunto a derecha. Se suele notar $F\dashv G$ o $(F,\eta)\dashv (G,\varepsilon)$. Esto también se suele presentar con el siguiente diagrama:
%  \[
%    \begin{tikzcd}[labels=description,column sep=2cm,row sep=1.5cm]
%      X\ar[r,"F"]\ar[d,"f",dashed] & F(X)\ar[d,"g",dashed]\\
%      G(Y) & Y\ar[l,"G"]
%    \end{tikzcd}
%  \]
%  
%
%  Una definición alternativa de adjunción es $F\dashv G$ si y sólo si 
  existe un isomirfismo 
  \[
    \Home[\mathbf C]{F(X)}Y\simeq\Home[\mathbf D]X{G(Y)}
  \]
  que es natural en $X$ e $Y$. Eso es, una transformación natural de dos variables entre $\Home[\mathbf C]{F(\mbox{---})}{\mbox{---}}$ y $\Home[\mathbf D]{\mbox{---}}{G(\mbox{---})}$ que preserve la estructura, ya que $X$ e $Y$ varían y es una bijección para todo $X$ e $Y$.

  Decimos que $(F,G)$ es un par adjunto de functores, $F$ es el adjunto a izquierda y $G$ el adjunto a derecha. Se suele notar $F\dashv G$ o $(F,\eta)\dashv (G,\varepsilon)$. 
\end{definicion}

\begin{ejercicio}
  Sea una adjunción $F\dashv G$ entre las categorías $\mathbf C$ y $\mathbf D$,
  dada por el isomorfismo natural
  \[
    \Phi_{X,Y}:\Home[\mathbf D]{F(X)}Y \;\simeq\; \Home[\mathbf C]X{G(Y)}.
  \]

  \begin{enumerate}
    \item Dado un morfismo $f:F(X)\to Y$ en $\mathbf D$, notemos por
      $f^\sharp$ su imagen $\Phi_{X,Y}(f):X\to G(Y)$.
      De la misma manera, para un morfismo $g:X\to G(Y)$ en $\mathbf C$,
      notemos por $g^*$ al morfismo $\Phi_{X,Y}^{-1}(g):F(X)\to Y$.

      Demostrar que para todo $f$ y todo $g$ se cumple
      \[
	(f^\sharp)^* = f
	\qquad\text{y}\qquad
	(g^*)^\sharp = g.
      \]

    \item Usando únicamente la naturalidad del isomorfismo,
      mostrar que para toda flecha $u:X'\to X$ en $\mathbf C$
      y toda flecha $v:Y\to Y'$ en $\mathbf D$,
      se verifican las igualdades
      \[
	(v\circ f)^\sharp
	= G(v)\circ f^\sharp,
	\qquad
	(g\circ u)^*
	= g^*\circ F(u).
      \]

  \end{enumerate}
\end{ejercicio}

\begin{ejemplo}
  Sea $B\in\Obj(\mathbf{Set})$ y sea $\mbox{---}\times B:\mathbf{Set}\to\mathbf{Set}$ el functor producto introducido en el Ejemplo~\ref{ej:FunctorProd} definido como
  \begin{align*}
    \mbox{---}\times B:\mathbf{Set}&\to\mathbf{Set}\\
    A&\mapsto B\times A\\
    (A\xlra f C) &\mapsto f\times\Id_{B}
  \end{align*}
  y sea $\home B{\mbox{---}}:\mathbf{Set}\to\mathbf{Set}$ el functor definido como
  \begin{align*}
    \home B{\mbox{---}}:\mathbf{Set}&\to\mathbf{Set}\\
    C &\mapsto\home BC\\
    (C\xlra f D) &\mapsto f\circ\mbox{---} 
  \end{align*}
  Entonces existe se tiene una adjunción
  $\mbox{---}\times B\dashv\home B{\mbox{---}}$, es decir, existe un isomorfismo
  \[
    \Home[\mathsf{Set}]{A\times B}C\simeq\Home[\mathsf{Set}]A{\home BC}
  \]
  que es natural en $A$ y $C$.

  Para ver esto, primero mostramos que el mapa $\mathsf{curry}$ de la Definición~\ref{def:objetoExpo} es una bijección:
  \begin{itemize}
    \item Supongamos que $\mathsf{curry}(g)=\mathsf{curry}(h)$ para fos funciones $g,h:(C\times A)\to B$. Entonces $g=\mathsf{eval}_{AB}\circ(\mathsf{curry}(g)\times\Id_A) =\mathsf{eval}_{AB}\circ(\mathsf{curry}(h)\times\Id_A)=h$. Por lo tanto $\mathsf{curry}$ es inyectiva.
    \item Sea $g':C\to\home AB$, y definamos $g=\mathsf{eval}_{AB}\circ(g'\times\Id_A)$. Por la unicidad de $\mathsf{curry}(g)$, tenemos que $\mathsf{curry}(g)=g'$, por lo tanto también es sobreyectiva.
  \end{itemize}
  Esta biyección se suele marcar como
  \[
    \infer{C\times A\to B}{C\to\home AB}
  \]
  Luego mostramos que $\mathsf{curry}$ es una transformación natural, es decir, que el siguiente mapa conmuta:
  \[
    \begin{tikzcd}[labels=description,column sep=2cm,row sep=1.5cm]
      \home{A\times B}C\ar[r,"\mathsf{curry}"]\ar[d,"\home{f\times\Id_B}g"] &\home A{\home BC}\ar[d,"\home f{\home{\Id_B}g}"]\\
      \home{A'\times B}{C'}\ar[r,"\mathsf{curry}"] &\home{A'}{\home B{C'}}
    \end{tikzcd}
  \]
  Esto lo podemos comprobar evaluando los mapas:
  \[
    \begin{tikzcd}[labels=description,column sep=2cm,row sep=1.5cm]
      h\ar[mapsto,rr,"\mathsf{curry}"]\ar[mapsto,d,"\home{f\times\Id_B}g"] &&\mathsf{curry}(h)\ar[mapsto,d,"\home f{\home{\Id_B}g}"]\\
      g\circ h\circ (f\times\Id_B)\ar[mapsto,r,"\mathsf{curry}"] &\mathsf{curry}(g\circ h\circ (f\times\Id_B))\ar[r,equal,dashed,red] & g\circ\mathsf{curry}(h)\circ f
    \end{tikzcd}
  \]
  Donde la igualdad en rojo se justifica como sigue. Sea $a\in A$ y $b\in B$,
  \begin{align*}
    ((g\circ\mathsf{curry}(h)\circ f)(a))(b) &= g(\mathsf{curry}(h)(f(a)))(b)\\
    &= g(h(f(a),b)) \\
    &=g(h(f\times \Id_B)(a,b))\\
    &= (\mathsf{curry}(g\circ h\circ(f\times\Id_B))(a))(b)\\
    &= (\mathsf{curry}(g\circ h\circ(f\times\Id_B))(a))(b)
  \end{align*}
\end{ejemplo}

%\section{Algunos mapas últiles de la categoría \texorpdfstring{$\mathbf{Set}$}{Set}}
%\hl{TO-DO}
% Propiedades a enumerar:
% Naturalidad de $\delta$
% Naturalidad de $\eta^A$
% Axioma de adjunción $\varepsilon\circ\eta^A\times\Id=\Id$
\chapter{Semántica denotacional (categórica)}\label{sec:SemDen}
\section{Primeras definiciones}
\begin{description}
  \item[Sintaxis (o gramática)]: Cómo escribir los términos. Cuáles son válidos y cuáles no.
  \item[Semántica:] Qué significan.
\end{description}
\begin{definicion}
  [Semántica] La semántica de un lenguaje es una relación $\hookrightarrow$ que
  a cada expresión le asocia \emph{algo} que le da significado.
\end{definicion}
%
\paragraph{Semántica denotacional (en programas deterministas).} Para cada
programa $p$, la relación entre las entradas y las salidas de $p$ es una función
que escribimos $\sem p$. La relación se define entonces como
\[
  p,e\hookrightarrow s\iff\sem p e = s
\]
%
La pregunta es, obviamente, cómo definir \sem p. (Y ese es el tópico de este capítulo).
%
\paragraph{Semántica operacional a grandes pasos}
También llamada semántica operacional a grandes pasos o semántica natural.
Consiste en dar una {\it definición inductiva} de $\hookrightarrow$ que nos relacione
un término con su valor. Por ejemplo, 
\[
  \underbrace{(\lambda x.\elimor{\inl(\star)}y{y;x}zz)\star}_{\textrm{Significado de esta
  expresión: $\star$}}\hookrightarrow \star
\]
En ese ejemplo damos semántica de acuerdo a lo que calcula. Una definición inductiva para esta relación es lo que se conoce como ``Intérprete'' (ver, por ejemplo, \cite[Capítulo 3]{DowekLevy11}).
%
Así, si considero
\[
  (\lambda x.\elimor{\inl(\star)}y{x;y}zz)\star \textrm{\quad y\quad} (\lambda x.\elimor{\inl(\star)}yyzz)\star 
\]
puedo ver que los 3 programas tienen la misma semántica.
%
\paragraph{Semántica operacional a pequeños pasos}
También llamada semántica por reescritura. Consiste en definir $\hookrightarrow$
{\it a partir de otra relación} $\lra$ que describe las etapas elementales. Ejemplo:
\[
  (\lambda x.\elimor{\inl(\star)}y{y;x}zz)\star
  \lra
  \elimor{\inl(\star)}y{y;\star}zz
  \lra
  \star;\star
  \lra
  \star
\]
%
\[
  t\hookrightarrow r\iff t\lra^* r\qquad\textrm{y $r$ irreducible}
\]
donde $\lra^*$ es la clausura reflexiva y transitiva de $\lra$.
%
\paragraph{La no terminación.}
Un programa puede dar un resultado, producir un error o no terminar. Los errores
se pueden considerar como resultados particulares. Para expresar programas que
no terminan hay varias formas de expresar su semántica:
%
La primera consiste en considerar que si $t$ no termina, entonces no existe $r$
tal que $t\hookrightarrow r$.
%
La segunda consiste en agregar un elemento particular $\bot$ a los valores de
salida y considerar que si $t$ no termina, entonces $t\hookrightarrow\bot$.
 %
En este curso no trataremos el tema de la no terminación, sin embargo, un tratamiento fácil de seguir del punto fijo en cálculo lambda tipado se puede leer en \cite{DowekLevy11}.
%
\section{La semántica denotacional del lambda cálculo extendido}
En general, en los lenguajes funcionales buscamos reducir la distancia que
separa la noción de \underline{programa} de la de \underline{función}. Es
decir, se busca reducir la distancia entre un programa y su semántica
denotacional.
%

El primer paso es definir una categoría cartesiana cerrada de manera tal de dar una interpretación de los tipos en objetos de dicha categoría, y los términos tipados interpretarlos en flechas de la categoría.
La idea es que si $\Gamma = x_1:A_1,\dots,x_n:A_n$, queremos definir 
\[
  \sem{\Gamma\vdash t:B} = \sem{A_1}\times\cdots\times\sem{A_n}\xlra{t}\sem B
\]

Para interpretar el cálculo tipado visto en la Sección~\ref{sec:calculo}, utilizaremos la categoría $\mathsf{Set}$ vista en la sección anterior.
\paragraph{Interpretación de los tipos.}
A cada tipo le asociamos un objeto de $\mathsf{Set}$:
\begin{align*}
  \sem\top &= 1\\
  \sem\bot &= \emptyset\\
  \sem{A\Rightarrow B} &= \home{\sem A}{\sem B}\\
  \sem{A\wedge B} &=\sem A\times\sem B\\
  \sem{A\vee B} &=\sem A+\sem B
\end{align*}

El mapa $\sem{\cdot}$ también lo definimos para contextos de la siguiente manera:
\begin{align*}
  \sem{\emptyset} := 1\\
  \sem{\Gamma,x:A} :=\sem{\Gamma}\times\sem A
\end{align*}
%
\paragraph{Interpretación de los términos.}
A cada secuente $\Gamma\vdash t:A$ le asociamos una flecha de la categoría $\mathsf{Set}$. Dado que las reglas de tipado dadas en la Definición~\ref{def:reglas-de-tipado} son syntax directed (eso es, a cada secuente le corresponde una única derivación), definimos la relación para cada regla en lugar de para cada secuente. 
Algunas flechas utilizadas se encuentran definidas en la Práctica 3 (y sus propiedades son demostradas allí).
\begin{itemize}
  \item $\sem{\vcenter{\infer[\textrm{ax}]{\Gamma,x:A\vdash x:A}{}}} =\sem{\Gamma}\times\sem A\xlra{\pi_{\sem A}}\sem A\xlra{\Id}\sem A$ \\
  \item $\sem{\vcenter{\infer[\top_i]{\Gamma\vdash\star:\top}{}}}  = \sem{\Gamma}\xlra{\lambda^{-1}}1\times\sem{\Gamma}\xlra{\pi_1} 1$ \\
  \item $\sem{\vcenter{\infer[\top_e]{\Gamma\vdash\elimtop tr:A}{\Gamma\vdash t:\top & \Gamma\vdash r:A}}}  =\sem\Gamma\xlra{\delta}\sem\Gamma\times\sem\Gamma\xlra{t\times r} 1\times\sem A\xlra{\lambda}\sem A$ \\
  \item $\sem{\vcenter{\infer[\bot_e]{\Gamma\vdash\elimbot t:C}{\Gamma\vdash t:\bot}}}  =\sem\Gamma\xlra t\emptyset\xlra\emptyset\sem C $ \\
  \item $\sem{\vcenter{\infer[\Rightarrow_i]{\Gamma\vdash\lambda x.t:A\Rightarrow B}{\Gamma,x:A\vdash t:B}}} =\sem\Gamma\xlra{\eta^{\sem A}}\home{\sem A}{\sem\Gamma\times\sem A}\xlra{\home{\sem A}{t}}\home{\sem A}{\sem B}$ \\
  \item $\sem{\vcenter{\infer[\Rightarrow_e]{\Gamma\vdash tr:B}{\Gamma\vdash t:A\Rightarrow B & \Gamma\vdash r:A}}} 
    \begin{aligned}[t]
      & =\sem{\Gamma}\xlra\delta\sem\Gamma\times\sem\Gamma\\
      & \xlra{t\times r}\home{\sem A}{\sem B}\times\sem A\xlra{\varepsilon}\sem B
    \end{aligned}$\\
  \item $\sem{\vcenter{\infer[\wedge_i]{\Gamma\vdash\pair tr:A\wedge B}{\Gamma\vdash t:A & \Gamma\vdash r:B}}}  =\sem\Gamma\xlra\delta\sem\Gamma\times\sem\Gamma\xlra{t\times r}\sem A\times\sem B$ \\
  \item $\sem{\vcenter{\infer[\wedge_{e_1}]{\Gamma\vdash\elimandl t:A}{\Gamma\vdash t:A\wedge B}}}  =\sem\Gamma\xlra{t}\sem A\times\sem B\xlra{\pi_1}\sem A$ \\
  \item $\sem{\vcenter{\infer[\wedge_{e_2}]{\Gamma\vdash\elimandr t:B}{\Gamma\vdash t:A\wedge B}}}  =\sem\Gamma\xlra{t}\sem A\times\sem B\xlra{\pi_2}\sem B $ \\
  \item $\sem{\vcenter{\infer[\vee_{i_1}]{\Gamma\vdash\inl(t):A\vee B}{\Gamma\vdash t:A}}}  =\sem\Gamma\xlra{t}\sem A\xlra{i_1}\sem A+\sem B$ \\
  \item $\sem{\vcenter{\infer[\vee_{i_2}]{\Gamma\vdash\inr(t):A\vee B}{\Gamma\vdash t:B}}}  =\sem\Gamma\xlra{t}\sem B\xlra{i_2}\sem A+\sem B$ \\
  \item $\sem{\vcenter{\infer[\vee_e]{\Gamma\vdash\elimor txrys:C}{\Gamma\vdash t:A\vee B & \Gamma,x:A\vdash r:C & \Gamma,y:B\vdash s:C}}}$\\
    $=\sem{\Gamma}\xlra\delta\sem\Gamma\times\sem\Gamma\xlra{\Id\times t}\sem\Gamma\times(\sem A+\sem B)\xlra d(\sem\Gamma\times\sem A)+(\sem\Gamma\times\sem B)\xlra{\coprodu rs}\sem C$ \\
\end{itemize}

La correctitud de esta definición se establece por medio del teorema de ``Soundness'' (Teorema~\ref{thm:soundness}), que dice que si un término reduce a otro, ambos tienen la misma interpretación (es decir, denotan la misma flecha de la categoría). Para probar este teorema, primero debemos probar la propiedad de substitución (Lema~\ref{lem:subsSem}).

\begin{lema}
  [Substitución]
  \label{lem:subsSem}
  Si $\Gamma,x:A\vdash t:B$ y $\Gamma\vdash r:A$, entonces el siguiente diagrama conmuta (modulo permutaciones).
  \[
    \begin{tikzcd}[labels=description,row sep=1cm,column sep=2cm]
      \sem\Gamma\ar[d,"\delta"]\ar[r,"(r/x)t"] & \sem B\\
      \sem\Gamma\times\sem\Gamma  \ar[r,"\Id\times r"]
      &\sem\Gamma\times\sem A \ar[u,"t"]
    \end{tikzcd}
  \]
\end{lema}
\begin{proof}
  Procedemos por inducción en $t$. Reescribiremos el diagrama para cada caso, agregando en líneas de puntos ponemos flechas extras que simplifiquen el diagrama, y en rojo la justificación de cada subdiagrama.
  \begin{itemize}
    \item Sea $t=x$, entonces $(r/x)t = r$, $A=B$ y $\sem{\Gamma,x:A\vdash x:A} = \Id\circ\pi_{\sem A}$. El diagrama queda como sigue 
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=2cm,
	    execute at end picture={
	      \path (\tikzcdmatrixname-1-3) -- (\tikzcdmatrixname-3-3) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-2) -- (aux) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-1-3) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-2) -- (aux) node[midway,red] {\small ($\Id\circ r=r$)};
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-3-1) node[midway,red,sloped] {\small ($\pi\circ\delta = \Id$)};
	      \path (\tikzcdmatrixname-3-1) -- (\tikzcdmatrixname-3-3) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-2) -- (aux) node[midway,red,xshift=-1cm] {\small ($r\circ\pi = \pi\circ(\Id\times r)$)};
	    }
	  ]
	  \sem\Gamma\ar[dd,"\delta",bend right]\ar[rr,"r"]\ar[rd,"r",dashed] && \sem A\\
	  &\sem A\ar[ur,"\Id",dashed] &\\
	  \sem\Gamma\times\sem\Gamma\ar[uu,"\pi_{\sem\Gamma}",dashed,bend right]  \ar[rr,"\Id\times r"] &&\sem\Gamma\times\sem A \ar[ul,"\pi_{\sem A}",dashed]\ar[uu,"x"]
	\end{tikzcd}
      \]
    \item Sea $t=y\neq x$, entonces $(r/x)t = y$, y $\Gamma=\Delta,y:B$. El diagrama queda como sigue.
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=1cm,
	    execute at end picture={
	      \path (\tikzcdmatrixname-1-3) -- (\tikzcdmatrixname-3-3) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-2) -- (aux) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-1-3) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-2) -- (aux) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-3-1) -- (\tikzcdmatrixname-3-3) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-2) -- (aux) node[midway,red,xshift=-1cm] {\small ($\pi\circ (\Id\times r)\circ\delta=\pi$)};
	    }
	  ]
	  \sem\Delta\times\sem B\ar[dr,"\pi_{\sem B}",dashed]\ar[dd,"\delta"]\ar[rr,"y"] && \sem B\\
	  &\sem B\ar[ru,"\Id",dashed]& \\
	  (\sem\Delta\times\sem B)\times(\sem\Delta\times\sem B)  \ar[rr,"\Id\times r"] &&(\sem\Delta\times\sem B)\times\sem A \ar[uu,"y"]\ar[ul,"\pi_{\sem B}",dashed]
	\end{tikzcd}
      \]
    \item Sea $t=\star$, entonces $(r/x)t = \star$ y $B=\top$ (y $\sem\top=1$). El diagrama queda como sigue.
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=1.5cm,
	    execute at end picture={
	      \path (\tikzcdmatrixname-1-3) -- (\tikzcdmatrixname-4-3) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-3-2) -- (aux) node[midway,red,yshift=-4mm] {\small (Definición)};
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-1-3) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-2) -- (aux) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-4-1) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-2) -- (\tikzcdmatrixname-3-2) coordinate[pos=0.5](aux1)
	      (aux) -- (aux1) node[midway,red] {\small (*)};
	      \path (\tikzcdmatrixname-2-2) -- (\tikzcdmatrixname-3-2) coordinate[pos=0.5](aux)
	      (aux) -- (\tikzcdmatrixname-1-3) node[midway,red,sloped,yshift=-2mm] {\small ($\pi_1\circ\pi_{1\times\sem\Gamma}=\pi_1$)};
	    }
	  ]
	  \sem\Gamma\ar[dr,"\lambda^{-1}",dashed,sloped]\ar[ddd,"\delta"]\ar[rr,"\star"] && 1\\
	  &1\times\sem\Gamma\ar[ru,"\pi_1",dashed,sloped]& \\
	  &1\times\sem\Gamma\times\sem A\ar[u,"\pi_{1\times\sem\Gamma}",dashed]\ar[ruu,"\pi_1",dashed,sloped,bend right=20]& \\
	  \sem\Gamma\times\sem\Gamma  \ar[rr,"\Id\times r"] &&\sem\Gamma\times\sem A \ar[uuu,"\star"]\ar[ul,"\lambda^{-1}",dashed,sloped]
	\end{tikzcd}
      \]
      donde el diagrama {\color{red}(*)} lo justificamos analizando los mapas. Sea $g\in\sem\Gamma$:
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=1.5cm]
	  g\ar[dr,"\lambda^{-1}",mapsto,sloped]\ar[ddd,"\delta",mapsto] && \\
	  &(\star,g)& \\
	  &(\star,g,r(g))\ar[u,"\pi_{1\times\sem\Gamma}",mapsto]& \\
	  (g,g) \ar[rr,"\Id\times r",mapsto] &&(g,r(g))\ar[ul,"\lambda^{-1}",mapsto,sloped]
	\end{tikzcd}
      \]
    \item Sea $t=t_1;t_2$, entonces $(r/x)t = (r/x)t_1;(r/x)t_2$ con $\Gamma,x:A\vdash t_1:\top$ y $\Gamma,x:A\vdash t_2:B$. El diagrama queda como sigue.
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=4mm,
	    execute at end picture={
	      \path (\tikzcdmatrixname-1-5) -- (\tikzcdmatrixname-4-5) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-4) -- (\tikzcdmatrixname-3-4) coordinate[pos=0.5](aux1)
	      (aux) -- (aux1) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-1-5) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-2) -- (\tikzcdmatrixname-2-4) coordinate[pos=0.5](aux1)
	      (aux) -- (aux1) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-3-2) -- (\tikzcdmatrixname-3-4) coordinate[pos=0.5](aux)
	      (aux) -- (aux1) node[midway,red] {\small (HI y functorialidad de $\times$)};
	      \path (\tikzcdmatrixname-4-1) -- (\tikzcdmatrixname-4-5) coordinate[pos=0.5](aux1)
	      (aux) -- (aux1) node[midway,red,yshift=-3mm] {\small (Naturalidad de $\delta$)};
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-4-1) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-2) -- (\tikzcdmatrixname-3-2) coordinate[pos=0.5](aux1)
	      (aux) -- (aux1) node[midway,red] {\small (Naturalidad de $\delta$)};
	    }
	  ]
	  \sem\Gamma\ar[ddd,"\delta"]\ar[dr,"\delta",dashed,sloped]\ar[rrrr,"(r/x)t_1;(r/x)t_2"] &&&& \sem B\\
	  &\sem\Gamma\times\sem\Gamma\ar[d,"\delta\times\delta",dashed]\ar[rr,"(r/x)t_1\times (r/x)t_2",sloped,dashed]&& 1\times\sem B\ar[ru,"\lambda",dashed,sloped]&& \\
	  &(\sem\Gamma\times\sem\Gamma)\times(\sem\Gamma\times\sem\Gamma)\ar[rr,bend right=15,sloped,"(\Id\times r)\times(\Id\times r)",dashed]&&(\sem\Gamma\times\sem A)\times(\sem\Gamma\times\sem A)\ar[u,"t_1\times t_2",dashed]& \\
	  \sem\Gamma\times\sem\Gamma  \ar[rrrr,"\Id\times r"]\ar[ru,"\delta",dashed,sloped] &&&&\sem\Gamma\times\sem A \ar[uuu,"t_1;t_2"]\ar[ul,"\delta",dashed,sloped]
	\end{tikzcd}
      \]
    \item Sea $t=\elimbot{t'}$, entonces $(r/x)t = \elimbot{(r/x)t'}$, con $\Gamma,x:A\vdash t':\bot$.
      El diagrama queda como sigue.
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=2cm,
	    execute at end picture={
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-1-3) coordinate[pos=0.5](aux)
	      (aux) -- (\tikzcdmatrixname-2-2) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-1-3) -- (\tikzcdmatrixname-3-3) coordinate[pos=0.5](aux)
	      (aux) -- (\tikzcdmatrixname-2-2) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-3-3) node[midway,red,sloped,yshift=-8mm,xshift=-5mm] {\small (Hipótesis de inducción)};
	    }
	  ]
	  \sem\Gamma\ar[rd,"(r/x)t'",dashed,sloped]\ar[dd,"\delta"]\ar[rr,"\elimbot{(r/x)t'}"] && \sem B\\
	  &\emptyset\ar[ru,"\emptyset",dashed,sloped]& \\
	  \sem\Gamma\times\sem\Gamma  \ar[rr,"\Id\times r"] &&\sem\Gamma\times\sem A \ar[uu,"\elimbot{t'}"]\ar[ul, "t'",dashed,sloped]
	\end{tikzcd}
      \]
    \item Sea $t=\lambda y.t'$, entinces $(r/x)t = \lambda y.(r/x)t'$, con $B=C\Rightarrow D$, $\Gamma,x:A,y:C\vdash t':D$.
      Para la hipótesis de inducción se debe utilizar la propiedad de weakening, mostrando que como $\Gamma\vdash r:A$, también tenemos $\Gamma,y:C\vdash r:A$, y entonces la hipótesis de inducción dice lo siguiente, donde llamamos $r_w$ a la flecha $\sem{\Gamma,y:C\vdash r:A}$ para diferenciarla de $r=\sem{\Gamma\vdash r:A}$.
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=2cm]
	  \sem\Gamma\times\sem C\ar[d,"\delta"]\ar[r,"(r_w/x)t'"] & \sem D\\
	  (\sem\Gamma\times\sem C)\times(\sem\Gamma\times\sem C)  \ar[r,"\Id\times r_w"] &(\sem\Gamma\times\sem C)\times\sem A \ar[u,"t'"]
	\end{tikzcd}
      \]

      Por lo tanto, el diagrama queda como sigue.
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=2mm,
	    execute at end picture={
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-1-3) coordinate[pos=0.5](aux)
	      (aux) -- (\tikzcdmatrixname-2-2) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-3-3) -- (\tikzcdmatrixname-6-3) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-4-2) -- (\tikzcdmatrixname-5-2) coordinate[pos=0.5](aux1)
	      (aux) -- (aux1) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-3-2) -- (\tikzcdmatrixname-1-3) node[midway,red,sloped] {\small (HI y functorialidad de $\mathsf{hom}$)};
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-4-1) node[pos=0.3,red] {\small (Naturalidad de $\eta^{\sem C}$)};
	      \path (\tikzcdmatrixname-4-1) -- (\tikzcdmatrixname-3-2) node[midway,red] {\small (*)};
	    }
	  ]
	  \sem\Gamma\ar[ddddd,"\delta",bend right=60]\ar[dr,"\eta^{\sem C}",dashed,sloped]\ar[rr,"\lambda y.(r_w/x)t'"] && \home{\sem C}{\sem D}\\
	  &\home{\sem C}{\sem\Gamma\times\sem C}\ar[ddl,"\home{\sem C}{\delta\times\Id}",dashed,sloped,out=180,in=80]\ar[ru,"\home{\sem C}{(r_w/x)t'}",dashed,sloped]\ar[d,"\home{\sem C}{\delta}",dashed] & \\
	  &\home{\sem C}{(\sem\Gamma\times\sem C)\times(\sem\Gamma\times\sem C)}\ar[d,"\home{\sem C}{\Id\times r_w}",dashed] & {\phantom{A}}\\
	  \home{\sem C}{\sem\Gamma\times\sem\Gamma\times\sem C}\ar[rd,out=-60,in=180,sloped,"\home{\sem C}{\Id\times r\times\Id}",dashed]&\home{\sem C}{\sem\Gamma\times\sem C\times\sem A}\ar[d,equal,dashed,"\home{\sem C}{\sigma}"]\ar[uuur,"\home{\sem C}{t'}",dashed,sloped,out=0,in=225] & \\
	  &\home{\sem C}{\sem\Gamma\times\sem A\times\sem C} & \\
	  \sem\Gamma\times\sem\Gamma  \ar[rr,"\Id\times r"] &&\sem\Gamma\times\sem A \ar[uuuuu,"\lambda y.t'"]\ar[ul,"\eta^{\sem C}",dashed,sloped] 
	\end{tikzcd}
      \]
      donde el diagrama {\color{red}(*)} lo justificamos analizando los mapas. Sea $f\in\home{\sem C}{\sem\Gamma\times\sem C}$.
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=2cm]
	  &f\ar[ddl,"\home{\sem C}{\delta\times\Id}",mapsto,sloped,out=180,in=80]\ar[d,"\home{\sem C}{\delta}",mapsto] \\
	  &\delta\circ f\ar[d,"\home{\sem C}{\Id\times r_w}",mapsto] \\
	  (\delta\times\Id)\circ f\ar[d,"\home{\sem C}{\Id\times r\times\Id}",mapsto]& (\Id\times r_w)\circ\delta\circ f\ar[d,mapsto,"\home{\sem C}{\sigma}"] & \\
	  (\Id\times r\times\Id)\circ(\delta\times\Id)\circ f\ar[r,blue,equal] &\sigma\circ(\Id\times r_w)\circ\delta\circ f  
	\end{tikzcd}
      \]
      Donde la igualdad final es justificada como sigue. Sea $c\in\sem C$, $f(c) = (g,c')$ y $r(g)=a$. Finalmente, notar que $r_w = r\circ\pi_{\sem\Gamma}$, entonces
      \begin{align*}
	((\Id\times r\times\Id)\circ(\delta\times\Id)\circ f)(c)
	&= ((\Id\times r\times\Id)\circ(\delta\times\Id))(f(c))\\
	&= ((\Id\times r\times\Id)\circ(\delta\times\Id))(g,c')\\
	&= (\Id\times r\times\Id)(g,g,c')\\
	&=(g,a,c') \\
	&=\sigma(g,c',a)\\
	&=(\sigma\circ(\Id\times r))(g,c',g)\\
	&=(\sigma\circ(\Id\times(r\circ\pi_{\Gamma})))(g,c',g,c')\\
	&=(\sigma\circ(\Id\times r_w))(g,c',g,c')\\
	&=(\sigma\circ(\Id\times r_w)\circ\delta)(g,c')\\
	&=(\sigma\circ(\Id\times r_w)\circ\delta)(f(c))\\
	&=(\sigma\circ(\Id\times r_w)\circ\delta\circ f)(c)
      \end{align*}

    \item Sea $t=t_1t_2$, entonces $(r/x)t = ((r/x)t_1)((r/x)t_2)$, con $\Gamma,x:A\vdash t_1:C\Rightarrow B$ y $\Gamma,x:A\vdash t_2:C$.
      El diagrama queda como sigue.
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=0cm,
	    execute at end picture={
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-1-4) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-2) -- (\tikzcdmatrixname-2-3) coordinate[pos=0.5](aux1)
	      (aux) -- (aux1) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-3-2) -- (\tikzcdmatrixname-3-3) coordinate[pos=0.5](aux)
	      (aux) -- (aux1) node[midway,red] {\small (HI y functorialidad de $\times$)};
	      \path (\tikzcdmatrixname-4-1) -- (\tikzcdmatrixname-4-4) coordinate[pos=0.5](aux1)
	      (aux) -- (aux1) node[midway,red] {\small (Naturalidad de $\delta$)};
	      \path (\tikzcdmatrixname-1-4) -- (\tikzcdmatrixname-4-4) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-3) -- (\tikzcdmatrixname-3-3) coordinate[pos=0.5](aux1)
	      (aux) -- (aux1) node[midway,red] {\small (Definición)};
	    }
	  ]
	  \sem\Gamma\ar[ddd,"\delta"]\ar[dr,"\delta",dashed,sloped]\ar[rrr,"((r/x)t_1)((r/x)t_2)"] &&[3cm]& {\sem B} \\
	  &\sem\Gamma\times\sem\Gamma\ar[r,"(r/x)t_1\times(r/x)t_2",dashed]\ar[d,"\delta\times\delta",dashed]&\home{\sem C}{\sem B}\times\sem C\ar[ru,"\varepsilon",dashed,sloped]&\\
	  &\sem\Gamma\times\sem\Gamma\times\sem\Gamma\times\sem\Gamma\ar[r,"\Id\times r\times\Id\times r",dashed]&\sem\Gamma\times\sem A\times\sem\Gamma\times\sem A\ar[u,"t_1\times t_2",dashed]& \\
	  \sem\Gamma\times\sem\Gamma  \ar[rrr,"\Id\times r"] &&&\sem\Gamma\times\sem A \ar[uuu,"t_1t_2"]\ar[ul,"\delta",dashed,sloped]
	\end{tikzcd}
      \]
    \item Sea $t=\pair{t_1}{t_2}$, entonces $(r/x)t = \pair{(r/x)t_1}{(r/x)t_2}$ con $B=B_1\wedge B_2$, $\Gamma,x:A\vdash t_1:B_1$ y $\Gamma,x:A\vdash t_2:B_2$. El diagrama queda como sigue.
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=3cm,
	    execute at end picture={
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-1-3) coordinate[pos=0.5](aux)
	      (aux) -- (\tikzcdmatrixname-2-2) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-1-3) -- (\tikzcdmatrixname-5-3) coordinate[pos=0.5](aux)
	      (aux) -- (\tikzcdmatrixname-4-2) node[midway,red,yshift=-1cm] {\small (Definición)};
	      \path (\tikzcdmatrixname-3-2) -- (\tikzcdmatrixname-1-3) node[midway,red,sloped] {\small (HI y functorialidad de $\times$)};
	      \path (\tikzcdmatrixname-5-1) -- (\tikzcdmatrixname-5-3) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-4-2) -- (aux) node[midway,red] {\small (Naturalidad de $\delta$)};
	    }
	  ]
	  \sem\Gamma\ar[dddd,"\delta"]\ar[dr,"\delta",dashed,sloped]\ar[rr,"\pair{(r/x)t_1}{(r/x)t_2}"] && \sem{B_1}\times\sem{B_2}\\
	  &\sem\Gamma\times\sem\Gamma\ar[d,"\delta\times\delta",dashed]\ar[ur,"(r/x)t_1\times(r/x)t_2",dashed,sloped]&\\
	  &\sem\Gamma\times\sem\Gamma\times\sem\Gamma\times\sem\Gamma\ar[d,"\Id\times r\times\Id\times r",dashed]&\\
	  &\sem\Gamma\times\sem A\times\sem\Gamma\times\sem A\ar[uuur,"t_1\times t_2",dashed,sloped,bend right=10]&\\
	  \sem\Gamma\times\sem\Gamma  \ar[rr,"\Id\times r"] &&\sem\Gamma\times\sem A \ar[uuuu,"\pair{t_1}{t_2}"]\ar[ul,"\delta",dashed,sloped]
	\end{tikzcd}
      \]
    \item Sea $t=\pi_1 t'$, entonces $(r/x)t=\pi_1((r/x)t')$, con $\Gamma,x:A\vdash t':B\wedge C$. El diagrama queda como sigue.
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=3cm,
	    execute at end picture={
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-1-3) coordinate[pos=0.5](aux)
	      (aux) -- (\tikzcdmatrixname-2-2) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-1-3) -- (\tikzcdmatrixname-3-3) coordinate[pos=0.5](aux)
	      (aux) -- (\tikzcdmatrixname-2-2) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-3-1) -- (\tikzcdmatrixname-2-2) node[midway,red] {\small (Hipótesis de inducción)};
	    }
	  ]
	  \sem\Gamma\ar[dd,"\delta"]\ar[dr,"(r/x)t'",dashed,sloped]\ar[rr,"\pi_1((r/x)t')"] && \sem{B}\\
	  &\sem B\times\sem C\ar[ur,"\pi_1",dashed,sloped]&\\
	  \sem\Gamma\times\sem\Gamma  \ar[rr,"\Id\times r"] &&\sem\Gamma\times\sem A \ar[uu,"\pi_1{t'}"]\ar[ul,"t'",dashed,sloped]
	\end{tikzcd}
      \]
    \item Sea $t=\pi_2 t'$. Este caso es análogo al anterior.
    \item Sea $t=\inl(t')$, entonces $(r/x)t=\inl((r/x)t')$, con $B=B_1\vee B_2$ y $\Gamma,x:A\vdash t':B_1$. El diagrama queda como sigue.
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=3cm,
	    execute at end picture={
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-1-3) coordinate[pos=0.5](aux)
	      (aux) -- (\tikzcdmatrixname-2-2) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-1-3) -- (\tikzcdmatrixname-3-3) coordinate[pos=0.5](aux)
	      (aux) -- (\tikzcdmatrixname-2-2) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-3-1) -- (\tikzcdmatrixname-2-2) node[midway,red] {\small (Hipótesis de inducción)};
	    }
	  ]
	  \sem\Gamma\ar[dd,"\delta"]\ar[dr,"(r/x)t'",dashed,sloped]\ar[rr,"\inl((r/x)t')"] && \sem{B_1}+\sem{B_2}\\
	  &\sem B_1\ar[ur,"i_1",dashed,sloped]&\\
	  \sem\Gamma\times\sem\Gamma  \ar[rr,"\Id\times r"] &&\sem\Gamma\times\sem A \ar[uu,"\inl(t')"]\ar[ul,"t'",dashed,sloped]
	\end{tikzcd}
      \]
    \item Sea $t=\inr(t')$. Este caso es análogo al anterior.
    \item Sea $t=\elimor{t'}y{t_1}z{t_2}$, entonces $(r/x)t=\elimor{(r/x)t'}y{(r/x)t_1}z{(r/x)t_2}$, con $\Gamma,x:A\vdash t':C_1\vee C_2$, $\Gamma,x:A,y:C_1\vdash t_1:B$, y $\Gamma,x:A,z:C_1\vdash t_2:B$. El diagrama queda como sigue.
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=-5mm,
	    execute at end picture={
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-1-4) coordinate[pos=0.5](aux)
	      (\tikzcdmatrixname-2-2) -- (\tikzcdmatrixname-2-3) coordinate[pos=0.5](aux1)
	      (aux) -- (aux1) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-4-3) -- (\tikzcdmatrixname-5-3) node[midway,red] {\small (Definición)};
	      \path (\tikzcdmatrixname-3-2) -- (\tikzcdmatrixname-5-2) node[midway,red] {\small (*)};
	      \path (\tikzcdmatrixname-3-3) -- (\tikzcdmatrixname-1-4) node[midway,red,sloped,yshift=-3mm] {\small (HI y funct. de $+$)};
	    }
	  ]
	  \sem\Gamma\ar[dr,"\delta",dashed,sloped]\ar[ddddd,"\delta"]\ar[rrr,"\elimor{(r/x)t'}y{(r/x)t_1}z{(r/x)t_2}"] &&& \sem{B}\\
	  &\sem\Gamma\times\sem\Gamma\ar[d,"\Id\times (r/x)t'",dashed]&(\sem\Gamma\times\sem{C_1})+(\sem\Gamma\times\sem{C_2})\ar[ur,"\coprodu{(r/x)t_1}{(r/x)t_2}",dashed,sloped]\ar[d,"\coprodu \delta\delta",dashed]& \\
	  &\sem\Gamma\times(\sem{C_1}+\sem{C_2})\ar[ur,"d",dashed,sloped]&
	  {{\parbox{5cm}{\centering$
	    \begin{array}{c}
	    (\sem\Gamma\times\sem{C_1}\times\sem\Gamma\times\sem{C_1})
	    \\+\\
	    (\sem\Gamma\times\sem{C_2}\times\sem\Gamma\times\sem{C_2})
    \end{array}$}}}\ar[d,"\coprodu{\Id\times r_w}{\Id\times r_w}",dashed]& \\
    &&(\sem\Gamma\times\sem A\times\sem{C_1})+(\sem\Gamma\times\sem A\times\sem{C_2})\ar[uuur,"\coprodu{t_1}{t_2}",dashed,sloped,bend right]& \\
	  &(\sem\Gamma\times\sem A)\times(\sem{C_1}+\sem{C_2})\ar[ur,"d",dashed]&\sem\Gamma\times\sem A\times\sem\Gamma\times\sem A\ar[l,"\Id\times t'",dashed]& \\
	  \sem\Gamma\times\sem\Gamma  \ar[rrr,"\Id\times r"] &&&\sem\Gamma\times\sem A \ar[uuuuu,"\elimor{t'}y{t_1}z{t_2}",sloped]\ar[ul,"\delta",dashed,sloped]
	\end{tikzcd}
      \]
      donde el diagrama {\color{red}(*)} se justifica de la siguiente manera. Sea $g\in\sem\Gamma$,$r(g)=a$, $((r/x)t')(g) = c_1+c_2$, y, por hipótesis de inducción, $t'(g,a)=c_1+c_2$. Entonces tenemos,
      \[
	\begin{tikzcd}[labels=description,row sep=1cm,column sep=5mm]
	  g \arrow[mapsto,rd, "\delta"] \arrow[mapsto,ddddd, "\delta"] &&& \\
	  & {(g,g)} \arrow[mapsto,d, "\Id\times(r/x)t'"] & {(g,c_1)+(g,c_2)} \arrow[mapsto,d, "{[\delta,\delta]}"] & \\
	  & {(g,c_1+c_2)} \arrow[mapsto,ru, "d"] & {(g,c_1,g,c_1)+(g,c_2,g,c_2)} \arrow[mapsto,d, "{[\Id\times r_w,\Id\times r_w]}"] &\\
	  & & {(g,c_1,a)+(g,c_2,a)}\simeq{(g,a,c_1)+(g,a,c_2)} & \\
	  &{(g,a,(c_1+c_2))} \arrow[mapsto,ur, "d"]& {(g,a,g,a)} \arrow[mapsto,l, "\Id\times t'"]& \\
	  {(g,g)} \arrow[mapsto,rrr, "\Id\times r"]&&& {(g,a)} \arrow[mapsto,lu, "\delta"]
	\end{tikzcd}
      \]
      \qedhere
  \end{itemize}
\end{proof}

\begin{teorema}[Soundness]\label{thm:soundness}
  Si $\Gamma\vdash t:A$ y $t\to r$, entonces 
  $\sem{\Gamma\vdash t:A}=\sem{\Gamma\vdash r:A}$.
\end{teorema}
\begin{proof}
  Inducción sobre la relación $\lra$.
  Damos uno de los casos base y un caso inductivo a modo de ejemplo:
  \begin{itemize}
    \item Caso base $(\lambda x.t)r\lra (r/x)t$.

      Consideramos $\sem{\vcenter{
	  \infer{\Gamma\vdash(\lambda x.t)r:A}{
	    \infer{\Gamma\vdash\lambda x.t:B\Rightarrow A}{\Gamma,x:B\vdash t:A}
	    &
	    \Gamma\vdash r:B
	  }
      }}$
      y
      $\sem{\Gamma\vdash (r/x)t:A}$

      El diagrama conmutativo correspondiente es el siguiente.
      \[
	\begin{tikzcd}[labels=description,row sep=2cm,column sep=3.5cm,
	    execute at end picture={
	      \path (\tikzcdmatrixname-1-1) -- (\tikzcdmatrixname-2-2) node[midway,red]{\small (Lema~\ref{lem:subsSem})};
	      \path (\tikzcdmatrixname-2-2) -- (\tikzcdmatrixname-2-3) node[midway,red]{\small (Ax.~de adjunción)};
	      \path (\tikzcdmatrixname-1-3) -- (\tikzcdmatrixname-2-3)  node[midway,red,xshift=-2.5cm]{\small (Functorialidad de $\times$)};
	      \path (\tikzcdmatrixname-3-1) -- (\tikzcdmatrixname-2-2) coordinate[pos=0.5](aux1)
	      (\tikzcdmatrixname-2-3) -- (\tikzcdmatrixname-3-3) coordinate[pos=0.5](aux2)
	      (aux1) -- (aux2) node[pos=0.35,red]{\small (Naturalidad de $\varepsilon$)};
	  }]
	  \sem\Gamma\ar[rr,"\delta"]\ar[dd,"(r/x)t"] && \sem\Gamma\times\sem\Gamma\ar[d,"\eta^{\sem B}\times r"]\ar[dl,"\Id\times r",dashed,sloped,bend right=15]\\
	  &\sem\Gamma\times\sem B\ar[dl,"t",dashed,sloped]\ar[r,"\eta^{\sem B}\times\Id",dashed,sloped,bend left=20]&\home{\sem B}{\sem\Gamma\times\sem B}\times\sem B\ar[d,"\home{\sem A}{t}\times\Id"]\ar[l,"\varepsilon",dashed,sloped,bend left=20]\\
	  \sem A&&\home{\sem B}{\sem A}\times\sem B\ar[ll,"\varepsilon"]
	\end{tikzcd}
      \]
    \item Caso inductivo $\vcenter{\infer{ts\lra rs}{t\lra r}}$


      Consideramos
      $\sem{
	\vcenter{
	  \infer{\Gamma\vdash ts:A}{\Gamma\vdash t:B\Rightarrow A & \Gamma\vdash s:B}
	}
      }$
      y
      $\sem{
	\vcenter{
	  \infer{\Gamma\vdash rs:A}{\Gamma\vdash r:B\Rightarrow A & \Gamma\vdash s:B}
	}
      }$

      El diagrama conmutativo correspondiente es el siguiente.
      \[
	\begin{tikzcd}[labels=description,row sep=2cm,column sep=2.5cm,
	    execute at end picture={
	      \path (\tikzcdmatrixname-2-1) -- (\tikzcdmatrixname-1-3) node[midway,red,sloped,yshift=1cm,xshift=-1.5cm]{\small (Igualdad sintáctica)};
	      \path (\tikzcdmatrixname-2-1) -- (\tikzcdmatrixname-1-3) node[midway,red,sloped]{\small (Hipótesis de inducción y functorialidad de $\times$)};
	      \path (\tikzcdmatrixname-2-2) -- (\tikzcdmatrixname-2-3) node[midway,red,yshift=5mm,xshift=5mm]{\small (Functorialidad de $\times$)};
	      \path (\tikzcdmatrixname-2-1) -- (\tikzcdmatrixname-2-2) node[midway,red,yshift=-5mm,xshift=-4mm]{\small (Functorialidad de $\times$)};
	      \path (\tikzcdmatrixname-2-2) -- (\tikzcdmatrixname-3-1) coordinate[pos=0.5](aux1)
	      (\tikzcdmatrixname-2-3) -- (\tikzcdmatrixname-3-3) coordinate[pos=0.5](aux2)
	      (aux1) -- (aux2) node[midway,red]{\small (Igualdad sintáctica)};
	    }
	  ]
	  \sem\Gamma\ar[rr,"\delta"]\ar[d,"\delta"] && \sem\Gamma\times\sem\Gamma\ar[d,"t\times s"]\ar[dl,"t\times\Id",dashed,sloped]\\
	  \sem\Gamma\times\sem\Gamma\ar[d,"r\times s"]\ar[rru,equal,dashed,bend left=10]\ar[r,"r\times\Id",dashed]&\home{\sem B}{\sem A}\times\sem\Gamma\ar[r,"\Id\times s",dashed]\ar[dl,"\Id\times s",dashed,sloped]& \home{\sem B}{\sem A}\times\sem B\ar[d,"\varepsilon"]\\
	 \home{\sem B}{\sem A}\times\sem B\ar[rr,"\varepsilon"] && \sem B
	\end{tikzcd}
      \]

      \begin{ejercicio}
      Resolver todos los casos restantes.
      \end{ejercicio}
      \qedhere
  \end{itemize}
\end{proof}

\part{Computación cuántica}\label{part:CC}
\chapter{Introducción a la computación cuántica}\label{ch:intro}
\hfill\parbox{\textwidth}{\begin{quote} 
    {\em I feel that a deep understanding of why quantum algorithms
      work is still lacking. Surely the power of quantum computers
      has something to do with entanglement, quantum parallelism,
      and the vastness of Hilbert space, but I think that it should
      be possible to pinpoint more precisely the true essence of
    the matter.}
%
    \hfill{\small\rm John~\cite{PreskillRSL98}}
  \end{quote}
} \pagestyle{fancy}
\section{Introducción}
La computación cuántica, una rama de las ciencias de la computación
teórica, tiene su origen en la física, y más precisamente en el
físico estadounidense Richard Feynman, quien en 1981 dedicó una
charla en el Massachusetts Institute of Technology (MIT) al
problema de la simulación de la física cuántica con computadoras
clásicas. Sus ya célebres palabras finales resumen su frustración
de ese entonces:
\begin{quote}
  {\em And I'm not happy with all the analyses that go with just
    the classical theory, because nature isn't classical, dammit,
    and if you want to make a simulation of nature, you'd better
    make it quantum mechanical, and by golly it's a wonderful
  problem, because it doesn't look so easy. Thank you.}

  \hfill {\small (ver, por ejemplo, \cite[pp.100]{Brown01})}
\end{quote}
Esta provocación, lejos de plantear soluciones, abrió las puertas a
interrogantes nunca antes concebidos. ¿Qué ganancia se lograría si
las computadoras fuesen regidas por las leyes de la mecánica
cuántica? Fueron los algoritmos de \cite{GroverSTOC96} y
\cite{ShorSIAM97} los cuales despertaron el gran interés desde las
ciencias de la computación en este nuevo paradigma. El primero es
un algoritmo de búsqueda sobre registros desordenados, el cual
provee una ganancia cuadrática de complejidad temporal frente a
cualquier algoritmo clásico conocido. El segundo es un algoritmo
para la factorización de números, con una ganancia exponencial.

\medskip

Actualmente existen muchas áreas de investigación dentro de la
computación cuántica. Por ejemplo, desde un punto de vista práctico
se plantea el problema de construir el hardware de una computadora
cuántica. Desde sus orígenes, en las palabras de Feinmann, la idea
es que un algoritmo cuántico sea una simulación cuántica en
hardware que se comporta de acuerdo a las leyes de la física
cuántica. Es decir que un experimento cuántico en un laboratorio,
puede considerarse como un algoritmo. O dicho de otro modo: podemos
describir el comportamiento de un sistema cuántico a través de un
algoritmo. La pregunta es, ¿podemos realizar el experimento
cuántico que describe un algoritmo dado? Allí es donde se
manifiesta el desafío técnico.

Otra área es la de desarrollar algoritmos que obtengan una ganancia
con respecto a su contraparte clásica. En general los algoritmos de
Grover y Shor mencionados anteriormente se consideran como los
ejemplos canónicos de aceleración obtenida gracias a la computación
cuántica. Muchos otros algoritmos cuánticos son derivados de ellos.
La pregunta aquí es ¿qué otros algoritmos podemos obtener que nos
den una ganancia respecto a los algoritmos clásicos?

Otra rama de investigación es la del diseño de lenguajes de
programación que permitan expresar los algoritmos cuánticos de una
manera amigable, y quizá permitiendo descubrir nuevos algoritmos al
tener una herramienta de alto nivel para pensarlos.

Desde un punto de vista más fundamental, y como lo expresara
Preskill en la cita que abre este capítulo, los fundamentos lógicos
detrás de la computación cuántica, siguen siendo un misterio. Si
bien existe una lógica cuántica~\citep{BirkhoffVonNeumannAM36},
ésta fue propuesta muchos años antes de la computación cuántica,
por lo que encontrar la correspondencia entre computación y lógica
cuántica no es trivial. Esta área tiene muchas subáreas con
metodologías diferentes. En particular, el estudio de semántica de
lenguajes de programación sigue este objetivo. En este caso no se
persigue el estudio del lenguaje en sí mismo, sino que el objetivo
es el estudio de la lógica subyacente. Estudiar la lógica detrás de
la computación cuántica implica estudiar la lógica detrás de la
física cuántica, lo cual puede tener influencia en el desarrollo de
nuevas teorías sobre el mundo que nos rodea. 
\medskip

En este curso nos interesa este último aspecto: el estudio de propiedades de
lenguajes de programación que nos acerquen hacia una lógica computacional de la
física cuántica.

\section{Preliminares: un poco de álgebra}
\subsection{Espacio de Hilbert}

\paragraph{TL;DR} $\mathbb{C}^n$ con la suma ($+$) y el producto ($\cdot$)
usuales, y el producto escalar definido por
\[
  \prodi{\vec v}{\vec w}=\prodi{(v_1,v_2,\dots,v_n)}{(w_1,w_2,\dots,w_n)} =
  \sum_{i=1}^n\conj{v_i}\cdot{w_i}
\]
donde $\conj v$ es el complejo conjugado de $v$, es un {\em espacio de Hilbert}.
\medskip

{\bfseries En el resto de la sección se define formalmente qué es un espacio de
  Hilbert.}

\begin{definicion}[Producto escalar] Sea $E$ un espacio vectorial sobre el cuerpo $\mathbb{K}$ ($\mathbb{R}$ o $\mathbb{C}$). Un producto escalar (también llamado producto interno) definido sobre $E$ es una función $\prodi{}{} : E \times E \rightarrow \mathbb{K}$ que verifica las siguientes propiedades.

  Para todo $\vec u,\vec v,\vec w\in E$, ${a},{b}\in\mathbb{K}$, se cumple:
  \begin{align*}
    &\left\{\begin{array}{l}
              \prodi{\vec u}{\vec u} \geq 0\\
              \prodi{\vec u}{\vec u} = 0 \Leftrightarrow\vec u =\vec 0_E
            \end{array}\right.
    & \textrm{(Definida positiva)}\\
    &\prodi{\vec w}{{a} \vec u + {b} \vec v} = {a} \prodi{\vec w}{\vec u} + {b} \prodi{\vec w}{\vec v}
    & \textrm{(Lineal por derecha)}\\
    &\prodi{{a}\vec u + {b}\vec v}{\vec w} = \conj{{a}} \prodi{\vec u}{\vec w} + \conj{{b}} \prodi{\vec v}{\vec w}
    & \textrm{(Antilineal por izquierda)}\\
    &\prodi{\vec u}{\vec v} = \conj{\prodi{\vec v}{\vec u}}
    & \textrm{(Hermítica)}
  \end{align*}
\end{definicion}

\begin{definicion}[Espacio pre-Hilbert] Un espacio pre-Hilbert es un espacio
  vectorial sobre $\mathbb{K}$ con producto escalar.
\end{definicion}

\begin{observacion} Todo espacio pre-Hilbert es un espacio vectorial normado con
  la norma \[\norma{\vec v} = \sqrt{\prodi{\vec v}{\vec v}}\]
\end{observacion}

\begin{ejercicio}
  Determinar si el espacio vectorial y la función dada en cada caso definen
  un espacio pre-Hilbert.

  \begin{enumerate}
    \item En el espacio vectorial $\mathcal M_2$ de matrices cuadradas de
      dimensión $2\times 2$ sobre $\mathbb R$, la función
      \(
        f(M,N)
        = \sum_{i=1}^2\sum_{j=1}^2 m_{ij}\, n_{ij}
      \),
      donde $M=(m_{ij})_{ij}$ y $N=(n_{ij})_{ij}$.

    \item Ídem anterior, pero considerando el espacio vectorial de matrices
      cuadradas de dimensión $2\times 2$ sobre $\mathbb C$.

    \item En el espacio vectorial $\mathbb C^2$, la función
      \(
        f(\vec v,\vec w)
        = \conj{v_1}\, w_2 + \conj{v_2}\, w_1
      \).
  \end{enumerate}
\end{ejercicio}


\begin{definicion}[Sucesión de Cauchy]
  Sea $\vec v_n$ una sucesión de vectores del espacio $E$.

  Si $\norma{\vec v_n - \vec v_m} \rightarrow 0$ cuando $n,m \rightarrow
  \infty$, entonces la sucesión $\vec v_n$ es una sucesión de Cauchy. (Esto
  quiere decir que puedo hacer distar entre sí los términos tan poco como
  quiera).
\end{definicion}

\begin{observacion} Toda sucesión convergente es de Cauchy, pero no toda
  sucesión de Cauchy es convergente.
\end{observacion}

\begin{definicion}[Espacio completo] $E$ es completo para la norma
  $\norma{\cdot}$, si y sólo si toda sucesión de Cauchy converge con esa norma.
\end{definicion}

\begin{definicion}[Espacio de Hilbert] Un espacio pre-Hilbert completo en su
  norma se denomina espacio de Hilbert.
\end{definicion}

\subsection{Productos tensoriales}

En esta sección consideramos espacios vectoriales equipados con una base
canónica.

\begin{definicion}[Producto tensorial]\label{def:prodTen}
  Sean $E$ y $F$ dos espacios vectoriales con bases canónicas $B=\{\vec
  b_i~|~i\in I\}$ y $C=\{\vec c_j~|~j\in J\}$ respectivamente. El producto
  tensorial $E\otimes F$ de $E$ y $F$ es el espacio vectorial de base canónica
  $\{\vec b_i\otimes\vec c_j~|~i\in I$ y $j\in J\}$, donde $\vec b_i\otimes\vec
  c_j$ es el par ordenado formado por el vector $\vec b_i$ y el vector $\vec
  c_j$. La operación $\otimes$ se extiende a vectores de $E$ y $F$
  bilinearmente:
  \[
    (\sum_i\alpha_i\vec b_i)\otimes(\sum_j\beta_j\vec
    c_j)=\sum_{ij}\alpha_i\beta_j(\vec b_i\otimes \vec c_j)
  \]
\end{definicion}

\begin{ejercicio}
  Determinar el producto tensorial entre $\mathbb R^3$ y $\mathbb R^2$:
  describir una base explícita de $\mathbb R^3\otimes\mathbb R^2$ y su
  dimensión.

  ¿Qué sucede si queremos calcular el producto tensorial entre $\mathbb R^3$
  y $\mathbb C^2$? ¿Es posible hacerlo? Explicar.
\end{ejercicio}

\begin{definicion}
  [Producto cartesiano entre dos subconjuntos de espacios vectoriales] Sean $E$
  y $F$ dos espacios vectoriales equipados con bases $B$ y $C$, y sean $S$ y $T$
  dos subconjuntos de $E$ y $F$ respectivamente. Definimos el conjunto $S\times
  T$, subconjunto del espacio vectorial $E\otimes F$, de la siguiente manera:
  \[
    S\times T = \{\vec u\otimes\vec v~|~\vec u\in S, \vec v\in T\}
  \]
\end{definicion}
\begin{observacion}
  $E\times F \neq E\otimes F$. Por ejemplo, si $E=F=\mathbb C^2$, con base
  canónica $\{\vec i,\vec j\}$, entonces $E\times F$ contiene a $\vec
  i\otimes\vec i$ y a $\vec j\otimes\vec j$, pero no a $\vec i\otimes\vec i+\vec
  j\otimes\vec j$, que no es producto de dos vectores de $\mathbb C^2$.
\end{observacion}

\begin{definicion}
  [Generador] Sea $E$ un espacio vectorial equipado con una base $B$, y
  $S\subseteq E$. Escribimos $\mathcal G(S)$ al espacio vectorial sobre $\mathbb
  C$ generado por $S$, es decir, que contiene todas las combinaciones lineales
  de elementos de $S$.
\end{definicion}

\begin{observacion}
  Si $E$ y $F$ son dos espacios vectoriales con bases $B$ y $C$ respectivamente,
  entonces
  \[
    E\otimes F = \mathcal G(B\times C) = \mathcal G(E\times F)
  \]
\end{observacion}

La operación $\otimes$ introducida genéricamente en la
Definición~\ref{def:prodTen}, puede ser definida más precisamente para matrices
(y vectores, tomando matrices columna o fila) de la siguiente manera.

\begin{definicion}[Producto tensorial entre matrices] El producto tensorial de
  dos matrices, $P$ y $Q$ se define como la matriz
  \[
    P\otimes Q = \matriz{
      p_{11} Q & \ldots & p_{1m} Q \\
      \vdots &	& \vdots \\
      p_{n1} Q & \ldots & p_{nm} Q }
  \]
\end{definicion}

\begin{ejemplo}
  \[
    \matriz{
      1 & 2\\
      3 & 4 } \otimes \matriz{
      5 & 6\\
      7 & 8 } = \matriz{ 1 \matriz{
        5 & 6\\
        7 & 8 } & 2 \matriz{
        5 & 6\\
        7 & 8 }
      \\
      3 \matriz{
        5 & 6\\
        7 & 8 } & 4 \matriz{
        5 & 6\\
        7 & 8 } } = \matriz{
      5 & 6 & 10 & 12\\
      7 & 8 & 14 & 16\\
      15 & 18 & 20 & 24\\
      21 & 24 & 28 & 32 }
  \]
  \[
    \matriz{
      1\\
      2 } \otimes \matriz{
      3\\
      4 } = \matriz{ 1 \matriz{
        3\\
        4 }
      \\
      2 \matriz{
        3\\
        4 } } = \matriz{
      3\\
      4\\
      6\\
      8 }
  \]
\end{ejemplo}
\begin{observacion}
  El producto escalar, o producto interno, entre dos vectores nos da un número.
  El producto tensorial, o producto externo, entre dos vectores nos da un vector
  de mayor dimensión.
\end{observacion}

\begin{ejercicio}
  \label{ej:tensor-matrices}
  Calcular los siguientes productos tensoriales de matrices / vectores.

  \begin{multicols}{3}
    \begin{enumerate}
      \item\label{ite:matrices}
        $\matriz{ 1 \\ 2 }\otimes\matriz{ 3 & 4 }$
      \item $\matriz{3 & 4}\otimes\matriz{1 \\ 2}$
      \item $\matriz{2 \\ 1}\otimes\matriz{a & b\\ c & d}$
      \item $\matriz{a & b\\ c & d}\otimes\matriz{2 \\ 1}$
      \item $\matriz{0\\0\\1}\otimes\matriz{1\\0}$
      \item $\matriz{1 & 0\\0 & 1}\otimes\matriz{0 & 1 \\ 1 & 0}$
    \end{enumerate}
  \end{multicols}
\end{ejercicio}


Como se dijo anteriormente, $E\times F\neq E\otimes F$, y por lo tanto:
\begin{center}
  \fbox{\parbox{.55\textwidth}{\centering Existen vectores de $E\otimes F$ que
      no son producto tensorial entre uno de $E$ y uno de $F$.}}
\end{center}

\begin{ejemplo}
  Consideremos el espacio $\mathbb{C}^2\otimes \mathbb{C}^2$. Una base de
  $\mathbb{C}^2$ es $\{\matriz{0\\1}, \matriz{1\\0}\}$ Por lo tanto
  \[
    \mathbb{C}^2\otimes \mathbb{C}^2 = \textrm{Gen}(\left\{ \matriz{0\\ 0\\ 0\\
        1}, \matriz{0\\ 0\\ 1\\ 0}, \matriz{0\\ 1\\ 0\\ 0}, \matriz{1\\ 0\\ 0\\
        0} \right\}) = \mathbb{C}^4
  \]

  Tomemos $\vec v=(\alpha,0,0,\beta)^T$, con $\alpha,\beta\neq 0$. Es fácil
  verificar que $\vec v\in\mathbb{C}^4$. Sin embargo, no existen $\vec v_1, \vec
  v_2\in\mathbb{C}^2$ tal que $\vec v=\vec v_1\otimes\vec v_2$.

  \begin{proof}
    Supongamos que existen $\vec v_1$ y $\vec v_2$ tales que $\vec
    v_1\otimes\vec v_2=\vec v$, entonces
    \[
      \matriz{
        a \\
        b } \otimes \matriz{
        c \\
        d } = \matriz{
        ac \\
        ad \\
        bc \\
        bd } = \matriz{
        \alpha \\
        0 \\
        0 \\
        \beta } \Rightarrow \left\lbrace
        \begin{array}{l}
          ac = \alpha \\
          ad = 0 \\
          bc = 0 \\
          bd = \beta
        \end{array}
      \right.
    \]
    pero este es un sistema que no tiene solución.
  \end{proof}
\end{ejemplo}

\subsection{Notación bra--ket}
Notación introducida por Paul \citet{DiracMPCPS39} para describir estados
cuánticos.
\subsubsection{Notación bra y ket para vectores}
En lugar de escribir los vectores como $\vec v$ la notación ket usa $\ket{v}$.

En particular definimos:
\[
  \ket{0}=\matriz{1\\0} \qquad \ket{1}=\matriz{0\\1}
\]
Por lo tanto, cualquier vector de $\mathbb{C}^2$ puede escribirse como
\[
  \matriz{\alpha\\\beta} = \alpha\matriz{1\\0}+\beta\matriz{0\\1} =
  \alpha\ket{0}+\beta\ket{1}
\]
Podemos, por ejemplo, definir vectores como los siguientes
\[
  \ket{+}=\frac{1}{\sqrt{2}}(\ket{0}+\ket{1}) =
  \matriz{\frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}} \qquad \qquad
  \ket{-}=\frac{1}{\sqrt{2}}(\ket{0}-\ket{1}) =
  \matriz{\phantom{-}\frac{1}{\sqrt{2}}\\
    -\frac{1}{\sqrt{2}}}
\]
y como estos son dos vectores ortogonales (por ende, forman una base), ahora es
posible también escribir cualquier vector de $\mathbb{C}^2$ como combinación
lineal de $\ket{+}$ y $\ket{-}$.

Por ejemplo:
\[
  \matriz{\alpha\\\beta} = \alpha\ket{0}+\beta\ket{1} =
  \frac{1}{\sqrt{2}}(\alpha+\beta)\ket{+}+\frac{1}{\sqrt{2}}(\alpha-\beta)\ket{-}
\]

\begin{observacion}
  Al menos que se indique lo contrario, en el resto del apunte consideraremos el
  espacio complejo de dimensión $N=2^n$, $\mathbb{C}^N=\mathbb C^{2^n}$.
\end{observacion}

\begin{definicion}[Bra y Ket]
  Llamamos ket a un vector de la forma
  \[
    \ket{\psi}= \matriz{\alpha_1 \\ \vdots \\ \alpha_N }
  \]
  y bra a un vector de la forma
  \[
    \bra{\psi} = (\conj{\alpha_1},\ldots,\conj{\alpha_N})
  \]
  donde $\alpha_i \in \mathbb{C}$ y $\conj{\alpha_i}$ denota el conjugado de
  $\alpha_i$.
\end{definicion}

\begin{observaciones}\conlista
  \begin{itemize}
  \item Haciendo un abuso de notación, podemos escribir vectores como el
    siguiente:
    \[
      \ket{\alpha_1 \psi_1 + \alpha_2 \psi_2} = \alpha_1 \ket{\psi_1} + \alpha_2
      \ket{\psi_2}
    \]

  \item A partir la definición de bras y kets, llamamos ``braket'' al producto
    escalar:
    \[
      \braket{\psi}{\phi}= (\conj{\alpha_1},\ldots,\conj{\alpha_N}) \matriz{
        \beta_1 \\
        \vdots \\
        \beta_N } = a \in \mathbb{C}
    \]

  \item Recordatorio de álgebra: Una {\em base ortonormal} de un espacio
    vectorial normado es una base donde todos los vectores tienen norma $1$.
    Además, en una base, todos los vectores son ortogonales entre sí (es decir,
    el producto escalar entre ellos es $0$). Por lo tanto:

    Dado un conjunto $B=\{\ket{u_1},\dots,\ket{u_N}\}$, $B$ es una base
    ortonormal de $\mathbb{C}^N$ si y sólo si para todo $i$, $j$ tenemos
    $\braket{u_i}{u_j}=\delta_{ij}$, donde $\delta_{ij}$ es la delta de
    Kronecker (igual a $1$ si $i=j$, y $0$ en otro caso).

  \item Entonces, todo Ket $\ket{\psi}$ se puede expresar como
    $\ket{\psi}=\sum_{i=1}^{N}{a_i \ket{u_i}}$.

  \item Si tomamos la base canónica de $\mathbb{C}^N$, con $\ket{u_i}$ el vector
    $i$-ésimo de dicha base, podemos calcular la componente $i$-ésima de un
    vector cualquiera de la siguiente manera:
    \[
      \braket{u_i}{\psi}=\bra{u_i}\sum_{j=1}^{N}{a_j \ket{u_j}} =
      \sum_{j=1}^{N}{a_j \underbrace{{\braket{u_i}{u_j}}}_{\delta_{ij}}} = a_i
    \]
  \end{itemize}
\end{observaciones}

\begin{ejercicio}
  Escribir los siguientes vectores usando notación de Dirac en la base
  canónica.

  \begin{multicols}{3}
    \begin{enumerate}
      \item $\matriz{2\\ 3}$
      \item $\matriz{2 & 3}$
      \item $\matriz{1& 2 & 3 & i}$
    \end{enumerate}
  \end{multicols}
\end{ejercicio}

\begin{teorema}
  Sea $B=\{\ket{u_1},\dots,\ket{u_N}\}$ una base ortonormal, entonces \(
  \sum\limits_{i=1}^{N}{\ket{u_i}\bra{u_i}}=I \).
\end{teorema}
\begin{proof}

  \begin{align*}
    \left( \sum_{i=1}^{N}{\ket{u_i}\bra{u_i}}\right) \ket{\psi} 
    &=
      \left(\sum_{i=1}^{N}{\ket{u_i}\bra{u_i}}\right) \left(\sum_{j=1}^{N}{a_j \ket{u_i}}\right)\\
    & =\sum_{i=1}^{N}\sum_{j=1}^N{a_j\ket{u_i}\underbrace{\braket{u_i}{u_j}}_{\delta_{ij}}}=
      \sum_{i=1}^{N}{a_i\ket{u_i}}=\ket{\psi}
      \qedhere
  \end{align*}
\end{proof}

\begin{observaciones}\conlista
  \begin{itemize}
  \item Análogamente a los kets, todo bra $\bra{\phi}$ puede ser descompuesto
    como $\bra{\phi}=\sum\limits_{i=1}^{N}{\conj{b_i}\bra{u_i}}$.
  \item Podemos ver que $\conj{b_i}=\braket{\phi}{u_i} \in \mathbb{C}$ ya que
    \[
      \bra{\phi}=\bra{\phi}\underbrace{\left[ \sum_{i=1}^{N}{\ket{u_i}\bra{u_i}}
        \right]}_{I} =
      \sum_{i=1}^{N}{\braket{\phi}{u_i}\bra{u_i}}\quad\Rightarrow\quad\conj{b_i}=\braket{\phi}{u_i}
    \]
  \end{itemize}
\end{observaciones}

\begin{observacion}
  De aquí en más, trabajaremos sólo con los vectores normalizados de
  $\mathbb{C}^N$ (es decir, vectores cuya norma es $1$). Esto es
  \[
    1 = \norma{\psi}^2 = \braket{\psi}{\psi}= \left(
      \sum_{j=1}^{N}{\conj{a_j}\bra{u_j}} \right) \left( \sum_{i=1}^{N}{a_i
        \ket{u_i}} \right) = \sum_{i,j=1}^{N}{\conj{a_j} a_i
      \underbrace{\braket{u_j}{u_i}}_{\delta_{ij}}}=\sum_{i=1}^{N}{|a_i|^2}=1
  \]
  Es decir, trabajamos con vectores cuya suma de los módulos al cuadrado de sus
  componentes es $1$.
\end{observacion}

 \begin{propiedad}\label{prop:para-nc}
   \[
     \braket{ab}{cd}=\sum_{i=1}^{N}{\conj{a_i}c_i}{\sum_{j=1}^{N}}{\conj{b_j}
     d_j} = \braket{a}{c}\braket{b}{d}\;\;\;\forall a,b,c,d \in \mathbb{C}^N
   \]
 \end{propiedad}

\subsubsection{Notación bra y ket para matrices}
Para toda matriz cuadrada de dimensión $N$ a coeficientes complejos $A$, tenemos
la siguiente representación:
\[
  A= \left( \underbrace{\sum_{i=1}^{N}{\ket{u_i}\bra{u_i}}}_{I}\right) A \left(
    \underbrace{\sum_{j=1}^{N}{\ket{u_j}\bra{u_j}}}_{I}\right) =
  \sum_{i=1}^{N}\sum_{j=1}^N{\ket{u_i}\underbrace{\bra{u_i}\overbrace{A\ket{u_j}}^{\ket{vector}}}_{\alpha_{ij}}\bra{u_j}}=
  \sum_{i=1}^{N}\sum_{j=1}^N{\alpha_{ij}\ket{u_i}\bra{u_j}}
\]
donde $\alpha_{ij}$ es la componente $ij$ de la matriz.

Con esta representación, podemos representar el producto de una matriz por un
vector de la siguiente manera:
\begin{align*}
  A\ket{\psi}&=\left( \sum_{i=1}^{N}\sum_{j=1}^N{\alpha_{ij}\ket{u_i}\bra{u_j}}\right) 
               \left( \sum_{k=1}^{N}{a_k\ket{u_k}}\right)\\
             &=\sum_{i=1}^{N}\sum_{j=1}^N\sum_{k=1}^N{\alpha_{ij}a_k\ket{u_i}\underbrace{\braket{u_j}{u_k}}_{\delta_{jk}}}=
               \sum_{i=1}^{N}\sum_{j=1}^N{\alpha_{ij}a_j\ket{u_i}}
\end{align*}
Es decir, las componentes del vector $A\ket{\psi}$ son
$b_i=\sum_{j=1}^{N}{\alpha_{ij}a_j}$.

\begin{ejercicio}
  Reescribir las matrices obtenidas en el Ejercicio~\ref{ej:tensor-matrices}
  en notación de Dirac.
\end{ejercicio}
\begin{ejercicio}
  Usando notación de Dirac, calcular rápidamente
  \[
    \Bigl(\matriz{1\\2}\otimes\matriz{3 & 4}\Bigr)\matriz{2\\3}.
  \]
\end{ejercicio}

\section{Bits cuánticos y operadores}
\subsection{Primera intuición}
En computación clásica la unidad mínima de información es el bit, el cual puede
estar en un estado $0$ o $1$. Leer un bit es una operación que no conlleva
ninguna particularidad. En contraposición, un bit cuántico o qubit puede estar
en un estado que sea una superposición de los estados $0$ y $1$. Un qubit es un
vector de $\mathbb{C}^2$, por lo tanto lo podemos representar como $\alpha\ket
0+\beta\ket 1$, lo cual representa el estado que es $0$ y en $1$ a la vez. Leer
un qubit en cambio se produce a través de una operación llamada medición, y al
medir un qubit, éste colapsa, cambia su estado (dependiendo de la medición puede
cambiar por ejemplo a $\ket0$ o $\ket1$, pero también podría usarse otro
operador de medición que lo colapse a otra base).

\subsection{Bits cuánticos}
\begin{definicion}[Qubit]
  Un qubit o bit cuántico es un vector normalizado (es decir, con norma $1$) del
  espacio de Hilbert $\mathbb{C}^2$.
\end{definicion}
\begin{observacion}
  Considerando la base $\{\ket{0}, \ket{1}\}$ de $\mathbb{C}^2$, cualquier qubit
  puede escribirse como $\ket{\psi}=\alpha\ket{0}+\beta\ket{1}$, con
  $|\alpha|^2+|\beta|^2=1$.
\end{observacion}

\begin{definicion}[$n$-qubits]
  Un sistema de $n$-qubits es un vector normalizado del espacio
  $\mathbb{C}^{2^n}=\bigotimes\limits_{i=1}^{n}{\mathbb{C}^2}$.
\end{definicion}

\begin{observaciones}\conlista
  \begin{itemize}
  \item En lugar de escribir $\ket 0\otimes\ket 1\otimes\dots\otimes\ket 0$
    escribimos $\ket{01\dots0}$.

  \item De la misma manera, en ocasiones, en lugar de $\ket 0\otimes (\alpha\ket
    0+\beta\ket 1)$ escribimos simplemente $\ket 0(\alpha\ket 0+\beta\ket 1)$.
  \item La base canónica del espacio $\mathbb C^{2^n}$ es $\{\ket{0 \ldots
      00},\ket{0 \ldots 01},\ldots,$ $\ket{1 \ldots 11}\}$.
  \end{itemize}
\end{observaciones}

Un algoritmo cuántico consiste en la evolución
 (Definición~\ref{def:evolucion}) de un sistema representado por $n$--qubits.


\begin{ejercicio}
  Mostrar que $\{\ket{00},\ket{01},\ket{10},\ket{11}\}$ es una base
  ortonormal del espacio $\mathbb C^4$ (con el producto interno usual).
  ¿De cuántos qubits se trata?
\end{ejercicio}
\subsection{Operadores}
\begin{definicion}[Operador]
  Un operador de $\mathbb{C}^{N}$ es una matriz cuadrada de dimensión $N$ a
  coeficientes complejos.
\end{definicion}


\begin{definicion}[Adjunto]\label{def:adjunto}
  El adjunto de un operador $A$ se nota por $\adj{A}$ y se define como el
  operador transpuesto y conjugado de $A$. Es decir, si
  $\alpha_{ij}=\bra{u_i}A\ket{u_j}$ son las componentes de $A$, las componentes
  de $\adj{A}$ son \( \conj{\alpha_{ji}}=\conj{\bra{u_j}A\ket{u_i}} =
  \bra{u_i}\adj{A}\ket{u_j} \).
\end{definicion}


\begin{propiedades}
  Sean $A$ y $B$ operadores de $\mathbb{C}^N$, $a\in\mathbb{C}$ y $\ket{\psi}
  \in \mathbb{C}^{N}$
  \begin{multicols}{3}
    \begin{itemize}
    \item $\adj{(\adj{A})}=A$
    \item $\adj{(A+B)}=\adj{A}+\adj{B}$
    \item $\adj{(a A)}=\conj{a}\adj{A}$
    \item $\adj{(AB)}=\adj{B}\adj{A}$
    \item $\bra{A\psi}=\bra{\psi}\adj{A}$
    \end{itemize}
  \end{multicols}
\end{propiedades}

\begin{ejercicio}
  Probar estas propiedades.
\end{ejercicio}

\begin{definicion}[Proyector]
  A los operadores de la forma $P=\ket{\phi}\bra{\phi}$ se les llama
  proyectores, ya que proyecta ortogonalmente un ket $\ket{\psi}$ cualquiera
  sobre el ket $\ket{\phi}$:
  \[
    P\ket{\psi}=\ket{\phi}\underbrace{\braket{\phi}{\psi}}_{a\in
      \mathbb{C}}=a\ket{\phi}
  \]
\end{definicion}
\begin{ejemplo}
  Tomemos la base $\{\ket{0},\ket{1}\}$, con $\ket 0=\matriz{1\\0}$ y
  $\ket{1}=\matriz{0\\1}$. Un vector $\ket{\psi}$ cualquiera puede escribirse
  como $\ket{\psi}=\alpha\ket 0+\beta\ket 1$. Por lo tanto
  \[
    \ket 0\braket 0 {\psi} = \ket 0\bra 0(\alpha\ket 0+\beta\ket 1) = \ket
    0(\alpha\underbrace{\braket 0 0}_1+\beta\underbrace{\braket 1 0}_0)
    =\alpha\ket 0
  \]
\end{ejemplo}

\begin{definicion}[Operador hermítico]
  Un operador $A$ es hermítico si $A=\adj A$.
\end{definicion}

\begin{ejercicio}
  Mostrar que si un operador $A$ es hermítico, entonces todos los elementos
  de la diagonal de su matriz (en cualquier base ortonormal) son reales.
\end{ejercicio}
 % Si es hermítico, su diagonal debe ser real, ya que $\alpha_{ij}=\conj{\alpha_{ji}}$, por lo tanto $\alpha_{ii}=\conj{\alpha_{ii}}$.


\begin{definicion}[Operador unitario]
  Un operador $U$ es unitario si $\adj{U}U=U\adj{U}=I$, o lo que es lo mismo
  $\adj{U}=\inv{U}$.
\end{definicion}

\begin{propiedades} Para cualquier operador $U$ unitario vale:
  \begin{itemize}
  \item $U$ preserva el producto interno:
    $\braket{U\phi}{U\psi}=\bra{\phi}\adj{U}U\ket{\psi}=\braket{\phi}{\psi}$
  \item $\inv{U}$ es unitario.
  \item Si $\{\ket{\psi_1},\dots,\ket{\psi_N}\}$ es base ortonormal, entonces
    $\{U\ket{\psi_1},\dots,U\ket{\psi_N}\}$ también lo es.
  \end{itemize}
\end{propiedades}

\begin{definicion}[Operador de medición]
  Un conjunto de proyectores $\{M_1,\dots,M_k\}$ se dice que es un operador de
  medición si satisface
  \[
    \sum_{i=1}^{k}{M_i \adj{M_i}}=I
  \]
Si los proyectores son proyectores sobre una base ortonormal, entonces el
operador de medición queda completamente determinado por dicha base.

Más precisamente, si $B=\{\ket{u_1},\dots,\ket{u_N}\}$ es una base ortonormal
del espacio, los operadores
\[
  P_i = \ket{u_i}\bra{u_i}
\]
forman un operador de medición, ya que cada $P_i$ es un proyector y
$\sum_i P_iP_i^\dagger = \sum_iP_i = I$.

En este caso, se dice que la medición se realiza \emph{respecto de la base}
$B$ (también llamada \emph{base de medición}).
\end{definicion}

\begin{ejercicio}
  Considerar el operador de medición $\{\ket +\bra +, \ket -\bra -\}$, con
  $\ket +=\nicefrac 1{\sqrt 2}\ket 0+\nicefrac 1{\sqrt 2}\ket 1$ y
  $\ket -=\nicefrac 1{\sqrt 2}\ket 0-\nicefrac 1{\sqrt 2}\ket 1$.
  Determinar los resultados posibles (y sus probabilidades) de medir con ese
  operador cada uno de los siguientes qubits:

  \begin{multicols}{3}
    \begin{enumerate}
      \item $\nicefrac 13\ket 0+\nicefrac{\sqrt 8}3\ket 1$
      \item $\nicefrac 1{\sqrt 2}(\ket 0+\ket 1)$
      \item $\ket -$
    \end{enumerate}
  \end{multicols}
\end{ejercicio}
\begin{ejercicio}
  Para cada par de estado y base de medición, dar los posibles resultados
  y sus probabilidades.

  \begin{multicols}{2}
    \begin{enumerate}
      \item $\nicefrac{\sqrt 3}2\ket 0-\nicefrac 12\ket 1$, base $\{\ket 0,\ket 1\}$.
      \item $\nicefrac{\sqrt 3}2\ket 1-\nicefrac 12\ket 0$, base $\{\ket 0,\ket 1\}$.
      \item $-i\ket 0$, base $\{\ket 0,\ket 1\}$.
      \item $-\ket 1$, base $\{\ket 0,\ket 1\}$.
      \item $\ket 0$, base $\{\ket +,\ket -\}$.
      \item $\ket +$, base
        $\left\{\nicefrac 12\ket 0+\nicefrac{\sqrt 3}2\ket 1,\;
        \nicefrac{\sqrt 3}2\ket 0-\nicefrac 12\ket 1\right\}$.
    \end{enumerate}
  \end{multicols}
\end{ejercicio}

\begin{definicion}[Compuertas cuánticas]
  A los operadores unitarios se les llama compuertas cuánticas, como analogía a
  las compuertas lógicas de la computación clásica, ya que serán esos los que se
  utilizan para realizar el cómputo.
\end{definicion}

\begin{observacion}
  La mayoría de las compuertas cuánticas que usaremos a lo largo del curso serán
  además de operadores unitarios, hermíticos, por lo que coinciden con su
  inversa.
\end{observacion}

\begin{definicion}[Evolución]\label{def:evolucion}
  Se dice que un sistema representado por un ket $\ket\psi$ evoluciona al
  sistema $\ket\phi$, cuando se realiza una de las siguientes operaciones:
  \begin{itemize}
  \item Se premultiplica por una compuerta cuántica $U$:
    \[
      \ket\phi=U\ket\psi
    \]
  \item Se aplica un operador de medición $M=\{M_1,\dots,M_k\}$ de la siguiente
    manera:
    \[
      \ket{\phi}=\frac{M_i\ket{\psi}}{\sqrt{\bra{\psi}\adj{M_i}M_i\ket{\psi}}}
      \textrm{ para algún } 1 \leq i \leq k
    \]
    La elección del $M_i$ no se conoce de antemano, sólo se conoce la
    probabilidad para cada $i$, la cual viene dada por la siguiente ley:
    \[
      p(i)=\bra{\psi}\adj{M_i}M_i\ket{\psi}
    \]
  \end{itemize}
\end{definicion}
\begin{observaciones}\conlista
  \begin{itemize}
  \item Usaremos también la notación $\ket\psi\xrightarrow{U}\ket\phi$ o
    $\ket\psi\xrightarrow{M}\ket\phi$ para indicar que el ket $\ket\psi$
    evoluciona al ket $\ket\phi$.
  \item Cuando se quiera hacer evolucionar sólo un qubit de un sistema de
    $n$-qubits, digamos el qubit $i$, se premultiplica tensorialmente $i-1$
    veces y se postmultiplica $n-i$ veces la compuerta a aplicar por la matriz
    identidad. Ejemplo: $U$ aplicada al segundo qubit de un sistema de
    $2$--qubits, será la compuerta $I\otimes U$.
  \end{itemize}
\end{observaciones}

\begin{ejemplo}
  Consideramos el operador medición de $\{M_0,M_1\}$ con
  \[
    M_0=\ket{0}\bra{0}= \matriz{
      1 & 0 \\
      0 & 0 } \qquad M_1=\ket{1}\bra{1}= \matriz{
      0 & 0 \\
      0 & 1 }
  \]
  Podemos verificar que $M_0\adj{M_0}+M_1\adj{M_1}=M_0+M_1=I$, y por lo tanto es
  un operador de medición.

  Sea $\ket{\psi}=\alpha\ket{0}+\beta\ket{1}$, entonces, la probabilidad de que
  el proyector que se aplique sea $M_0$ es
  \begin{align*}
    p(0) &=\bra{\psi}\adj{M_0}M_0\ket{\psi}\\
         &=(\conj{\alpha}\bra{0}+\conj{\beta}\bra{1})M_0(\alpha\ket{0}+\beta\ket{1})\\
         &=|\alpha|^2 {\bra{0}{M_0\ket{0}}}
           +\conj{\alpha}\beta {{\bra{0}M_0}\ket{1}}
           +\alpha\conj{\beta} {\bra{1}{M_0\ket{0}}}
           + |\beta|^2 {\bra{1}{M_0\ket{1}}}\\
         &
           =|\alpha|^2 {\underbrace{\braket 00}_1\underbrace{\braket 00}_1}
           +\conj{\alpha}\beta \underbrace{\braket 00}_1\underbrace{\braket 01}_0
           +\alpha\conj{\beta} \underbrace{\braket 10}_0\underbrace{\braket 00}_1
           + |\beta|^2 \underbrace{\braket 10}_0\underbrace{\braket 01}_0
    \\
         &=
           |\alpha|^2
  \end{align*}
  Análogamente, $p(1)=\bra{\psi}\adj{M_1}M_1\ket{\psi}=\dots=|\beta|^2$.

  Dado que el vector está normalizado,
  $p(0)+p(1)=|\alpha|^2+|\beta|^2=\norma{\psi}=1$.

  Luego de aplicar este operador de medición, la evolución es la siguiente. Si
  se aplicó el proyector $M_0$, el sistema queda en el siguiente estado:
  \[
    \frac{M_0\ket{\psi}}{\sqrt{\bra{\psi}\adj{M_0}M_0\ket{\psi}}}=\frac{M_0\ket{\psi}}{\sqrt{p(0)}}=\frac{\alpha}{|\alpha|}\ket{0}
  \]

  Este estado está normalizado ya que ${\left\vert \frac{\alpha}{|\alpha|}
    \right\vert}^2 = \frac{|\alpha|^2}{|\alpha|^2}=1$. \medskip

  Análogamente si se aplicó $M_1$ se obtiene \(
  \frac{M_1\ket{\psi}}{\sqrt{p(1)}}=\frac{\beta}{|\beta|}\ket{1} \).
\end{ejemplo}


\begin{definicion}[Compuertas más comunes y operadores de Pauli]\label{def:comp}
  Las compuertas cuánticas más importantes, por su utilidad en el diseño de
  algoritmos, son las siguientes:

  \begin{itemize}
    \item La compuerta $H$ de Hadamard:
      $$\begin{array}{ccc}
	\begin{array}{c}
	  H\ket{0}=\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})\\
	  H\ket{1}=\frac{1}{\sqrt{2}}(\ket{0}-\ket{1})
	\end{array} &
	\textrm{ donde: } &
	H=\frac{1}{\sqrt{2}}
	\matriz{
	  1 & \phantom{-}1\\
	  1 & -1
	}
      \end{array}$$
    \item La identidad $I$:
      $$\begin{array}{ccc}
	\begin{array}{c}
	  I\ket{0}=\ket{0}\\
	  I\ket{1}=\ket{1}
	\end{array} &
	\textrm{ donde: } &
	I=
	\matriz{
	  1 & 0\\
	  0 & 1
	}
      \end{array}$$
    \item La negación $X$:
      $$\begin{array}{ccc}
	\begin{array}{c}
	  X\ket{0}=\ket{1}\\
	  X\ket{1}=\ket{0}
	\end{array} &
	\textrm{ donde: } &
	X=
	\matriz{
	  0 & 1\\
	  1 & 0
	}
      \end{array}$$
    \item El cambio de fase $Z$:
      $$\begin{array}{ccc}
	\begin{array}{c}
	  Z\ket{0}=\ket{0}\\
	  Z\ket{1}=-\ket{1}
	\end{array} &
	\textrm{ donde: } &
	Z=
	\matriz{
	  1 & \phantom{-}0\\
	  0 & -1
	}
      \end{array}$$
    \item La No-controlada $CNOT$:
      $$\begin{array}{ccc}
	\begin{array}{l}
	  CNOT\ket{0x}=\ket{0x}\\
	  CNOT\ket{1x}=\ket{1}\otimes X\ket{x}
	\end{array} &
	\textrm{ donde: } &
	CNOT=
	\matriz{
	  I & 0\\
	  0 & X
	}
      \end{array}$$
  \end{itemize}

  En particular, las matrices $I$, $X$, $iXZ$ (llamada $Y$) y $Z$ son las llamadas {\em
  matrices de Pauli} en honor a Wolfgang Pauli
\end{definicion}

\begin{ejercicio}
  Mostrar que las compuertas cuánticas $H$, $I$, $X$, $Z$, $Y$ y $CNOT$
  están bien definidas; es decir, mostrar que efectivamente son compuertas
  cuánticas (unitarias).
\end{ejercicio}


\section{Teorema del no-clonado}
El teorema de no-clonado \citep{WoottersZurekNature82} dice que es imposible
construir una máquina universal de clonado. Es decir, no podemos copiar un qubit
arbitrario, ya que no existe ningún método que pueda copiarlo sin saber su
estado preciso, y como la medición cambia el qubit, no podemos saber su estado
preciso. En consecuencia, no podemos copiar un qubit arbitrario.

\begin{teorema}[No-cloning]\label{thm:no-cloning}
  No existe ninguna compuerta cuántica $U$ tal que para algún $\ket\phi \in
  \mathbb{C}^N$ y $\forall \ket{\psi} \in \mathbb{C}^{N}$ se cumpla \(
  U\ket{\psi\phi}=\ket{\psi\psi} \).
\end{teorema}
\begin{proof}
  Supongamos que existe la operación $U$ de la cual se habla en el teorema,
  entonces, dados cualesquiera $\ket{\psi}, \ket{\phi} \in \mathbb{C}^N$, se
  cumple
  \begin{align*}
    U\ket{\psi\phi} &=\ket{\psi\psi}\\
    U\ket{\varphi\phi} &=\ket{\varphi\varphi}
  \end{align*}
  Por lo tanto, \(
  \braket{U\psi\phi}{U\varphi\phi}=\braket{\psi\psi}{\varphi\varphi} \). Sin
  embargo, por un lado
  \[
    \braket{U\psi\phi}{U\varphi\phi}=\bra{\psi\phi}\adj{U}U\ket{\varphi\phi}=\braket{\psi\phi}{\varphi\phi}=\braket{\psi}{\varphi}\braket{\phi}{\phi}=\braket{\psi}{\varphi}
  \]
  Mientras por el otro \(
  \braket{\psi\psi}{\varphi\varphi}=\braket{\psi}{\varphi}\braket{\psi}{\varphi}={\braket{\psi}{\varphi}}^2
  \)

  Pero si $\braket{\psi}{\phi}={\braket{\psi}{\phi}}^2$, entonces
  $\braket{\psi}{\phi}=0$ o $\braket{\psi}{\phi}=1$, lo cual es imposible: $0$
  implica que los dos vectores tomados al azar son ortogonales, y $1$ que son
  iguales.
\end{proof}

\begin{ejercicio}
  Usando sólo compuertas vistas en la Definición~\ref{def:comp}, construir una máquina
  de clonado que clone los qubits $\ket 0$ y $\ket 1$
  (no importa lo que haga con otros qubits, por lo tanto, dicha máquina no
  será universal).
\end{ejercicio}

\begin{ejercicio}
  Usando sólo compuertas vistas en la Definición~\ref{def:comp}, construir una máquina
  de clonado que clone los qubits $\ket +$ y $\ket -$.
\end{ejercicio}

\section{Estados de Bell}
Consideremos el siguiente {\em circuito} cuántico
\begin{eqnarray*}
  \Qcircuit @C=1em @R=1em {
  \lstick{\ket{x}} & \gate{H} \qw & \ctrl{2} \qw & \qw \\
                   &              &              & \rstick{\beta_{xy}} \\
  \lstick{\ket{y}} & \qw          & \targ \qw    & \qw
                                                   }
\end{eqnarray*}
Es decir, partiendo del estado inicial $\ket{xy}$, se aplica $H$ al primer
qubit. Luego se aplica $CNOT$ a ambos, donde el primero es el de control
(marcado con el punto negro). En otras palabras, este circuito representa la
siguiente ecuación:
\[
  \beta_{xy} = CNOT (H\otimes I) \ket{xy}
\]

Las posibles salidas de este circuito, cuando $x$ e $y$ varían entre $0$ y $1$
son las siguientes:
\begin{align*}
  \ket{00} & \xrightarrow{H(1)}
             \frac{1}{\sqrt{2}}
             \left(
             \ket{0} + \ket{1}
             \right)
             \ket{0} = 
             \frac{1}{\sqrt{2}}
             \left(
             \ket{00} + \ket{10}
             \right)
             \xrightarrow{CNOT(1,2)}
             \frac{1}{\sqrt{2}}
             \left(
             \ket{00} + \ket{11}
             \right) = \beta_{00}
  \\
  \ket{01} & \xrightarrow{H(1)}
             \frac{1}{\sqrt{2}}
             \left(
             \ket{0} + \ket{1}
             \right)
             \ket{1} = 
             \frac{1}{\sqrt{2}}
             \left(
             \ket{01} + \ket{11}
             \right)
             \xrightarrow{CNOT(1,2)}
             \frac{1}{\sqrt{2}}
             \left(
             \ket{01} + \ket{10}
             \right) = \beta_{01}
  \\
  \ket{10} & \xrightarrow{H(1)}
             \frac{1}{\sqrt{2}}
             \left(
             \ket{0} - \ket{1}
             \right)
             \ket{0} = 
             \frac{1}{\sqrt{2}}
             \left(
             \ket{00} - \ket{10}
             \right)
             \xrightarrow{CNOT(1,2)}
             \frac{1}{\sqrt{2}}
             \left(
             \ket{00} - \ket{11}
             \right) = \beta_{10}
  \\
  \ket{11} & \xrightarrow{H(1)}
             \frac{1}{\sqrt{2}}
             \left(
             \ket{0} - \ket{1}
             \right)
             \ket{1} = 
             \frac{1}{\sqrt{2}}
             \left(
             \ket{01} - \ket{11}
             \right)
             \xrightarrow{CNOT(1,2)}
             \frac{1}{\sqrt{2}}
             \left(
             \ket{01} - \ket{10}
             \right) = \beta_{11}
\end{align*}

\begin{observacion}
  $\beta_{00}=(X\otimes I)\beta_{01}=(Z\otimes I)\beta_{10}=(XZ\otimes
  I)\beta_{11}$.
\end{observacion}

A estos cuatro estados se les llama {\em Estados de Bell}, en honor a John
S.~Bell. Estos son estados {\em entrelazados}, es decir, estados que no pueden
representarse como el producto tensorial de dos estados individuales.

A los estados entrelazados también se les llama estados EPR por
\citet*{EinsteinPodolskyRosenPR35} quienes detectaron, en pleno auge de las
formulaciones de la teoría cuántica, que existía una acción a distancia que
parecía no razonable. Por muchos años se llamó la ``paradoja EPR''. Lo que
determinaron es que cuando se tiene un par entrelazado (físicamente el estado
representa por ejemplo el {\em spin} en un par de electrones, o la polarización
de un par de fotones), sucede que cuando se colapsa (por acción de la medición)
un estado del par, el segundo también colapsará, incluso cuando físicamente se
encuentren a años luz de distancia. Con el tiempo se demostró experimentalmente
que esto es exactamente lo que sucede, y por lo tanto no hay paradoja. También
se demuestra que esto no contradice la teoría de la relatividad (que entre otras
cosas determina que nada puede viajar a mayor velocidad que la luz, ni siquiera
la información), ya que no hay trasmisión de información en este colapso a
distancia.

Matemáticamente la acción de medir un estado de un par se ve con el siguiente
ejemplo:

\begin{ejemplo}
  Consideremos el siguiente operador de medición: $M=\{M_0, M_1\}$ donde
  $M_{0}=\ket{0}\bra{0}$ y $M_{1}=\ket{1}\bra{1}$.

  Aplicando este operador al primer qubit del estado $\beta_{00}$, se obtiene
  uno de los siguientes resultados:
  \begin{itemize}
  \item Si se aplica el proyector $M_{0}$ (el cual lo expresamos como
    $M_{0}\otimes I$ para que se aplique $M_{0}$ al primer qubit y la identidad
    al segundo), el estado resultante será
    \begin{align*}
      \frac{(M_0\otimes I)\beta_{00}}{\sqrt{p(0)}}
      &=\frac{
        (\ket{00}\bra{00}+\ket{01}\bra{01})
        \frac{1}{\sqrt{2}}
        (\ket{00}+\ket{11})
        }{\sqrt{
        \frac{1}{\sqrt{2}}(\bra{00}+\bra{11})(\ket{00}\bra{00}+\ket{01}\bra{01})\frac{1}{\sqrt{2}}(\ket{00}+\ket{11})
        }}\\
      &=\frac{
        \frac{1}{\sqrt{2}}(\ket{00}\braket{00}{00})
        }
        {\sqrt{\frac{1}{2}\braket{00}{00}\braket{00}{00}}
        }
        =\ket{00}
    \end{align*}

    Análogamente, si se aplica $M_1$ se obtiene $\ket{11}$
  \end{itemize}
  Es decir, al medir el primer qubit del estado entrelazado $\beta_{00}$, se
  obntiene $\ket{00}$ o $\ket{11}$, es decir que ambos qubits colapsan.
\end{ejemplo}


\begin{ejercicio}
  Dar un circuito que genere el estado
  $\nicefrac 1{\sqrt 2}\ket{000}+\nicefrac 1{\sqrt 2}\ket{111}$
  a partir de la entrada $\ket{000}$.
\end{ejercicio}

\begin{ejercicio}
  ¿Cuáles son los resultados posibles al medir
  $\nicefrac 1{\sqrt 2}\ket{000}+\nicefrac 1{\sqrt 2}\ket{111}$ en la base
  canónica? Dar las probabilidades.
\end{ejercicio}

\begin{ejercicio}
  ¿Cuáles son los resultados posibles al medir el estado
  $\alpha\ket{000}+\alpha\ket{001}+\alpha\ket{100}$? Determinar el valor de
  $\alpha$ para que el estado esté normalizado.
\end{ejercicio}

\section{Usando los estados de Bell}
Como se mencionó en la sección anterior, el colapso de un par entrelazado no
transmite información (y por eso no viola la teoría de la relatividad), sin
embargo, es posible utilizar dicho colapso como canal de comunicación, el cual
necesita también de un canal clásico para terminar la transmisión (y por ende,
el canal clásico implica todas las limitaciones impuestas por la relatividad).

El algoritmo cuántico descripto en la sección~\ref{sec:codSuper}, descripto por
primera vez por \citet{BennettWiesnerPRL92}, permite transmitir dos bit
clásicos, enviando sólo un bit cuántico, utilizando un par entrelazado como
canal de comunicación. Es llamado ``codificación superdensa'' ya que se trata de
codificar dos bits de información en un bit cuántico, o dicho de otro modo: dos
bits de información en el estado de una partícula cuántica.

El algoritmo descripto en la sección~\ref{sec:telep}, descripto por primera vez
por \citet*{BennettBrassardCrepeauJozsaPeresWoottersPRL93}, permite enviar un
bit cuántico enviando dos bit clásicos y utilizando un par entrelazado como
canal de comunicación. Es llamado ``teleportación cuántica'' ya que se trata de
mover el valor de un bit cuántico (recordemos que un bit cuántico no puede ser
copiado (ver Teorema~\ref{thm:no-cloning})) a otro bit cuántico, o dicho de otro
modo: se trata de teletransportar el estado de una partícula a una nueva
partícula, destruyendo la primera.

\subsection{Codificación superdensa}\label{sec:codSuper}
El objetivo de esta técnica es transmitir 2 bits clásicos enviando tan sólo 1
qubit.

Los pasos a seguir por el emisor (a quien llamaremos ``Alice'') y el receptor (a
quien llamaremos ``Bob'') son los siguientes.
\begin{enumerate}%[label=Paso \arabic*.]
\item Alice y Bob preparan un estado $\beta_{00}$.
\item Alice se queda con el primer qubit del par y Bob se lleva el segundo.
  Podemos considerar que estos dos pasos son la preparación del canal cuántico.
  \begin{observacion}
    El estado entrelazado no se puede separar en el sentido de que no puede
    considerarse matemáticamente como un qubit multiplicado tensorialmente por
    otro qubit. Debemos considerarlos como un vector del espacio
    $\mathbb{C}^2\otimes\mathbb{C}^2$, es decir, un vector de dimensión 4. Pero
    físicamente son un par de electrones, o fotones (u otra partícula
    elemental), las cuales sí pueden ser separadas físicamente (más allá de que
    no es trivial el problema experimental que representa manipular dichas
    partículas sin que interaccionen con el ambiente).
  \end{observacion}
\item Alice aplica una transformación a su qubit, de acuerdo a los bits que
  quiere enviar: $Z^{b_1}X^{b_2}$, donde $C^0=I$ y $C^1=C$.
\item Alice envía su qubit a Bob.
\item Bob aplica CNOT a los dos elementos del par y luego Hadamard al primero.
\item Bob realiza una medición.
\end{enumerate}

El circuito completo queda de la siguiente manera
\begin{eqnarray*}
  \Qcircuit @C=1em @R=1em {
  & \gate{Z^{b_1}X^{b_2}} \qw	& \push{|} \qw	& \ctrl{2} \qw	& \gate{H}	& \meter	& \qw	& \rstick{\ket{b_1}} \\
  \lstick{\beta_{00}} & & \push{|} & & & & & \\
  & \qw				& \push{|} \qw	& \targ \qw	& \qw		& \meter	& \qw	& \rstick{\ket{b_2}}
                                                                    }
\end{eqnarray*}
donde la línea punteada determina el paso 4, en el que Alice envía su qubit a
Bob.

\begin{ejemplo}
  Se quiere enviar los bits $11$. Por lo tanto se aplica $(ZX\otimes I)$ a
  $\beta_{00}$, con lo que se obtiene $\beta_{11}$ (en general, la aplicación de
  la compuerta $Z^{b_1}X^{b_2}$ cambia el estado $\beta_{00}$ a
  $\beta_{b_1b_2}$):
  \begin{align*}
    (ZX\otimes I)\beta_{00}
    &=(Z\otimes I)\left((X\otimes I)\beta_{00}\right)\\
    &=(Z\otimes I)\left((X\otimes I)\frac{1}{\sqrt{2}}(\ket{00}+\ket{11})\right)\\
    &=(Z\otimes I)\left(\frac{1}{\sqrt{2}}(\ket{10}+\ket{01})\right)\\
    &=\frac{1}{\sqrt{2}}(-\ket{10}+\ket{01})=\beta_{11}
  \end{align*}

  El resto del circuito (a partir de la línea punteada vertical) es el circuito
  inverso al de Bell, y como toda compuerta unitaria es tal que $U=U^{-1}$,
  aplicando el circuito inverso al de Bell se obtiene los estados iniciales. En
  este caso, $\ket{11}$.
\end{ejemplo}

\subsection{Teleportación cuántica}\label{sec:telep}
El objetivo de esta técnica es transmitir un qubit mediante el envío de dos bits
clásicos.

Los pasos a seguir por Alice y Bob son los siguientes.
\begin{enumerate}
\item Alice y Bob preparan un estado $\beta_{00}$.
\item Alice se queda con el primer qubit del par y Bob se lleva el segundo.
\item Alice aplica CNOT entre el qubit a transmitir y el primero del par
  $\beta_{00}$, y luego Hadamard al primero.
\item Alice realiza una medición sobre los dos qubits en su posesión y envía el
  resultado de la medición (2 bits clásicos) a Bob.
\item Bob aplica una transformación sobre su qubit, de acuerdo a los bits
  recibidos: $Z^{b_1}X^{b_2}$.
\end{enumerate}

El circuito completo queda de la siguiente manera
\begin{eqnarray*}
  \Qcircuit @C=1em @R=1em {
  \lstick{\ket{\psi}} & \ctrl{1} \qw & \gate{H} \qw & \meter & \controlo \cw \cwx[1] \\
                      & \targ \qw    & \qw          & \meter & \controlo \cw \cwx \\
  \lstick{\beta_{00}} & \push{\rule{0em}{1ex}} & \push{\rule{0em}{1ex}} & \push{\rule{0em}{1ex}} & \cwx \\
                      & \qw          & \qw          & \qw    & \gate{Z^{b_1}X^{b_2}} \cwx & \qw & \rstick{\ket{\psi}}
                                                                                                  }
\end{eqnarray*}
\noindent donde $\ket{\psi}$ es el qubit a transmitir (o ``teleportar'').

\begin{ejemplo}
  Se quiere transmitir el qubit $\ket{\psi}=\alpha\ket{0}+\beta\ket{1}$,
  entonces
  \begin{align*}
    \ket{\psi} \otimes \beta_{00}
    &=(\alpha\ket{0}+\beta\ket{1})\left(\frac{1}{\sqrt{2}}(\ket{00}+\ket{11})\right)\\
    &=\frac{1}{\sqrt{2}}\left(\alpha\ket{0}(\ket{00}+\ket{11})+\beta\ket{1}(\ket{00}+\ket{11})\right)\\
    &\xrightarrow{CNOT(1,2)}
      \frac{1}{\sqrt{2}}\left(\alpha\ket{0}(\ket{00}+\ket{11})+\beta\ket{1}(\ket{10}+\ket{01})\right)\\
    &\xrightarrow{H(1)}
      \frac{1}{\sqrt{2}}\left(\alpha\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})(\ket{00}+\ket{11})+\beta\frac{1}{\sqrt{2}}(\ket{0}-\ket{1})(\ket{10}+\ket{01})\right)\\
    &=\frac{1}{2}\left[\ket{00}(\alpha\ket{0}\!+\!\beta\ket{1}) 
      +\ket{01}(\alpha\ket{1}\!+\!\beta\ket{0}) 
      +\ket{10}(\alpha\ket{0}\!-\!\beta\ket{1}) 
      +\ket{11}(\alpha\ket{1}\!-\!\beta\ket{0})\right]\\
    &=\frac{1}{2}\sum_{b_1=0}^{1}\sum_{b_2=0}^1\ket{b_1b_2}(X^{b_2}Z^{b_1})\ket{\psi}
  \end{align*}
  Por lo tanto, aplicando $Z^{b_1}X^{b_2}$, Bob obtendrá el estado original
  $\ket{\psi}$. (Nótese que para toda compuerta $U$, $U=U^{-1}$).
\end{ejemplo}

\begin{observacion}
  Si se quiere escribir la compuerta~ $\Qcircuit @C=1em @R=1em {&
    \gate{Z^{b_1}X^{b_2}} & \qw }$~como dos dos compuertas, debe
  escribirse~$\Qcircuit @C=1em @R=1em {& \gate{X^{b_2}} & \gate{Z^{b_1}} & \qw
  }$, ya que en $Z^{b_1}X^{b_2}$ primero se aplica la compuerta $X^{b_2}$ y
  luego $Z^{b_1}$.
\end{observacion}

\begin{ejercicio}
  Usar el algoritmo de teleportación dos veces para teleportar el 2-qubit
  $\beta_{00}$ (teleportar primero el primer qubit y luego el segundo
  qubit).
\end{ejercicio}

\begin{ejercicio}
  En base al ejercicio anterior, describir cómo generalizaría el algoritmo
  de teleportación a un estado de $n$ qubits.
\end{ejercicio}

\section{Paralelismo Cuántico}\label{sec:paralelismo}
Consideremos una función $f:\{0,1\}\rightarrow\{0,1\}$. Clásicamente para
obtener todos los resultados posibles de esta función, es necesario evaluarla
tantas veces como sea el cardinal del dominio (2 en este caso, una evaluación
para la entrada $0$, y otra para la entrada $1$).

Esta es una función que toma un bit y devuelve un bit. Si fuese un bit cuántico,
sería posible evaluar la función en una superposición de $0$ y $1$ (por ejemplo
$\frac{1}{2}(\ket 0+\ket 1)$), lo cual nos daría como resultado una
superposición de $f$ aplicada a $0$ y a $1$.

El método es el siguiente. Primero se debe construir una matriz unitaria $U_f$
de $\mathbb{C}^4$ que calcule la función, de la siguiente manera:
\[
  U_f\ket{x,0}=\ket{x,f(x)}
\]
En realidad, aunque vamos a usar la definición que acabamos de dar, se debe
definir también qué sucede cuando el segundo qubit es $\ket 1$, por lo que esta
compuerta se define más generalmente como $U_f\ket{x,y} = \ket{x,y\oplus f(x)}$,
donde $\oplus$ es la suma módulo $2$.

Lo que se pretende es aplicar $f$ a todas las entradas posibles, por lo que
primero se aplicará Hadamard al $\ket 0$, a fin de obtener una superposición, y
luego se aplicará la compuerta $U_f$. El circuito es el siguiente:
\begin{eqnarray*}
  \Qcircuit @C=1em @R=1em {
  \lstick{\ket{0}} & \gate{H} \qw & \multigate{2}{U_f} & \qw \\
                   & & & \rstick{\ket{\psi}=\frac{\ket{0,f(0)}+\ket{1,f(1)}}{\sqrt{2}}} \\
  \lstick{\ket{0}} & \qw          & \ghost{U_f}        & \qw
                                                         }
\end{eqnarray*}

Es decir:
\[
  \ket{00}\xrightarrow{H(1)}\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})\ket{0}
  =\frac{1}{\sqrt{2}}(\ket{00}+\ket{10})
  \xrightarrow{U_f}\frac{1}{\sqrt{2}}(\ket{0,f(0)}+\ket{1,f(1)})
\]

La salida de este circuito es un estado que es superposición de todos los
resultados posibles de la aplicación de la función $f$. Y la compuerta $U_f$ fue
utilizada una sola vez. El problema ahora pasa porque el resultado es una
superposición de todos los resultados posibles, y al querer leerlo (es decir, al
medirlo), éste colapsará a uno de los dos. El problema de los algoritmos
cuánticos pasa por utilizar la superposición de manera inteligente para
aprovechar el paralelismo, pero obteniendo el resultado buscado y no una
superposición de resultados sin utilidad.

En el siguiente capítulo mostraremos algunos de los algoritmos que, haciendo uso
del paralelismo, consiguen ganancias en complejidad respecto a su contrapartida
clásica.


\chapter{Algoritmos cuánticos y aplicación a criptografía}\label{ch:algos}
En este capítulo veremos algunos de los algoritmos cuánticos más conocidos. En
particular, los algoritmos de \citet{DeustchRSL85} y de
\citet{DeutschJozsaPRSLA92}, que pueden considerarse como los primeros
algoritmos cuánticos que hacen uso del paralelismo (ver
Sección~\ref{sec:paralelismo}). El algoritmo de \citet{GroverSTOC96}, que es uno
de los que motivó que los investigadores en computación se interesaran en el
área. No se incluye el algoritmo de \citet{ShorSIAM97}, el otro importante
algoritmo que motivó a investigadores en computación a adentrarse en el área.

Finalmente, el último ejemplo es una aplicación directa de la física cuántica en
criptografía, diseñado por \citet{BennettBrassardCSSP84}, la cual no sigue el
esquema de los otros algoritmos cuánticos presentados, pero es también el
puntapié de un área de investigación activa dentro de la computación cuántica.



\section{Algoritmo de Deutsch}
El objetivo de este algoritmo es saber si una función que toma un bit y devuelve
un bit, es constante o no.

El algoritmo se resume en el siguiente circuito
\[
  \Qcircuit @C=1em @R=1em {
    \lstick{\ket{0}} & \gate{H} \qw & \multigate{1}{U_f} & \gate{H} \qw & \meter & \qw \\
    \lstick{\ket{1}} & \gate{H} \qw & \ghost{U_f} & \qw & \qw & \qw }
\]

\begin{observacion}
  $U_f$ es la compuerta definida en la Sección~\ref{sec:paralelismo}, y viene dada por
  \[
    U_f\ket{x,y}=\ket{x,y\oplus f(x)}
  \]
\end{observacion}
\begin{ejercicio}
  Mostrar que $U_f$ es una compuerta cuántica para cualquier función booleana
  $f$ dada (es decir, que $U_f$ es un operador unitario).
\end{ejercicio}

Las primeras dos compuertas Hadamard, aplicadas a $\ket 0$ y $\ket 1$, producen
lo siguiente:
\begin{equation}
  \label{eq:DeutschHad12}
  \ket{01}\xrightarrow{H(1,2)}\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})\frac{1}{\sqrt{2}}(\ket{0}-\ket{1})=\ket{x}\frac{1}{\sqrt{2}}(\ket{0}-\ket{1})
  =\frac{1}{\sqrt 2}(\ket{x,0}-\ket{x,1})
\end{equation}
donde $\ket{x}=\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})$ es una abreviación
introducida por comodidad.

La aplicación de $U_f$ sobre el estado \eqref{eq:DeutschHad12} produce el
siguiente estado:
\begin{equation}
  \label{eq:DeutschUf}
  \begin{aligned}
    U_f&(\frac{1}{\sqrt{2}}(\ket{x,0}-\ket{x,1}))\\
    &=\frac{1}{\sqrt{2}}\left(U_f\ket{x,0}-U_f\ket{x,1}\right)\\
    &=\frac{1}{\sqrt{2}}\left(\frac{1}{\sqrt{2}}(\ket{0,f(0)}+\ket{1,f(1)})-\frac{1}{\sqrt{2}}(\ket{0,1\oplus f(0)}+\ket{1,1\oplus f(1)})\right)\\
    &=\frac{1}{2}\left(\ket{0,f(0)}+\ket{1,f(1)}-\ket{0,1\oplus
        f(0)}-\ket{1,1\oplus f(1)}\right)
  \end{aligned}
\end{equation}
Si $f(0)\neq f(1)$, \eqref{eq:DeutschUf} es igual a
\[
  \pm \frac{1}{2}\left(\ket{00}+\ket{11}-\ket{01}-\ket{10}\right) =\pm
  \left(\frac{\ket{0}-\ket{1}}{\sqrt{2}}\right)\left(\frac{\ket{0}-\ket{1}}{\sqrt{2}}\right)
\]
en cambio si $f(0)=f(1)$,
\[
  \pm \frac{1}{2}\left(\ket{00}+\ket{10}-\ket{01}-\ket{11}\right) =\pm
  \left(\frac{\ket{0}+\ket{1}}{\sqrt{2}}\right)\left(\frac{\ket{0}-\ket{1}}{\sqrt{2}}\right)
\]
Es decir, el primer qubit es $\pm\ket -$, si $f(0)\neq f(1)$ y $\pm\ket +$ si
$f(0)=f(1)$. Aplicando Hadamard al primer qubit, obtenemos $\ket 1$ si éste era
$\ket -$ y $\ket 0$ si éste era $\ket +$.
\begin{align*}
  \mbox{Si } f(0)\neq f(1)\mbox{, aplicando Hadamard se obtiene} &\pm\ket{1}\left[\frac{\ket{0}-\ket{1}}{\sqrt{2}}\right]\\
  \mbox{Si } f(0)=f(1)\mbox{, aplicando Hadamard se obtiene} & \pm\ket{0}\left[\frac{\ket{0}-\ket{1}}{\sqrt{2}}\right]
\end{align*}
es decir, aplicando Hadamard, se obtiene
\[
  \pm\ket{f(0)\oplus f(1)}\left[\frac{\ket{0}-\ket{1}}{\sqrt{2}}\right]
\]
Dado que el primer qubit es $\ket 0$ o $\ket 1$, podemos medirlo y nos dará con
probabilidad $1$ el valor $0$ si $f$ es constante y con probabilidad $1$ el
valor $1$ si $f$ no lo es.

\begin{observacion}
  Este algoritmo hace uso del paralelismo, ya que la evaluación de la función se
  realiza una vez sobre el estado en superposición de $0$ y $1$. El algoritmo
  clásico equivalente haría dos evaluaciones de la función y una comparación.
\end{observacion}

\begin{ejercicio}
  Escribir la traza del algoritmo de Deutsch para la función identidad.
\end{ejercicio}

\section{Algoritmo de Deutsch-Jotza}
Este algoritmo es una generalización del anterior. Dada una función que toma $n$
bits y devuelve uno, el algoritmo permite distinguir si la función es constante
o balanceada (o sea, con la mitad de las entradas devuelve $0$ y con la otra
mitad $1$). Sólo se distinguen esos dos casos, el algoritmo no es útil para otro
tipo de funciones.

El circuito es el siguiente:
\[
  \Qcircuit @C=1em @R=1em {
    \lstick{\ket{0}} & \gate{H} \qw & \multigate{3}{\;\;U_f\;\;} & \gate{H} \qw & \meter & \qw \\
    \lstick{\ket{0}} & \gate{H} \qw & \ghost{\;\;U_f\;\;}        & \gate{H} \qw & \meter & \qw \\
    & \push{\vdots} & \push{\rule{0em}{1em}} & \push{\vdots} & \push{\rule{0em}{1em}} & \push{\rule{0em}{1em}} \\
    \lstick{\ket{1}} & \gate{H} \qw & \ghost{\;\;U_f\;\;} & \qw & \qw & \qw }
\]
La entrada de este algoritmo son $n+1$ qubits: $\ket{0}^{\otimes
  n}\ket{1}=\ket{0\ldots 01}$.

Aplicando las $n+1$ compuertas Hadamard sobre la entrada, se obtiene
\begin{equation}
  \label{eq:DJHad}
  \left(\frac{\ket{0}+\ket{1}}{\sqrt{2}}\right)^{\otimes n}
  \left(\frac{\ket{0}-\ket{1}}{\sqrt{2}}\right)
  =
  \sum_{\overline{x}\in \{0,1\}^n}{
    \frac{\ket{\overline{x}}}{\sqrt{2^n}}
    \left[
      \frac{\ket{0}-\ket{1}}{\sqrt{2}}
    \right]
  }
\end{equation}
La compuerta $U_f$ que se utiliza es una generalización del caso anterior
definida por
\[
  U_f\ket{\overline{x},y}=\ket{\overline{x},y\oplus f(\overline{x})}
\]
donde $\overline x$ son cadenas de $n$ bits.

Es decir
\[
  U_f\ket{\overline{x},0}=\ket{\overline{x},f(\overline{x})} \qquad \qquad
  \qquad U_f\ket{\overline{x},1}=\ket{\overline{x},1\oplus f(\overline{x})}
\]
Por lo tanto, aplicando $U_f$ sobre el estado \eqref{eq:DJHad} se obtiene
\begin{equation}
  \label{eq:DJUf}
  \begin{aligned}
    U_f\left( \sum_{\overline{x}\in \{0,1\}^n}
      \frac{\ket{\overline{x}}}{\sqrt{2^n}} \left[
        \frac{\ket{0}-\ket{1}}{\sqrt{2}} \right] \right) &=
    \sum_{\overline{x}\in \{0,1\}^n} \frac{1}{\sqrt{2^n}}U_f\ket{\overline{x}}
    \left[ \frac{\ket{0}-\ket{1}}{\sqrt{2}} \right]
    \\
    &=\sum_{\overline{x}\in \{0,1\}^n} \frac{1}{\sqrt{2^{n+1}}} \left(
      U_f\ket{\overline{x},0}-U_f\ket{\overline{x},1} \right)
    \\
    &=\sum_{\overline{x}\in \{0,1\}^n} \frac{1}{\sqrt{2^{n+1}}} \left(
      \ket{\overline{x},f(\overline{x})}-\ket{\overline{x},1\oplus
        f(\overline{x})} \right)
    \\
    &=\sum_{\overline{x}\in \{0,1\}^n} \frac{1}{\sqrt{2^{n}}} \ket{\overline{x}}
    \left( \frac{\ket{f(\overline{x})}-\ket{1\oplus f(\overline{x})}}{\sqrt{2}}
    \right)
  \end{aligned}
\end{equation}
Para simplificar la notación, la compuerta Hadamard puede expresarse como sigue
\[
  \left.
    \begin{array}{r}
      H\ket{0}=\frac{1}{\sqrt{2}}(\ket{0}+\ket{1}) \\
      H\ket{1}=\frac{1}{\sqrt{2}}(\ket{0}-\ket{1})
    \end{array}
  \right\rbrace \Rightarrow
  H\ket{x}=\frac{1}{\sqrt{2}}\sum_{y\in\{0,1\}}{(-1)^{xy}\ket{y}}
\]
De la misma manera, es posible generalizar la aplicación de $H$ a $n$ qubits
como sigue:
\begin{samepage}
  \begin{align*}
    H^{\otimes n}\ket{x_1 \ldots x_n}
    &=\left(
      \frac{1}{\sqrt{2}}
      \sum_{z_1\in\{0,1\}}{(-1)^{x_1z_1}\ket{z_1}}
      \right) \cdots
      \left(
      \frac{1}{\sqrt{2}}
      \sum_{z_n\in\{0,1\}}{(-1)^{x_nz_n}\ket{z_n}}
      \right)\\
    &=\frac{1}{\sqrt{2^n}}
      \sum_{\overline{z}\in\{0,1\}^n}{(-1)^{\overline{x}\cdot\overline{z}}\ket{\overline{z}}}
  \end{align*}
  donde $\overline{x}\cdot\overline{z}=x_1z_1+\ldots+x_nz_n$.
\end{samepage}

Con esta notación, se aplica Hadamard a los primeros $n$ qubits del estado
\eqref{eq:DJUf} (es decir, al ket $\ket{\overline x}$), obteniendo
\begin{equation}
  \label{eq:DJHadFinal}
  \begin{aligned}
    \sum_{\overline{x}\in\{0,1\}^n} \frac{1}{\sqrt{2^n}} \left(
      \frac{1}{\sqrt{2^n}}
      \sum_{\overline{z}\in\{0,1\}^n}{(-1)^{\overline{x}\cdot\overline{z}}\ket{\overline{z}}}
    \right) &\left( \frac{\ket{f(\overline{x})}-\ket{1\oplus
          f(\overline{x})}}{\sqrt{2}} \right)
    \\
    = \sum_{\overline{x}\in\{0,1\}^n}{ \sum_{\overline{z}\in\{0,1\}^n}{ \frac{
          (-1)^{\overline{x}\cdot\overline{z}}\ket{\overline{z}} }{2^n} } }
    &\left( \frac{\ket{f(\overline{x})}-\ket{1\oplus f(\overline{x})}}{\sqrt{2}}
    \right)
  \end{aligned}
\end{equation}
Casos:
\begin{itemize}
\item Si $f$ es constante, el estado \eqref{eq:DJHadFinal} es
  \[
    \pm \sum_{\overline{x}\in\{0,1\}^n}{ \sum_{\overline{z}\in\{0,1\}^n}{ \frac{
          (-1)^{\overline{x}\cdot\overline{z}}\ket{\overline{z}} }{2^n} } }
    \left( \frac{\ket{0}-\ket{1}}{\sqrt{2}} \right)
  \]
  Cuando $\overline{z}=0$, los primeros $n$ qubits son
  \[
    \pm \sum_{\overline{x}\in\{0,1\}^n}{\frac{\ket{0}^{\otimes n}}{2^n}}
    =\pm\frac{2^n}{2^n}\ket{0}^{\otimes n} =\pm\ket{0}^{\otimes n}
  \]
  Por lo tanto, dado que este vector tiene norma $1$, el resto de los términos
  de la suma deben anularse, debido a que el resultado tiene que ser
  forzosamente un vector normalizado. Por lo tanto, cuando $f$ es constante, el
  estado \eqref{eq:DJHadFinal} es
  \[
    \pm\ket{0}^{\otimes n} \left[ \frac{\ket{0}-\ket{1}}{\sqrt{2}} \right]
  \]
  Es decir, midiendo los primeros $n$ qubits se obtiene $0\dots 0$ en este caso.

\item Si $f$ es balanceada ($50\%$ de las veces devuelve $0$ y $50\%$ devuelve
  $1$), entonces para $\overline{z}=0$
  \[
    \sum_{\overline{x}\in\{0,1\}^n} { \frac{\ket{0}^{\otimes n}}{2^n} \left(
        \frac{\ket{f(\overline{x})}-\ket{1\oplus f(\overline{x})}}{\sqrt{2}}
      \right) } = \sum_{\overline{x}\in\{0,1\}^n}{ (-1)^{\overline{x}}
      \frac{\ket{0}^{\otimes n}}{2^n} \left( \frac{\ket{0}-\ket{1}}{\sqrt{2}}
      \right)} =0
  \]
  Es decir que los primeros $n$ qubits no incluyen al qubit ${\ket 0}^{\otimes
    n}$, y por lo tanto, al medir los primeros $n$ qubits no se puede obtener
  $0\dots 0$ en este caso.
\end{itemize}
Conclusión: Si se obtiene $\ket{0}^{\otimes n}$ a la salida de la medición, la
función es constante, en otro caso la función es balanceada.

\begin{ejercicio}
  Escribir la traza del algoritmo de Deutsch-Jozsa para la función
  $f(x,y,z) = x\oplus y\oplus z$, donde $\oplus$ es la suma módulo $2$.
\end{ejercicio}

\section{Algoritmo de Búsqueda de Grover}
Antes de analizar este algoritmo, son necesarias algunas compuertas extras: la
compuerta {\em Oráculo} (Sección~\ref{sec:oraculo}), y la compuerta de {\em
  inversión sobre el promedio} (Sección~\ref{sec:inversionSobreElPromedio}).

\subsection{Oráculo}\label{sec:oraculo}
Dada una función de un bit en un bit $f$, la compuerta $U_f$ definida en la
Sección~\ref{sec:paralelismo}, es \( U_f\ket{x,y}=\ket{x,y\oplus f(x)} \).

Si se elije $y=\ket - = \frac{1}{\sqrt{2}}(\ket{0}-\ket{1})$, entonces
\begin{align*}
  U_f\ket{x,y}
  &=U_f \left( \ket{x}\frac{1}{\sqrt{2}}(\ket{0}-\ket{1})\right)\\
  &=\frac{1}{\sqrt{2}}\left(U_f\ket{x,0}-U_f\ket{x,1}\right)\\
  &=\frac{1}{\sqrt{2}}(\ket{x,f(x)}-\ket{x,1\oplus f(x)})\\
  &=\ket{x}\frac{1}{\sqrt{2}}(\ket{f(x)}-\ket{1\oplus f(x)})\\
  &=(-1)^{f(x)}\ket{x,y}
\end{align*}
Dado que $U_f$ no modifica el estado $y$, es posible omitirlo y tomarlo como
parte de la definición de la compuerta. Entonces, definimos la compuerta
\[
  U\ket{x}=(-1)^{f(x)}\ket{x}
\]
a la cual se le llama {\em Oráculo}.

\subsection{Inversión sobre el promedio}\label{sec:inversionSobreElPromedio}
Sea el estado $\ket{\phi}=\nicefrac{1}{\sqrt{2^n}}\sum_{\overline x\in
  \{0,1\}^n}{\ket{\overline x}}$. Definimos la compuerta de {\em Inversión sobre
  el promedio} como $G=2\ket{\phi}\bra{\phi}-I$. Es decir
\begin{align*}
  G&=2\ket{\phi}\bra{\phi}-I\\
   &=
     2\matriz{
     \frac{1}{\sqrt{2^n}}\\
  \vdots\\
  \frac{1}{\sqrt{2^n}}
  }_{2^n}
  \matriz{
  \frac{1}{\sqrt{2^n}} \cdots \frac{1}{\sqrt{2^n}}
  }_{2^n}
  -I\\
   &=\matriz{
     \frac{2}{2^n}-1	& \frac{2}{2^n} 	& \cdots	& \frac{2}{2^n}	\\
  \frac{2}{2^n}	& \frac{2}{2^n}-1	& \cdots	& \frac{2}{2^n}	\\
  \vdots		& \vdots		&		& \vdots	\\
  \frac{2}{2^n}	& \frac{2}{2^n}		& \cdots	& \frac{2}{2^n}-1 
                                             }_{2^n\times 2^n}
\end{align*}
La aplicación de $G$ sobre un estado cualquiera $\ket{\psi}=\sum_{\overline
  x\in\{0,1\}^n} a_{\overline x}\ket{\overline x}$ es la siguiente

\[
  \begin{array}{c|c}
    G\ket{\psi}&
                 \matriz{
                 a_0 \\
    \vdots\\
    a_{2^n-1}
    }\\
    \hline
    \matriz{
    \dfrac{2}{2^n}-1	& \cdots	& \dfrac{2}{2^n}	\\
    \vdots		& 		& \vdots	\\
    \dfrac{2}{2^n}	& \cdots	& \dfrac{2}{2^n}-1 
                               }
                                &
                                  \matriz{
                                  \left(\displaystyle\sum\limits_{\overline x\in \{0,1\}^n}{\frac{2 a_{\overline x}}{2^n}}\right)-a_0 \\
    \vdots \\
    \left(\displaystyle\sum\limits_{\overline x\in \{0,1\}^n}{\frac{2 a_{\overline x}}{2^n}}\right)-a_{2^n-1} 
    }
  \end{array}
\]
Es decir:
\[
  G\left(\sum_{\overline x\in \{0,1\}^n}{a_{\overline x} \ket{\overline
        x}}\right) = \sum_{\overline x\in \{0,1\}^n}{\left[\left(\sum_{\overline
          y\in \{0,1\}^n}{\frac{2 a_{\overline y}}{2^n}}\right) -a_{\overline x}
    \right]\ket{\overline x}} =\sum_{\overline x\in \{0,1\}^n}{(2A-a_{\overline
      x})\ket{\overline x}}
\]
donde $A$ es el promedio de los $a_{\overline x}$.

\subsection{El algoritmo}
El algoritmo de Grover es un algoritmo de búsqueda sobre una lista desordenada.
Suponemos una lista de tamaño $N$, con $N=2^n$ (observar que siempre es posible
aumentar la lista con datos irrelevantes para cumplir la condición sobre $N$).
Los índices de la lista son $\overline x\in{0,1}^n$, es decir $\overline
x=0\dots 2^n-1$.

El objetivo del algoritmo es localizar el $\overline x_0$ tal que $f(\overline
x_0)=1$, para una función booleana $f$ dada.

El input del circuito es $\ket{0}^{\otimes n}$.

\subsubsection{Paso 1: Se aplica Hadamard (\texorpdfstring{$H^{\otimes n}$}{H(x)n})}
El primer paso es generar una superposición en todos los qubits.
\begin{equation}
  \label{eq:GroverHad}
  \ket{0}^{\otimes n}~\xrightarrow{H^{\otimes n}}~
  \frac{1}{\sqrt{2^n}}\sum_{\overline x\in \{0,1\}^n}{\ket{\overline x}}
\end{equation}
Este estado es una superposición de todos los elementos de la lista. La idea del
algoritmo es subir la probabilidad de que al medir este estado obtengamos el
elemento $\overline x_0$.

\subsubsection{Paso 2: Se aplica el oráculo (\texorpdfstring{$U$}{U})}
Aplicar el oráculo es el equivalente a aplicar la función booleana $f$ sobre la
superposición.
\begin{equation}
  \label{eq:GroverOraculo}
  \eqref{eq:GroverHad}~\xrightarrow{U}~
  \frac{1}{\sqrt{2^n}}\sum_{\overline x\in \{0,1\}^n}{(-1)^{f(\overline x)}\ket{\overline x}}
\end{equation}

\subsubsection{Paso 3: Se aplica la inversión sobre el promedio
  (\texorpdfstring{$G$}{G})}
\begin{equation}
  \label{eq:GroverInvProm}
  \begin{aligned}
    \eqref{eq:GroverOraculo} & =\sum_{\overline x\in \{0,1\}^n} \underbrace{\left[\frac{(-1)^{f(\overline x)}}{\sqrt{2^n}}\right]}_{a_{\overline x}} \ket{\overline x}\\
    &\xrightarrow{G}  \sum_{\overline x\in \{0,1\}^n}{(2A-a_{\overline x})\ket{\overline x}}\\
    &=\sum_{\overline x\in \{0,1\}^n}{ \left[ \left( 2 \sum_{\overline y\in
            \{0,1\}^n}{ \frac{(-1)^{f(\overline y)}}{2^n\sqrt{2^n}} } \right)
        -\frac{(-1)^{f(\overline x)}}{\sqrt{2^n}} \right] \ket{\overline x}
    }\\
    &=\sum_{\overline x\in \{0,1\}^n}{ \left[ \left( 2 \sum_{\substack{\overline
              y\in \{0,1\}^n\\\overline y\neq \overline x}}{
            \frac{(-1)^{f(\overline y)}}{2^n\sqrt{2^n}} } \right) +
        \frac{2(-1)^{f(\overline x)}}{2^n\sqrt{2^n}} - \frac{(-1)^{f(\overline
            x)}}{\sqrt{2^n}} \right] \ket{\overline x} }
    \\
    &=\sum_{\overline x\in \{0,1\}^n}{ \left[ \left( 2 \sum_{\substack{\overline
              y\in \{0,1\}^n\\\overline y\neq\overline x}}{
            \frac{(-1)^{f(\overline y)}}{2^n\sqrt{2^n}} } \right) +
        \frac{2-2^n}{2^n\sqrt{2^n}}(-1)^{f(\overline x)} \right] \ket{\overline
        x} }
  \end{aligned}
\end{equation}
En el estado \eqref{eq:GroverInvProm}, el término $\overline x=\overline x_0$,
con $f(\overline x_0)=1$, el cual estamos buscando es el siguiente:
\begin{align*}
  \left[
  \left(
  2
  \sum_{\substack{\overline y\in \{0,1\}^n\\\overline y\neq\overline x_0}}{
  \frac{1}{2^n\sqrt{2^n}}
  }
  \right)
  +
  \frac{2^n-2}{2^n\sqrt{2^n}}
  \right]
  \ket{\overline x_0}
  &=\left[
    \frac{2}{2^n\sqrt{2^n}}(2^n-1)
    +
    \frac{2^n-2}{2^n\sqrt{2^n}}
    \right]
    \ket{\overline x_0}
  \\
  &=\left[
    \frac{2^{n+1}+2^n-4}{2^n\sqrt{2^n}}
    \right]
    \ket{\overline x_0}
\end{align*}
mientras que los otros términos, donde $\overline x\neq\overline x_0$, son
\[
  \left[ \left( 2 \sum_{\substack{\overline y\in \{0,1\}^n\\\overline
          y\neq\overline x_0\\\overline y\neq\overline x} }{
        \frac{1}{2^n\sqrt{2^n}}} \right) + \frac{2(-1)}{2^n\sqrt{2^n}} +
    \frac{2-2^n}{2^n\sqrt{2^n}} \right] \ket{\overline x} =\left[
    \frac{2^{n+1}-2^n-4}{2^n\sqrt{2^n}} \right] \ket{\overline x}
\]
El algoritmo ha cambiado las amplitudes del estado, aumentando la amplitud del
estado $\overline x_0$ y disminuyendo las otras.

Repitiendo este proceso (pasos 2 y 3) se va subiendo la amplitud del estado que
se quiere encontrar y disminuyendo las otras. Sin embargo es cíclico: pasado
cierto número de repeticiones, esa amplitud vuelve a decrecer. En la
Sección~\ref{sec:repeticiones} se calcula el número optimo de repeticiones para
obtener la amplitud máxima. Cuando la amplitud es máxima, se realiza una
medición, obteniendo el estado $\overline x_0$ con la máxima probabilidad. En la
Sección~\ref{sec:repeticiones} se muestra que la probabilidad de error tiene
cota máxima en $\nicefrac{1}{2^n}$.

\subsubsection*{Ejemplo}
Sea una lista de $2^4=16$ elementos, de los que sólo uno, $\overline x_0$,
verifica la propiedad $f(\overline x_0)=1$.

El algoritmo comienza por tomar el estado $\ket{0}^{\otimes 4}$ y aplicar
$H^{\otimes 4}$ obteniendo,
\[
  \frac{1}{4}\sum_{\overline x\in \{0,1\}^4}{\ket{\overline x}}
\]
Inicialmente todas las amplitudes son iguales a $\nicefrac{1}{4}$. Se aplica el
oráculo y se obtiene
\[
  \frac{1}{4}\sum_{\overline x\in \{0,1\}^4}{(-1)^{f(\overline x)}\ket{\overline
      x}}
\]
Luego se aplica la inversión sobre el promedio, y la nueva amplitud del estado
$\overline x_0$ será
\[
  \frac{2^{5}+2^4-4}{2^4\sqrt{2^4}}=\frac{11}{16}=0.6875
\]
y para el resto de los $\overline x$ la amplitud será
\[
  \frac{2^{5}-2^4-4}{2^4\sqrt{2^4}}=\frac{3}{16}=0.1875
\]
Con las sucesivas repeticiones de la aplicación del oráculo y la inversión sobre
el promedio, se obtienen las siguientes amplitudes:
\begin{center}
  \begin{tabular}{|c|c|c|c|}
    \hline
    Repetición & Amplitud de $\overline x_0$ & Amplitud de $\overline x \neq\overline x_0$ & Probabilidad de error \\ \hline 
    1 & 0.6875\phantom{0000} 	& \phantom{-}0.1875\phantom{0000} 	& 0.527 \\ \hline 
    2 & 0.953125\phantom{00} 	& \phantom{-}0.078125\phantom{00} 	& 0.092 \\ \hline 
    3 & 0.98046875	& -0.05078125 	& 0.039 \\ \hline 
  \end{tabular} 
\end{center}
A partir de la iteración $4$ la probabilidad de error comienza a subir, por lo
tanto el número óptimo de iteraciones es $3$, con una probabilidad de error de
$0.039$.

\subsection{Cálculo del número óptimo de iteraciones}\label{sec:repeticiones}
Luego de $k$ iteraciones $\overline x_0$ tendrá una amplitud $b_k$ y el resto
tendrán todos una amplitud $m_k$. Es decir, el estado será
\[
  b_k\ket{\overline x_0}+m_k\sum_{\substack{\overline x\in\{0,1\}^n\\\overline
      x\neq\overline x_0}}\ket{\overline x}
\]
En cada iteración se aplica el oráculo $U$, el cual cambia el signo de $b_k$, y
luego $G$. Es posible definir recursivamente las amplitudes en la repetición
$k$:

\parbox{0.5\textwidth}{
  \begin{align*}
    m_0=b_0 & =\frac{1}{\sqrt{2^n}}\\
    m_{k+1} & =2A_k-m_k\\
    b_{k+1} & =2A_k+b_k
  \end{align*}
} \parbox{0.4\textwidth}{
  \[
    \mbox{donde}\qquad A_k=\frac{(2^n-1)m_k-b_k}{2^n}
  \]
}

Las fórmulas cerradas para estas recursiones son
\begin{align*}
  m_k &=\frac{1}{\sqrt{2^n-1}} \cos((2k+1)\gamma)\\
  b_k &=\sen((2k+1)\gamma)
\end{align*}
donde \( \cos(\gamma)=\sqrt{\dfrac{2^n-1}{2^n}} \) y \(
\sen(\gamma)=\sqrt{\dfrac{1}{2^n}} \).

Para conseguir la mínima probabilidad de error, se debe minimizar $\vert m_k
\vert$. Notar que $m_k=0$ si y sólo si $(2k+1)\gamma = \dfrac{\pi}{2}$, es
decir, si $k=\dfrac{\pi}{4\gamma}-\dfrac{1}{2}$.

Sin embargo, dado que $k$ es el número de repeticiones, debe ser entero, por lo
tanto, el número óptimo de iteraciones es
\[
  \tilde{k}=\left\lfloor \frac{\pi}{4\gamma} \right\rfloor
\]

Para calcular una cota de la probabilidad de error, observar primero que que
$\vert k-\tilde{k} \vert \leq \dfrac{1}{2}$, entonces
\[
  \vert \frac{\pi}{2} - (2\tilde{k}+1)\gamma \vert = \vert (2k+1)\gamma -
  (2\tilde{k}+1)\gamma \vert = \vert 2\gamma(k-\tilde{k}) \vert \leq \gamma
\]
Con esto, podemos determinar que la probabilidad de error luego de $\tilde{k}$
iteraciones es
\[
  (2^n-1)(m_k)^2={cos}^2((2\tilde{k}+1)\gamma)={sen}^2(\frac{\pi}{2}-(2\tilde{k}+1)\gamma)\leq
  sen^2(\gamma)=\frac{1}{2^n}
\]

En el ejemplo anterior
\[
  \tilde{k}=\left\lfloor\frac{\pi}{4 asen{(\sqrt{\frac{1}{16}})}}\right\rfloor=3
\]
y la probabilidad de error es $0.039\leq \dfrac{1}{2^4}=0.0625$.

\begin{ejercicio}
  En una lista de un millón de elementos distintos:
  \begin{enumerate}
    \item ¿Cuál es el número óptimo de iteraciones del algoritmo de Grover
      para buscar un elemento marcado?
    \item ¿Cuál es la probabilidad de error con el número óptimo de
      iteraciones?
    \item ¿Cuántos pasos serían necesarios, en promedio, en el caso clásico
      (búsqueda aleatoria uniforme)?
  \end{enumerate}
\end{ejercicio}

\section{Aplicación criptográfica}

\subsection{One-time pad}

Este es un método de criptografía clásica \citep{VernamAIEE26} que consiste en
compartir una secuencia de bits (clave) del largo del mensaje a transmitir y
aplicar la operación reversible $XOR$ para cifrar y descifrar. (Ver Figura
\ref{fig:otp}). Las claves deben ser secretas y no deben ser reutilizadas.

\begin{figure}[!ht]
  \centering \includegraphics[width=10cm]{one-time}
  \caption{One-Time pad}
  \label{fig:otp}
\end{figure}


Este método es $100\%$ seguro: un $0$ en el mensaje encriptado puede significar
un $0$ en el mensaje original y un $0$ en la clave, o un $1$ en el mensaje y un
$1$ en la clave. Lo mismo sucede con un $1$ en el mensaje encriptado. Es decir
que adivinar la clave tiene la misma probabilidad que adivinar el mensaje
original. La única debilidad de este método es la predistribución de claves, ya
que el canal que se use para distribuirla podría ser vulnerado.

El método cuántico que se describe a continuación, QKD-BB84 (por {\em Quantum
  Key Distribution} de \citet{BennettBrassardCSSP84}), es justamente un método
para la distribución segura de claves.

\subsection{Criptosistema Cuántico QKD-BB84}
La idea es transmitir una clave binaria por un canal inseguro.

Para transmitir el bit $0$, Alice (el emisor) puede elegir, al azar, la base
$\{\ket{0},\ket{1}\}$ (a la que llamaremos esquema $+$) y considerar
$0\equiv\ket{0}$, o la base $\{\ket{-},\ket{+}\}$ (a la que llamaremos esquema
$\times$) y considerar $0\equiv\ket{-}$. Análogamente al bit $1$ lo codificamos
como $\ket{1}$ en el esquema $+$ o como $\ket{+}$ en el esquema $\times$.

Bob realizará una medición sobre el estado recibido eligiendo al azar entre el
esquema $+$ y el esquema $\times$. Ver ejemplo en Figura \ref{fig:ej}. El paso
final es intercambiar información (por un canal abierto) de los esquemas
utilizados, y sólo conservar los bits producidos usando el mismo esquema.

\begin{figure}[!ht]
  \centering
  \begin{tikzpicture}[very thick]
    \draw[very thick] (0,0) rectangle (2,2); \draw (1,1) node{Alice};
    \draw[->,very thick] (2,1) -- (3.5,1) node[pos=0.5,above]{$1$}; \draw[very
    thick] (3.5,0.5) rectangle (6.5,1.5); \draw (5,1) node{Esquema $+$};
    \draw[->,very thick] (6.5,1) -- (8,1) node[pos=0.5,above]{$\ket 1$};
    \draw[very thick] (8,0.5) rectangle (11,1.5); \draw (9.5,1) node{Esquema
      $\times$}; \draw[->,very thick] (11,1) -- (12.5,1) node[pos=0.5,above]{$1$
      o $0$}; \draw[very thick] (12.5,0) rectangle (14.5,2); \draw (13.5,1)
    node{Bob}; \draw (-1,1) node{$(2)$};
        %
    \draw[very thick] (0,3) rectangle (2,5); \draw (1,4) node{Alice};
    \draw[->,very thick] (2,4) -- (3.5,4) node[pos=0.5,above]{$1$}; \draw[very
    thick] (3.5,3.5) rectangle (6.5,4.5); \draw (5,4) node{Esquema $+$};
    \draw[->,very thick] (6.5,4) -- (8,4) node[pos=0.5,above]{$\ket 1$};
    \draw[very thick] (8,3.5) rectangle (11,4.5); \draw (9.5,4) node{Esquema
      $+$}; \draw[->,very thick] (11,4) -- (12.5,4) node[pos=0.5,above]{$1$};
    \draw[very thick] (12.5,3) rectangle (14.5,5); \draw (13.5,4) node{Bob};
    \draw (-1,4) node{$(1)$};
  \end{tikzpicture}
  \caption{Ejemplo: (1) Alice transmite un $1$ codificado mediante el esquema
    $+$ y Bob elije al azar el esquema $+$ obteniendo un $1$ (2) si Bob elige el
    esquema $\times$ obtiene $0$ ó $1$ con probabilidad $\nicefrac{1}{2}$.}
  \label{fig:ej}
\end{figure}

El algoritmo paso a paso:
\begin{enumerate}
\item Alice comienza a transmitir una secuencia de $0$ y $1$, elegidos
  aleatoriamente, alternando los esquemas $+$ y $\times$ también aleatoriamente.
\item Bob recibe la secuencia y va alternando las mediciones entre los esquemas
  $+$ y $\times$ aleatoriamente.
\item Alice le transmite a Bob la sucesión de esquemas empleada.
\item Bob le informa a Alice en qué casos utilizó el mismo esquema.
\item Usando solamente los bits de los esquemas idénticos a dos puntas, ambos
  han definido una sucesión aleatoria de bits que servirá como one-time pad de
  encriptación para transmisiones futuras por cualquier canal.
  \begin{center}
    \begin{tabular}{ccccccccc}
      Esquemas de Alice & $\times$ & $+$ & $+$ & $\times$ & $\times$ & $+$ & $\times$ & $+$ \\ \hline
      Valores de Alice & $\ket{-}$ & $\ket{0}$ & $\ket{0}$ & $\ket{+}$ & $\ket{-}$ & $\ket{0}$ & $\ket{-}$ & $\ket{1}$ \\ \hline
      Esquemas de Bob & $+$ & $\times$ & $+$ & $\times$ & $+$ & $+$ & $\times$ & $\times$ \\ \hline
      Valores de Bob & $\ket{0}$ & $\ket{+}$ & $\ket{0}$ & $\ket{+}$ & $\ket{1}$ & $\ket{0}$ & $\ket{-}$ & $\ket{-}$ \\ \hline
      Coincidencias & & & $\surd$ & $\surd$ & & $\surd$ & $\surd$ & \\ \hline
      Clave & & & 0 & 1 & & 0 & 0 & \\
    \end{tabular} 
  \end{center}

\item Alice y Bob intercambian hashes de las claves (en bloques) para aceptarla
  o descartarla.
\end{enumerate}

\paragraph{Inviolabilidad}
Este protocolo es, en teoría, inviolable. Supongamos que Eve espía el canal de
comunicación entre Alice y Bob e intenta recuperar la clave. Eve está en la
misma situación que Bob y no conoce cuál esquema es el correcto, $+$ o $\times$.
Por lo tanto elige al azar y se equivocará, en promedio, la mitad de las veces.

En el paso 5 Alice y Bob se ponen de acuerdo en cuáles valores tomar en cuenta
(las coincidencias de la secuencia de esquemas). Esta información no le es útil
a Eve porque sólo en la mitad de las veces habrá usado el detector correcto,
de manera que mal interpretará sus valores finales.

Además el QKD brinda el método para que Alice y Bob puedan detectar el potencial
espionaje de Eve:

Imaginemos que Alice envió un $0$ con el esquema $\times$ (es decir, el qubit
$\ket{-}$). Si Eve usa el esquema $+$, colapsará el qubit a $\ket{0}$ o
$\ket{1}$. Si Bob usa el esquema $\times$ y mide $\ket{-}$ coincide con lo
enviado por Alice, pero si mide $\ket{+}$ Alice y Bob descubrirán esa
discrepancia durante el intercambio de hashes, por lo tanto descartarán el
bloque.

\begin{ejercicio}
  En el protocolo BB84, Alice y Bob descartan todos los bits en los que
  eligieron bases distintas, y se quedan con una clave candidata. Luego
  eligen al azar algunos bits de esa clave, los comparan públicamente y los
  descartan, para detectar la posible presencia de un espía (Eve).

  ¿Cuántos bits necesitan comparar Alice y Bob para tener al menos un
  $90\%$ de probabilidad de detectar la presencia de Eve, suponiendo que
  Eve mide siempre en una base al azar?
\end{ejercicio}

\chapter{Introducción a la mecánica cuántica}\label{ch:mc}
\section{Postulados de la mecánica cuántica}
En el Capítulo~\ref{ch:intro} vimos los postulados de la mecánica cuántica sin
mencionarlo, desde un punto de vista matemático formal. Revisitaremos los mismos
postulados, nombrándolos como tales. Estos cuatro postulados definen el entorno
matemático conocido como mecánica cuántica.

\begin{postulado}
  [Espacio de estados]\label{post:espacio} Todo sistema físico cuántico aislado
  tiene asociado un espacio vectorial complejo con producto escalar conocido
  como el \emph{espacio de estados} del sistema. El sistema se describe
  completamente por un \emph{vector de estado}, el cual es un vector unidad en
  el espacio de estados.
\end{postulado}
\begin{postulado}
  [Evolución]\label{post:evolucion} La evolución de un sistema físico cuántico
  aislado se describe por una \emph{transformación unitaria}. Es decir, el
  estado $\ket\psi$ del sistema en el tiempo $t_1$ se relaciona con el estado
  $\ket{\psi'}$ del sistema en el tiempo $t_2$ a través del operador unitario
  $U$, el cual sólo depende de los tiempos $t_1$ y $t_2$.
  \[
    \ket{\psi'}=U\ket\psi
  \]
\end{postulado}
El postulado anterior se puede tomar con tiempo continuo, para lo cual hace
falta una ecuación diferencial, y el postulado se transforma en el siguiente:

\begin{postuladobis}{post:evolucion}
  La evolución del estado de un sistema físico cuántico aislado es descripta por
  la \emph{ecuación de Shcrödinger},
  \[
    i\hbar\frac{d\ket\psi}{dt} = H\ket\psi
  \]
  En esta ecuación, $\hbar$ es una constante física conocida como
  \emph{constante de Planck} cuyo valor debe ser determinado
  experimentalmente.  El valor exacto no es importante, en la
  práctica es común absorber el valor $\hbar$ en $H$ tomando
  $\hbar=1$. El operador $H$ no es la compuerta Hadamard vista
  anteriormente sino un operador hermítico fijo conocido como el
  \emph{Hamiltoniano} del sistema.
\end{postuladobis}

\begin{postulado}
  [Medición cuántica]\label{post:medicion} La medición cuántica se describe por
  una colección $\{M_m\}$ de \emph{matrices de medición}. Dichas matrices actúan
  en el espacio de estados del sistema que se mide. El índice $m$ refiere a los
  resultados posibles de la medición. Si el estado del sistema es $\ket\psi$,
  inmediatamente antes de la medición, entonces la probabilidad de que el
  resultado $m$ ocurra viene dado por
  \[
    p(m) = \bra\psi M_m^\dagger M_m\ket\psi
  \]
  y el estado del sistema luego de la medición es
  \[
    \frac{M_m\ket\psi}{\sqrt{\bra\psi M_m^\dagger M_m\ket\psi}}
  \]
  Las matrices satisfacen la ecuación de completitud,
  \[
    \sum_m M_m^\dagger M_m = I
  \]
  La ecuación de completitud expresa el hecho de que las probabilidades suman
  uno:
  \[
    1 = \sum_m p(m) = \sum_m\bra\psi M_m^\dagger M_m\ket\psi
    =\bra\psi\left(\sum_m M_m^\dagger M_m\right)\ket\psi
  \]
\end{postulado}

\begin{postulado}
  [Sistema compuesto]
  \label{post:comp}
  El espacio de estados de un sistema físico compuesto es el producto tensorial
  de los espacios de estados de los componentes. Más aún, si tenemos sistemas
  enumerados de $1$ a $n$, donde el sistema $i$ está en el estado
  $\ket{\psi_i}$, el estado conjunto del sistema total es
  \[
    \ket{\psi_1}\otimes\ket{\psi_2}\otimes\dots\otimes\ket{\psi_n}
  \]
\end{postulado}

\subsection{Medición proyectiva}
Un caso particular del postulado~\ref{post:medicion} es el conocido como
\emph{medición proyectiva}. De hecho, la medición general es equivalente a las
mediciones proyectivas más operaciones unitarias. Por lo tanto, en general,
usaremos sólo mediciones proyectivas.
\subsubsection{Preliminares}

\begin{definicion}
  [Autovectores y autovalores] Un \emph{autovector} de un operador lineal $A$ en
  un espacio vectorial dado es un vector no-nulo $\ket v$ tal que $A\ket v=v\ket
  v$, donde $v$ es un número complejo conocido como \emph{autovalor} de $A$
  correspondiente a $\ket v$.
\end{definicion}
\begin{observacion}
  En la definición anterior, notar que $v\neq\ket v$. De hecho, $v\in\mathbb C$
  y $\ket v\in\mathbb C^2$. La ``$v$'' que aparece en $\ket v$ es simplemente
  una etiqueta, un nombre, para el vector.
\end{observacion}
\begin{ejemplo}
  Consideremos la matriz de Pauli $iXZ$
  \begin{align*}
    iXZ &= i(\ket 0\bra 1+\ket 1\bra 0)(\ket 0\bra 0-\ket 1\bra 1)\\
        &= i(\ket 0\braket 10\bra 0-\ket 0\braket 11\bra 1+\ket 1\braket 00\bra 0-\ket 1\braket 01\bra 1)\\
        &= i(\ket 1\bra 0-\ket 0\bra 1)
  \end{align*}
  O, en su notación matricial, $iXZ= \matriz{0 & -i \\ i & 0}$

  Queremos buscar un vector $\ket v=\alpha\ket 0+\beta\ket 1$ tal que $iXZ\ket
  v=v\ket v$, para algún $v$, y con $\norma{\ket v}=1$. Es decir:
  \[
    i(\ket 1\bra 0-\ket 0\bra 1)(\alpha\ket 0+\beta\ket 1) =i(\alpha\ket
    1-\beta\ket 0) =-\beta i\ket 0+\alpha i\ket 1
  \]
  debe ser igual a $v(\alpha\ket 0+\beta\ket 1)$, con $|\alpha|^2+|\beta|^2=1$.

  Por lo tanto,
  \[
    \left\{\begin{array}{l}
             |\alpha|^2+|\beta|^2=1\\
             v\alpha = -\beta i\\
             v\beta = \alpha i
           \end{array}\right.
         \Rightarrow \left\{\begin{array}{l}
                              \left|\dfrac{\beta i}v\right|^2+|\beta|^2=1\\
                              \alpha = -\dfrac{\beta i}v\\
                              v\beta = \left(-\dfrac{\beta i}v\right)i
                            \end{array}\right.
                          \Rightarrow \left\{\begin{array}{l}
                                               |\beta|=\sqrt{\dfrac{|v|}{|v|+1}}\\
                                               \alpha = -\beta i\\
                                               v^2 = 1
                                             \end{array}\right.
                                         \]
                                         \[
                                           \Rightarrow \left\{\begin{array}{l}
                                                                |\beta|=\dfrac 1{\sqrt 2}\\
                                                                \alpha = -\beta i\\
                                                                v = 1
                                                              \end{array}\right.
                                                            \textrm{ o }
                                                            \left\{\begin{array}{l}
                                                                     |\beta|=\dfrac 1{\sqrt 2}\\
                                                                     \alpha = -\beta i\\
                                                                     v = -1
                                                                   \end{array}\right.
                                                               \]
                                                               Tomando, por
                                                               ejemplo, $v=1$ y
                                                               $\beta =\nicefrac
                                                               1{\sqrt 2}$
                                                               tenemos
                                                               $\alpha=-\nicefrac
                                                               i{\sqrt 2}$, y
                                                               por lo tanto $1$
                                                               es un autovalor
                                                               de $iXZ$ con
                                                               autovector
                                                               $\nicefrac
                                                               1{\sqrt 2}(\ket
                                                               1-i\ket 0)$.
                                                             \end{ejemplo}
                                                             \begin{definicion}
                                                               [Función
                                                               característica]
                                                               La \emph{función
                                                                 característica}
                                                               de un operador
                                                               lineal $A$ es
                                                               $c(x) =
                                                               \det|A-xI|$.
                                                             \end{definicion}
                                                             \begin{teorema}
                                                               Las soluciones a
                                                               la ecuación $c(x)
                                                               = 0$ son los
                                                               autovalores del
                                                               operador $A$.
                                                               \qed
                                                             \end{teorema}
                                                             \begin{ejemplo}
                                                               En el ejemplo
                                                               anterior, podemos
                                                               ver que
                                                               \begin{align*}
                                                                 c(x) &= \det|iXZ-x I|\\
                                                                      &=\det|i(\ket 1\bra 0-\ket 0\bra 1)-x(\ket 0\bra 0+\ket 1\bra 1)|\\
                                                                      &=\det|-x\ket 0\bra 0-i\ket 0\bra 1+i\ket 1\bra 0-x\ket 1\bra 1|\\
                                                                      &=(-x)^2-(-i^2)\\
                                                                      &=x^2-1
                                                               \end{align*}
                                                               Y tenemos $c(x) =
                                                               0\Rightarrow
                                                               x^2-1=0\Rightarrow
                                                               x=\pm 1$.
                                                             \end{ejemplo}
\begin{definicion} [Autoespacio] El
                                                               \emph{autoespacio}
                                                               correspondiente
                                                               un autovalor $v$
                                                               de un operador
                                                               linear $A$ es el
                                                               conjunto de
                                                               vectores que
                                                               tienen a $v$ como
                                                               autovalor.
                                                             \end{definicion}
                                                             \begin{ejemplo}
                                                               Siguiendo con el
                                                               ejemplo anterior,
                                                               el autoespacio
                                                               correspondiente
                                                               al autovalor $1$
                                                               del operador
                                                               $iXZ$ es
                                                               $\{\beta\ket
                                                               1-\beta i\ket
                                                               0\mid\beta\in\mathbb
                                                               C\}$.
                                                             \end{ejemplo}
                                                             \begin{teorema}
                                                               El autoespacio de
                                                               un autovalor $v$
                                                               de un operador
                                                               lineal $A$ en un
                                                               espacio vectorial
                                                               $V$ es un
                                                               subespacio
                                                               vectorial de $V$.
                                                               \qed
                                                             \end{teorema}

                                                             \subsubsection{Medición
                                                               proyectiva}

\begin{definicion}
  Una medición proyectiva es descripta por un \emph{observable}, $M$, el cual es
  un operador hermítico en el espacio de estados del sistema que es objeto de la
  observación. El observable tiene descomposición espectral (es decir,
  factorización a forma canónica) dada por:
  \[
    M = \sum_m mP_m
  \]
  donde $P_m$ es el proyector al autoespacio de $M$ con autovalor $m$. Los
  posibles resultados de la medición corresponden con los autovalores $m$ del
  observable. Luego de medir $\ket\psi$, la probabilidad de obtener el resultado
  $m$ viene dada por
  \[
    p(m) = \bra\psi P_m\ket\psi
  \]
  Al obtener el resultado $m$, el estado del sistema inmediatamente luego de la
  medición es
  \[
    \frac{P_m\ket\psi}{\sqrt{p(m)}}
  \]
\end{definicion}

\begin{observacion}
  La medición proyectiva se puede ver como un caso particular del
  Postulado~\ref{post:medicion}. Si a las matrices que forman el
  operador medición del Postulado~\ref{post:medicion} le agregamos
  la condición que los $M_m$ son hermíticos y ortogonales, es
  decir $M_mM_{m'} = \delta_{m,m'}M_m$, entonces el
  Postulado~\ref{post:medicion} reduce a las mediciones proyectivas
  que acabamos de definir.
\end{observacion}

\begin{ejemplo}
  Consideremos la medición del observable $Z$.
  \[
    c(x) = \det|Z-x I|=\det \left| (1-x)\ket 0\bra 0-(1+x)\ket 1\bra 1 \right| =
    (1-x)(-1-x) =-1+x^2
  \]
  Por lo tanto, las soluciones a $c(x)=0$ son $1$ y $-1$, y esos son los
  autovalores de $Z$. Dichos autovalores corresponden a los autovectores $\ket
  0$ y $\ket 1$ respectivamente.

  Los proyectores $P_0$ y $P_1$ sobre los autoespacios $\{\alpha\ket
  0\mid\alpha\in\mathbb C\}$ y $\{\beta\ket 1\mid\beta\in\mathbb C\}$
  respectivamente son $P_0=\ket 0\bra 0$ y $P_1=\ket 1\bra 1$. Notar que
  \[
    Z = \ket 0\bra 0-\ket 1\bra 1
  \]

  Entonces, la medición de $Z$ sobre el estado $\ket-=\nicefrac{(\ket 0+\ket
    1)}{\sqrt 2}$ da como resultado $1$ con probabilidad $\braket-0\braket
  0-=\nicefrac 12$. Similarmente, se obtiene resultado $-1$ con probabilidad
  $\nicefrac 12$.
\end{ejemplo}
\subsection{Fase}
Consideremos por ejemplo el estado
$e^{i\theta}\ket\psi$\footnote{$e^{i\theta}=\cos\theta+i\sin\theta$
  ($ae^{i\theta}$ es la llamada \emph{notación exponencial} de un número
  complejo, donde $a$ su módulo y $\theta$ su argumento).}, donde $\ket\psi$ es
un vector de estado, y $\theta$ es un número real. Decimos que el estado
$e^{i\theta}\ket\psi$ es igual a $\ket\psi$, excepto por la fase global
$e^{i\theta}$. La medición sobre ambos estados es la misma: Supongamos que $M_m$
es una matriz de un operador de medición. Entonces las probabilidades de aplicar
esa matriz vienen dadas por $\bra\psi M_m^\dagger M_m\ket\psi$ y por $\bra\psi
e^{-i\theta}M_m^\dagger M_me^{i\theta}\ket\psi=\bra\psi M_m^\dagger
M_m\ket\psi$\footnote{$e^{-i\theta}e^{i\theta}=e^{-i\theta+i\theta}=e^0=1$.}.
Por lo tanto, desde un punto de vista observacional, ambos estados son
idénticos.

Por esta razón solemos ignorar las fases globales ya que son irrelevantes a las
propiedades observacionales de sistemas físicos.

\section{Operador densidad}
Hasta ahora hemos visto la mecánica cuántica en términos de
vectores de estados.  Una formulación alternativa es usando el
operador densidad (o matriz densidad).  Esta presentación es
equivalente matemáticamente, pero provee un lenguaje más
conveniente para razonar en algunos escenarios comunes que se
encuentran en la mecánica cuántica.
\subsection{Preliminares}
\begin{definicion}[Traza]
  La \emph{traza} de una matriz es la suma de sus elementos
  diagonales.  Así, si
  $A=\sum_i\sum_j\alpha_{ij}\ket{u_i}\bra{u_j}$, la traza se
  define por
  \[
    \tr(A)=\sum_i\alpha_{ii}
  \]
\end{definicion}
\begin{teorema}\label{thm}
  Sea $A=\ket\psi\bra\varphi$. Entonces,
  $\tr(A)=\braket\varphi\psi$.
\end{teorema}
\begin{proof}
  Sean $\ket\psi=\sum_ia_i\ket{u_i}$ y $\ket\varphi=\sum_jb_j\ket{u_j}$.
  Entonces,
  \[
    \tr(A) =\tr(\ket\psi\bra\varphi)
    =\tr(\sum_ia_i\ket{u_i}\sum_jb_j^*\bra{u_j})\
    =\tr(\sum_{ij}a_ib_j^*\ket{u_i}\bra{u_j}) =\sum_ia_ib_i^*
    =\braket\varphi\psi \qedhere
  \]
\end{proof}
\begin{ejemplo}
  Sea $A=\ket 0\bra -$. Entonces, $A=\nicefrac 1{\sqrt 2}(\ket 0\bra 0-\ket 0\bra 1)$ y
  $\tr(A)=\nicefrac1{\sqrt 2}$.

  Por otro lado, siguiendo el teorema, $\tr(A)=\braket0-=\nicefrac
  1{\sqrt 2}(\braket 00-\braket 01)=\nicefrac 1{\sqrt 2}$.
\end{ejemplo}
\begin{ejemplo}
  Sea $A=\ket +\bra -=\nicefrac 12(\ket 0\bra 0-\ket 0\bra 1+\ket 1\bra
  0-\ket 1\bra 1)$.

  Entonces, $\tr(A)=\nicefrac 12-\nicefrac 12=0=\braket +-$.
\end{ejemplo}
El siguiente corollario es muy útil para evaluar la traza de un operador.
\begin{corolario}
  \label{cor:TraceOperator}
  Sea $\ket\psi$ un vector normalizado y $A$ un operador
  cuántico. Entonces
  \[
    \tr(A\ket\psi\bra\psi) = \bra\psi A\ket\psi
  \]
\end{corolario}
\begin{ejercicio}
  Probar el Corollario~\ref{cor:TraceOperator}.
\end{ejercicio}
\begin{ejemplo}
  $\tr(X\ket 0\bra 0)=\bra 0X\ket 0= \braket 00\braket 10 + \braket 01\braket 00
  =0 $.
\end{ejemplo}
\begin{propiedades}[de la traza de una matriz]
  Sean $A$ y $B$ matrices de la misma dimensión, $U$ un operador unitario y
  $\lambda\in\mathbb C$. Entonces
  \begin{multicols}{2}
    \begin{enumerate}
    \item\label{prop:ciclico} $\tr(AB)=\tr(BA)$
    \item $\tr(A+B)=\tr(A)+\tr(B)$
    \item $\tr(\lambda A)=\lambda\tr(A)$
    \item\label{prop:simTrans} $\tr(UAU^\dagger) = \tr(A)$
    \end{enumerate}
  \end{multicols}
\end{propiedades}
\begin{proof}
  Sólo mostramos la propiedad \ref{prop:simTrans}, las otras se
  dejan como ejercicio. De la propiedad \ref{prop:ciclico} se tiene
  $\tr(UAU^\dagger)=\tr(U^\dagger UA)$, y como $U$ es unitaria,
  $\tr(U^\dagger UA)=\tr(A)$.
\end{proof}

\subsection{Conjuntos de estados cuánticos}
El operador densidad provee una manera conveniente de describir un sistema
cuántico en el cual el estado no se conoce del todo.

\begin{definicion}
  [Operador o matriz densidad] Supongamos que un sistema cuántico está en uno de
  un número de estados $\ket{\psi_i}$, donde la probabilidad de que el estado
  sea $\ket{\psi_i}$ viene dada por $p_i$.

  Decimos que el conjunto $\{p_i,\ket{\psi_i}\}$ es el \emph{conjunto de estados
    puros}. El \emph{operador densidad} o \emph{matriz densidad} para este
  estado viene dado por la ecuación
  \[
    \rho = \sum_i p_i\ket{\psi_i}\bra{\psi_i}
  \]
\end{definicion}
\begin{ejemplo}
  \label{ex:OpDens}
  El operador densidad del conjunto de estados puros $\{(\nicefrac 14,\ket
  +);(\nicefrac 34,\ket 1)\}$ tiene operador densidad
  \[
    \rho=\nicefrac 14\ket +\bra ++\nicefrac 34\ket 1\bra 1 = \nicefrac 18\ket
    0\bra 0+\nicefrac 18\ket 0\bra 1+\nicefrac 18\ket 1\bra 0+\nicefrac 78\ket
    1\bra 1
  \]
  Es decir
  \[
    \rho = \matriz{\nicefrac 18 & \nicefrac 18\\\nicefrac 18 &\nicefrac 78}
  \]
\end{ejemplo}

\begin{observacion}
  Todos los postulados de la mecánica cuántica se pueden reformular en términos
  del operador densidad, y haremos eso más adelante en esta sección.
\end{observacion}

\paragraph{Evolución}
Supongamos que la evolución de un sistema cuántico cerrado se describe por el
operador unitario $U$. Si el sistema estaba inicialmente en el estado
$\ket{\psi_i}$ con probabilidad $p_i$, entonces, luego de la evolución el
sistema estará en estado $U\ket{\psi_i}$ con probabilidad $p_i$. Por lo tanto,
la evolución del operador densidad se describe por
\[
  \rho = \sum_i p_i\ket{\psi_i}\bra{\psi_i}\xrightarrow{U}\sum_ip_i
  U\ket{\psi_i}\bra{\psi_i}U^\dagger=U\rho U^\dagger
\]
\begin{ejemplo}
  Siguiendo con el Ejemplo \ref{ex:OpDens}, tomemos $U=H$, entonces el conjunto
  de estados puros original $\{(\nicefrac 14,\ket +);(\nicefrac 34,\ket 1)\}$
  evolucionará a $\{(\nicefrac 14,\ket 0);\nicefrac 34,\ket -)\}$ y su matriz
  densidad puede calcularse de dos maneras equivalentes:
  \begin{enumerate}
  \item A partir del conjunto de estados puros:
    \[
      \rho'=\nicefrac 14\ket 0\bra 0+\nicefrac 34\ket -\bra - = \nicefrac 58\ket
      0\bra 0 -\nicefrac 38\ket 0\bra 1 -\nicefrac 38\ket 1\bra 0 +\nicefrac
      38\ket 1\bra 1 = \matriz{\nicefrac 58 & -\nicefrac 38 \\ -\nicefrac 38 &
        \nicefrac 38}
    \]
  \item O utilizando la igualdad dada más arriba: $\rho'= H\rho H^\dagger =
    H\rho H$.
  \end{enumerate}
\end{ejemplo}

\paragraph{Medición}
Supongamos que realizamos una medición descripta por las matrices $M_m$. Si el
estado inicial era $\ket{\psi_i}$, entonces la probabilidad de obtener el
resultado $m$ es
\[
  p(m|i) = \bra{\psi_i}M_m^\dagger
  M_m\ket{\psi_i}\stackrel{\textrm{Cor.\ref{cor:TraceOperator}}}{=}\tr(M_m^\dagger
  M_m\ket{\psi_i}\bra{\psi_i})
\]
Usando la ley de probabilidades totales, la probabilidad de obtener el resultado
$m$ es
\begin{align*}
  p(m) &= \sum_i p(m|i)p_i \\
       &=\sum_i p_i\tr(M_m^\dagger M_m\ket{\psi_i}\bra{\psi_i})\\
       &=\tr(\sum_i p_iM_m^\dagger M_m\ket{\psi_i}\bra{\psi_i})\\
       &=\tr(M_m^\dagger M_m\sum_i p_i\ket{\psi_i}\bra{\psi_i})\\
       &=\tr(M_m^\dagger M_m\rho)
\end{align*}
Si el estado inicial era $\ket{\psi_i}$, el estado luego de obtener el resultado
$m$ será
\[
  \ket{\psi_i^m} = \frac{M_m\ket{\psi_i}}{\sqrt{\bra{\psi_i}M_m^\dagger
      M_m\ket{\psi_i}}}
\]
Por lo tanto, luego de una medición que de resultado $m$ tendremos el conjunto
de estados $\ket{\psi_i^m}$, con probabilidades $p(i|m)$ respectivamente. Por lo
tanto, el operador densidad $\rho_m$ correspondiente es
\begin{equation}
  \label{eq:rhom}
  \rho_m = \sum_i p(i|m)\ket{\psi_i^m}\bra{\psi_i^m} = \sum_i p(i|m)\frac{M_m\ket{\psi_i}\bra{\psi_i}M_m^\dagger}{\bra{\psi_i}M_m^\dagger M_m\ket{\psi_i}}
\end{equation}
Pero, usando teoría de probabilidad condicional,
\[
  p(i|m) = \frac{p(m\cap i)}{p(m)}=\frac{p(m|i)p_i}{p(m)} =
  p_i\frac{\tr(M_m^\dagger M_m\ket{\psi_i}\bra{\psi_i})}{\tr(M_m^\dagger
    M_m\rho)} =p_i\frac{\bra{\psi_i}M_m^\dagger M_m\ket{\psi_i}}{\tr(M_m^\dagger
    M_m\rho)}
\]
Substituyendo en \eqref{eq:rhom}, obtenemos
\[
  \rho_m = \sum_i
  p_i\frac{M_m\ket{\psi_i}\bra{\psi_i}M_m^\dagger}{\tr(M_m^\dagger M_m\rho)} =
  \frac{M_m\rho M_m^\dagger}{\tr(M_m^\dagger M_m\rho)}
\]

\begin{ejemplo}
  Volviendo al conjunto de estados del Ejemplo~\ref{ex:OpDens}, tenemos
  \[
    \rho=\nicefrac 18\ket 0\bra 0+\nicefrac 18\ket 0\bra 1+\nicefrac 18\ket
    1\bra 0+\nicefrac 78\ket 1\bra 1
  \]
  que corresponde a la matriz densidad del conjunto de estados $\{(\frac 14,\ket
  +);(\frac 34,\ket 1)\}$.

  Utilizaremos la medición proyectiva $\{P_0,P_1\}$ con $P_0=\ket 0\bra 0$ y
  $P_1=\ket 1\bra 1$.

  Entonces, la probabilidad de medir $0$ viene dada por
  \begin{align*}
    \tr(P_0^\dagger P_0\rho)
    &=\ket 0\bra 0(\nicefrac 18\ket 0\bra 0+\nicefrac 18\ket 0\bra 1+\nicefrac 18\ket 1\bra 0+\nicefrac 78\ket 1\bra 1)\\
    &=\tr(\nicefrac 18\ket 0\bra 0+\nicefrac 18\ket 0\bra 1)\\
    &=\nicefrac 18\,\tr(\ket 0\bra 0)+\nicefrac 18\,\tr(\ket 0\bra 0)\\
    &=\nicefrac 18
  \end{align*}
  Similarmente, la probabilidad de medir $1$ por
  \[
    \tr(\ket 1\bra 1\rho)
    =\nicefrac 18\,\tr(\ket 1\bra 0)+\nicefrac 78\,\tr(\ket 1\bra 1)\\
    =\nicefrac 78
  \]
  Podemos ver que el conjunto de estados está en el estado $\ket 1$ con
  probabilidad $\nicefrac 34$. Si ese es efectivamente el estado inicial, la
  probabilidad de medir $1$ sería $1$. Por otro lado, en el estado $\ket +$, la
  probabilidad de medir $1$ es $\nicefrac 12$. De ahí que la probabilidad de
  medir $1$ es
  \[
    \nicefrac 34\times 1+\nicefrac 14\times\nicefrac 12=\nicefrac 78
  \]
  tal y como dedujimos con la traza.

  Luego de realizar la medición, si se midió $1$, el estado del sistema podrá
  ser descripto por el operador siguiente:
  \[
    \rho_1=\frac{P_1\rho P_1^\dagger}{\nicefrac 78} =\frac{\nicefrac 78\ket
      1\bra 1}{\nicefrac 78} =\ket 1\bra 1
  \]
  Efectivamente, si se midió $1$ y el estado inicial era $\ket +$, el estado
  final será $\ket 1$, pero lo mismo pasa si el estado inicial era $\ket 1$, por
  lo que la matriz densidad es la matriz densidad del conjunto de estados
  $\{1,\ket 1\}$.
\end{ejemplo}

\begin{definicion}
  Un sistema cuántico donde el estado $\ket\psi$ se conoce
  exactamente se dice que está en un \emph{estado puro}. En este
  caso, el operador densidad es simplemente
  $\rho=\ket\psi\bra\psi$.

  Si no es un estado puro, $\rho$ está en un \emph{estado mixto} (o
  mezcla), o que es una mezcla de diferentes estados puros.
\end{definicion}

\begin{teorema}
  \label{thm:TrazaCuadrado}
  Para todo operador densidad $\rho$ se tiene $\tr(\rho^2) \leq 1$.

  Más aún, la igualdad se cumple si y sólo si $\rho$ está en un estado puro
\end{teorema}
\begin{ejercicio}
  Probar el Teorema~\ref{thm:TrazaCuadrado}
\end{ejercicio}

\begin{teorema}
  Un estado cuántico que está en estado $\rho_i$ con probabilidad
$p_i$, puede ser descripto por la matriz densidad $\sum_i
p_i\rho_i$.  \end{teorema} \begin{proof} Supongamos que $\rho_i$
  viene de un conjunto $\{p_{ij},\ket{\psi_{ij}}\}$ de estados
  puros (con $i$ fijo). Por lo tanto, la probabilidad de estar en
  el estado $\ket{\psi_{ij}}$ viene dada por $p_ip_{ij}$. Es decir
  que la matriz densidad es \( \rho = \sum_i\sum_j
  p_ip_{ij}\ket{\psi_{ij}}\bra{\psi_{ij}}=\sum_i p_i\rho_i \).
\end{proof}
\subsection{Propiedades generales del operador densidad}
\begin{definicion}
  [Operador positivo] Un operador $A$ se dice \emph{positivo} si para todo
  vector $\ket\psi$, $\bra\psi A\ket\psi\geq 0$. Si $\bra\psi A\ket\psi >0$ para
  todo $\ket\psi\neq 0$, decimos que $A$ es \emph{definido positivo}.
\end{definicion}
\begin{teorema}
  \label{thm:DescPosit}
  Si $A$ es un operador positivo, entonces existe una descomposición \(
  A=\sum_j\lambda_j\ket j\bra j \) donde los vectores $\ket j$ son ortonormales
  y $\lambda_j\in\mathbb R^+_0$ son autovalores de $A$. \qed
\end{teorema}
\begin{teorema}
  [Caracterización de operadores densidad]
  \label{thm:CaractDens}
  Un operador $\rho$ es el operador densidad de un conjunto
  $\{p_i,\ket{\psi_i}\}$ si y sólo si satisface las siguientes condiciones:
  \begin{enumerate}
  \item $\tr(\rho)=1$
  \item $\rho$ es un operador positivo
  \end{enumerate}
\end{teorema}
\begin{proof}
  \conlista
  \begin{description}
  \item[$\Rightarrow)$] Sea $\rho=\sum_i p_i\ket{\psi_i}\bra{\psi_i}$ un
    operador densidad. Entonces,
    \begin{enumerate}
    \item \( \tr(\rho) = \sum_i p_i\tr(\ket{\psi_i}\bra{\psi_i})=\sum_i p_i=1
      \).
    \item Sea $\ket\varphi$ un vector arbitrario en el espacio de estados.
      Entonces,
      \[
        \bra\varphi\rho\ket\varphi = \bra\varphi\left(\sum_i
          p_i\ket{\psi_i}\bra{\psi_i}\right)\ket\varphi = \sum_i
        p_i\braket\varphi{\psi_i}\braket{\psi_i}\varphi =\sum_i
        p_i|\braket\varphi{\psi_i}|^2 \geq 0
      \]
    \end{enumerate}
  \item[$\Leftarrow)$] Sea $\rho$ cualquier operador positivo con traza igual a
    $1$. Como $\rho$ es positivo, usando el Teorema~\ref{thm:DescPosit} tenemos
    \( \rho=\sum_j\lambda_j\ket j\bra j \), donde los vectores $\ket j$ son
    ortogonales y $\lambda_j\in\mathbb R^+_0$ son autovalores de $\rho$. Por la
    condición de traza $1$, tenemos $\sum_j\lambda_j=1$. Por lo tanto, un
    sistema en el estado $\ket j$ con probabilidad $\lambda_j$ tendrá un
    operador de densidad $\rho$. \qedhere
  \end{description}
\end{proof}

El Teorema~\ref{thm:CaractDens} nos permite reformular el
Postulado~\ref{post:espacio} para no depender de vectores, y podemos entonces
escribir todos los postulados en términos del operador densidad.

\begin{postuladoAlt}{post:espacio}
  Todo sistema físico cuántico aislado tiene asociado un espacio vectorial
  complejo con producto escalar conocido como el \emph{espacio de estados} del
  sistema. El sistema se describe completamente por su \emph{operador densidad},
  el cual es un operador positivo $\rho$ con traza $1$, que actúa en el espacio
  de estados del sistema. Si un sistema cuántico está en estado $\rho_i$ con
  probabilidad $p_i$, entonces el operador densidad del sistema es $\sum_i
  p_i\rho_i$.
\end{postuladoAlt}

\begin{postuladoAlt}{post:evolucion}
  La evolución de un sistema físico cuántico aislado se describe por una
  \emph{transformación unitaria}. Es decir, el estado $\rho$ del sistema en el
  tiempo $t_1$ se relaciona con el estado $\rho'$ del sistema en el tiempo $t_2$
  a través del operador unitario $U$, el cual sólo depende de los tiempos $t_1$
  y $t_2$.
  \[
    \rho' = U\rho U^\dagger
  \]
\end{postuladoAlt}

\begin{postuladoAlt}{post:medicion}
  La medición cuántica se describe por una colección $\{M_m\}$ de \emph{matrices
    de medición}. Dichas matrices actúan en el espacio de estados del sistema
  que se mide. El índice $m$ refiere a los resultados posibles de la medición.
  Si el estado del sistema es $\rho$, inmediatamente antes de la medición,
  entonces la probabilidad de que el resultado $m$ ocurra viene dado por
  \[
    p(m) = \tr(M_m^\dagger M_m\rho)
  \]
  y el estado del sistema luego de la medición es
  \[
    \frac{M_m\rho M_m^\dagger}{\tr(M_m^\dagger M_m\rho)}
  \]
  Las matrices satisfacen la ecuación de completitud,
  \[
    \sum_m M_m^\dagger M_m = I
  \]
\end{postuladoAlt}

\begin{postuladoAlt}{post:comp}
  El espacio de estados de un sistema físico compuesto es el producto tensorial
  de los espacios de estados de los componentes. Más aún, si tenemos sistemas
  enumerados de $1$ a $n$, donde el sistema $i$ está en el estado $\rho_i$, el
  estado conjunto del sistema total es \(
  \rho_1\otimes\rho_2\otimes\dots\otimes\rho_n \).
\end{postuladoAlt}

\subsection{El operador densidad reducido}
Uno de los usos más interesantes del operador densidad es para describir
subsistemas de un sistema cuántico compuesto. Tal descripción viene dada por el
\emph{operador densidad reducido}.

\begin{definicion}
  Sean $A$ y $B$ dos sistemas físicos tales que su estado es descripto por el
  operador densidad $\rho^{AB}$. El operador densidad reducido para $A$ se
  define por
  \[
    \rho^A = \tr_B(\rho^{AB})
  \]
  donde $\tr_B$ es la \emph{traza parcial sobre el sistema $B$}, y es un
  operador lineal definido por
  \[
    \tr_B(\ket{a_1}\bra{a_2}\otimes\ket{b_1}\bra{b_2}) =
    \ket{a_1}\bra{a_2}\tr(\ket{b_1}\bra{b_2})=\braket{b_2}{b_1}\ket{a_1}\bra{a_2}
  \]
  para todo $\ket{a_1},\ket{a_2}$ en el espacio de estados de $A$ y $\ket{b_1}$,
  $\ket{b_2}$ en el espacio de estados de $B$.
\end{definicion}

\begin{ejemplo}
  Supongamos que tenemos un sistema cuántico en el estado
  $\rho^{AB}=\rho\otimes\sigma$, donde $\rho$ es el operador densidad del
  sistema $A$ y $\sigma$ el del sistema $B$. Entonces,
  \[
    \rho^A = \tr_B(\rho\otimes\sigma)=\rho\tr(\sigma)=\rho
  \]
  Similarmente, $\rho^B=\sigma$.
\end{ejemplo}
\begin{ejemplo}
  Un ejemplo menos trivial es el estado de Bell $\beta_{00}=\nicefrac 1{\sqrt
    2}(\ket{00}+\ket{11})$, que tiene el siguiente operador densidad
  \[
    \rho=\left(\frac{\ket{00}+\ket{11}}{\sqrt 2}\right)
    \left(\frac{\bra{00}+\bra{11}}{\sqrt 2}\right)
    =\frac{\ket{00}\bra{00}+\ket{11}\bra{00}+\ket{00}\bra{11}+\ket{11}\bra{11}}2
  \]
  Haciendo la traza sobre el segundo qubit obtenemos el operador densidad del
  primer qubit:
  \begin{align*}
    \rho^1 
    &= \tr_2(\rho)\\
    &=\frac{\tr_2(\ket{00}\bra{00})+\tr_2(\ket{11}\bra{00})+\tr_2(\ket{00}\bra{11})+\tr_2(\ket{11}\bra{11})}2\\
    &=\frac{\tr_2(\ket 0\bra 0\otimes\ket 0\bra 0)+\tr_2(\ket 1\bra 0\otimes\ket 1\bra 0)+\tr_2(\ket 0\bra 1\otimes\ket 0\bra 1)+\tr_2(\ket 1\bra 1\otimes\ket 1\bra 1)}2\\
    &=\frac{\braket 00\ket 0\bra 0+\braket 01\ket 1\bra 0+\braket 10\ket 0\bra 1+\braket 11\ket 1\bra 1}2\\
    &=\frac{\ket 0\bra 0+\ket 1\bra 1}2\\
    &=\frac I2
  \end{align*}
  Notar que este es un estado mixto, ya que $\tr((\nicefrac I2)^2) = \nicefrac
  12<1$. Es decir que al estar enredados, por más que el estado de dos qubits
  sea un estado puro, el primer qubit sólo está en un estado mixto: es decir, un
  estado que no conocemos completamente.
\end{ejemplo}
\subsubsection{Teleportación cuántica y el operador densidad reducido}
Podemos usar el operador densidad reducido para analizar el algoritmo de
teleportación.

Cuando presentamos el algoritmo de teleportación (Sección~\ref{sec:telep})
dijimos que no contradice la teoría de la relatividad (que entre otras cosas
determina que nada puede viajar a mayor velocidad que la luz, ni siquiera la
información), ya que no hay trasmisión de información hasta que Alice no le
envía (usando un canal clásico) el resultado de la medición a Bob. Podemos hacer
esta afirmación de manera más rigurosa utilizando el operador densidad reducido.

Antes de que Alice haga la medición, el estado del sistema es
\[
  \frac 12 \left( \ket{00}(\alpha\ket 0+\beta\ket 1)+ \ket{01}(\alpha\ket
    1+\beta\ket 0)+ \ket{10}(\alpha\ket 0-\beta\ket 1)+ \ket{11}(\alpha\ket
    1-\beta\ket 0) \right)
\]
por lo que al medir los dos primeros qubits, se obtendrá
\begin{align*}
  \ket{00}(\alpha\ket 0+\beta\ket 1)&\quad\textrm{con probabilidad }\nicefrac 14\\
  \ket{01}(\alpha\ket 1+\beta\ket 0)&\quad\textrm{con probabilidad }\nicefrac 14\\
  \ket{10}(\alpha\ket 0-\beta\ket 1)&\quad\textrm{con probabilidad }\nicefrac 14\\
  \ket{11}(\alpha\ket 1-\beta\ket 0)&\quad\textrm{con probabilidad }\nicefrac 14
\end{align*}
Por lo tanto, el operador densidad del sistema es
\begin{align*}
  \rho = \frac 14(
  &\ket{00}\bra{00}(\alpha\ket 0+\beta\ket 1)(\alpha^*\bra 0+\beta^*\bra 1)\\
  +&\ket{01}\bra{01}(\alpha\ket 1+\beta\ket 0)(\alpha^*\bra 1+\beta^*\bra 0)\\
  +&\ket{10}\bra{10}(\alpha\ket 0-\beta\ket 1)(\alpha^*\bra 0-\beta^*\bra 1)\\
  +&\ket{11}\bra{11}(\alpha\ket 1-\beta\ket 0)(\alpha^*\bra 1-\beta^*\bra 0))
\end{align*}
Por lo tanto, si hacemos la traza parcial sobre el sistema de Alice, obtenemos
que operador densidad del sistema de Bob es
\begin{align*}
  \rho^B &=
           \frac 14(
           (\alpha\ket 0+\beta\ket 1)(\alpha^*\bra 0+\beta^*\bra 1)
           +(\alpha\ket 1+\beta\ket 0)(\alpha^*\bra 1+\beta^*\bra 0)\\
         &\quad\;
           +(\alpha\ket 0-\beta\ket 1)(\alpha^*\bra 0-\beta^*\bra 1)
           +(\alpha\ket 1-\beta\ket 0)(\alpha^*\bra 1-\beta^*\bra 0))\\
         &=\frac{2(|\alpha|^2+|\beta|^2)\ket 0\bra 0+2(|\alpha|^2+|\beta|^2)\ket 1\bra 1}4\\
         &=\frac{\ket 0\bra 0+\ket 1\bra 0}2\\
         &=\frac I2
\end{align*}
Por lo tanto, el estado de Bob \emph{después} de que Alice hizo la medición,
pero \emph{antes} de que Bob obtuvo el resultado de esa medición es $\nicefrac
I2$. Este estado no depende del estado $\ket\psi$ que se transmitió, y por lo
tanto, cualquier medición que haga Bob no contendrá información sobre
$\ket\psi$, lo que previene que Alice use la teleportación para enviar
información a mayor velocidad que la luz.


\section{Descomposición de Schmidt}
\begin{teorema}
  [Descomposición de Schmidt]
  \label{thm:DescompSchimdt}
  Sea $\ket\psi$ un estado puro de un sistema compuesto $AB$. Entonces existen
  estados ortonormales $\ket{i_A}$ en el sistema $A$ y estados ortogonales
  $\ket{i_B}$ en el sistema $B$ tal que
  \[
    \ket\psi = \sum_i\lambda_i\ket{i_A}\ket{i_B}
  \]
  donde para todo $i$, $\lambda_i\in\mathbb R^+_0$ tales que
  $\sum_i\lambda_i^2=1$. A los $\lambda_i$ se los conoce como coeficientes de
  Schmidt. \qed
\end{teorema}
\begin{corolario}
  \label{cor:TrazaParcialSchm}
  Sea $\ket\psi$ un estado puro de un sistema compuesto $AB$. Entonces
  \[
    \rho^A=\sum_i\lambda_i^2\ket{i_A}\bra{i_A} \qquad\textrm y\qquad
    \rho^B=\sum_i\lambda_i^2\ket{i_B}\bra{i_B}
  \]
\end{corolario}
\begin{proof}
  Por el Teorema~\ref{thm:DescompSchimdt},
  $\ket\psi=\sum_i\lambda_i\ket{i_A}\ket{i_B}$. Por lo tanto,
  \begin{align*}
    \rho^{AB} 
    &=\left(\sum_i\lambda_i\ket{i_A}\ket{i_B}\right)
    \left(\sum_j\lambda_j\bra{j_A}\bra{j_B}\right)\\
    &=\sum_{ij}\lambda_i\lambda_j\ket{i_A}\ket{i_B}\bra{j_A}\bra{j_B}\\
    &=\sum_{ij}\lambda_i\lambda_j(\ket{i_A}\bra{j_A}\otimes\ket{i_B}\bra{j_B})
  \end{align*}
  Entonces
  \[
    \rho^A =\tr_B(\rho^{AB}) =\sum_{ij}\lambda_i\lambda_j
    \tr_B(\ket{i_A}\bra{j_A}\otimes\ket{i_B}\bra{j_B})
    =\sum_{ij}\lambda_i\lambda_j\braket{i_B}{j_B} \ket{i_A}\bra{j_A}
    =\sum_i\lambda_i^2\ket{i_A}\bra{i_A}
  \]
  Análogamente, \( \rho^B=\sum_i\lambda_i^2\ket{i_B}\bra{i_B} \).
\end{proof}
\begin{ejemplo}
  Sea $\ket\psi=\dfrac{\ket{00}+\ket{01}+\ket{11}}{\sqrt 3}$.

  Primero debemos hallar la descomposición de Schmidt de este estado, para luego
  obtener los operadores densidad reducidos.

  Llamamos $A$ a la matriz de coeficientes de $\ket\psi$:
  \[
    A=\frac 1{\sqrt 3}(\ket 0\bra 0 +\ket 0\bra 1 +\ket 1\bra 1)
  \]
  Buscamos los autovalores de $A^\dagger A$
  \begin{align*}
    A^\dagger A
    &=
    \frac 1{\sqrt 3}(\ket 0\bra 0
      +\ket 1\bra 0
    +\ket 1\bra 1)
    \frac 1{\sqrt 3}(\ket 0\bra 0
      +\ket 0\bra 1
    +\ket 1\bra 1)
    \\
    &=\frac 13(\ket 0\bra 0+\ket 0\bra 1+\ket 1\bra 0+2\ket 1\bra 1)
  \end{align*}
  Vemos que $\det(A^\dagger A-Ix)=x^2-x+\frac 19$, por lo tanto, los autovalores
  de $A^\dagger A$ son
  \[
    \lambda_1 = \frac{3+\sqrt 5}6 \qquad\textrm y\qquad \lambda_2 =
    \frac{3-\sqrt 5}6
  \]
  Sean $v_1=\alpha_1\ket 0+\beta_1\ket 1$ un autovalor de norma $1$ asosiado a
  $\lambda_1$. Entonces
  \begin{equation}
    \label{eq:AdagAv1}
    A^\dagger A v_1
    =
    \frac{\alpha_1+\beta_1}3\ket 0
    +\frac{\alpha_1+2\beta_1}3\ket 1
  \end{equation}
  y por otro lado
  \begin{equation}
    \label{eq:lam1v1}
    \lambda_1 v_1
    =
    \frac{3+\sqrt 5}6\alpha_1\ket 0+\frac{3+\sqrt 5}6\beta_1\ket 1
  \end{equation}
  Tomando \eqref{eq:AdagAv1} = \eqref{eq:lam1v1}, y $\norma{v_1}=1$ tenemos
  \[
    \left\{\begin{array}{ll}
	\dfrac{\alpha_1+\beta_1}3 &=\dfrac{3+\sqrt 5}6\alpha_1\\
	\dfrac{\alpha_1+2\beta_1}3 &=\dfrac{3+\sqrt 5}6\beta_1\\
	|\alpha|^2+|\beta|^2 &=1
    \end{array}\right.
    \implies \left\{\begin{array}{ll}
	|\alpha_1|^2 &= \dfrac 2{5+\sqrt 5}\\
	|\beta_1|^2 &= \dfrac{5+\sqrt 5}{10}
    \end{array}\right.
  \]
  Tomamos, por ejemplo,
  \[
    \alpha_1=\sqrt{\dfrac 2{5+\sqrt 5}} \qquad
    \beta_1=\sqrt{\dfrac{5+\sqrt 5}{10}}
  \]
  Análogamente, sea $v_2=\alpha_2\ket 0+\beta_2\ket 1$ un
  autovector de norma $1$ asociado a $\lambda_2$, entonces
  tomamos
  \[
    \alpha_2=\sqrt{\dfrac 2{5-\sqrt 5}} \qquad
    \beta_2=-\sqrt{\dfrac{5-\sqrt 5}{10}}
  \]

  Sean $u_1=\frac{A v_1}{\sqrt{\lambda_1}}$ y $u_2=\frac{A
  v_2}{\sqrt{\lambda_2}}$. Por la descomposición de valores
  singulares de $A$, tenemos que $\{u_1,u_2\}$ y
  $\{v_1,v_2\}$ son bases ortonormales de $\mathbb C^2$ y
  $A=UDV^\dagger$, donde $U=u_1\ket 0+u_2\ket 1$,
  $D=\sqrt{\lambda_1}\ket{00}+\sqrt{\lambda_2}\ket{11}$ y
  $V=v_1\ket 0+v_2\ket 1$.

  Luego, para la descomposición de Schmidt tomamos
  $\ket{1_A}=u_1$, $\ket{2_A}=u_2$, $\ket{1_B}=v_1$ y
  $\ket{2_B}=v_2$, y como coeficientes de Schmidt
  $\theta_1=\sqrt{\lambda_1}$ y $\theta_2=\sqrt{\lambda_2}$.
  Es decir,
  \[
    \ket{\psi}=\theta_1\ket{1_A}\ket{1_B}+\theta_2\ket{2_A}\ket{2_B}
  \]

  Por lo tanto, por el Corolario~\ref{cor:TrazaParcialSchm},
  \[
    \rho^A =
    \theta_1\ket{1_A}\bra{1_A}+\theta_2\ket{2_A}\bra{2_A}
  \]
  Por lo tanto, $\tr((\rho^A)^2) =
  \tr((\theta_1\ket{1_A}\bra{1_A}+\theta_2\ket{2_A}\bra{2_A})^2)
  =\theta_1^4+\theta_2^4=\dfrac 79$.
\end{ejemplo}

\begin{ejercicio}
  Considerar el estado de Bell
  \[
    \ket{\Phi^+}
    = \tfrac{1}{\sqrt 2}\ket{00}+\tfrac{1}{\sqrt 2}\ket{11}.
  \]
  \begin{enumerate}
    \item Dar la descomposición de Schmidt de $\ket{\Phi^+}$.
    \item Calcular la matriz densidad reducida $\rho^A$ del primer qubit.
    \item Calcular $\tr((\rho^A)^2)$ e interpretar el resultado en términos
      de entrelazamiento.
  \end{enumerate}
\end{ejercicio}


%
%
%\part{Hacia un Curry-Howard en Computación Cuántica}\label{part:LCC}
%\chapter{Extensiones cuánticas al lambda cálculo}
%\section{Control clásico, datos cuánticos}
%El paradigma de control clásico y datos cuánticos se atribuye a Peter
%\cite{SelingerMSCS04}, y la idea es que en cualquier lenguaje de programación
%cuántico, los datos (qubits), son cuánticos, pero el flujo de control del
%programa será clásico. Es decir, no se pueden superponer programas, sólo datos.
%La primer extensión a lambda cálculo en este paradigma vino de la mano de
%\cite{SelingerValironMSCS06}, y es el cálculo que vamos a estudiar en la
%Sección~\ref{sec:SelVal}.
%\section{El cálculo de Selinger y Valiron}\label{sec:SelVal}
%\subsubsection*{Gramática}
%La gramática del cálculo es la siguiente:
%\begin{align*}
%  t ::= & x \mid \fun xt\mid tt\mid \ifz ttt\mid 0\mid 1\mid\newq\mid\meas\mid U\mid\ast\mid(t,t)\mid\letl{(x,y)}tt
%\end{align*}
%Donde
%\begin{itemize}
%\item $\newq$ mapea un bit clásico en un qubit.
%\item $\meas$ mapea un qubit en un bit clásico, a través de una medición
%  cuántica.
%\item $U$ es un cualquier matriz unitaria.
%\end{itemize}
%
%Las que siguen son notaciones prácticas:
%\begin{align*}
%  (t_1,t_2,\dots,t_n) &=(t_1,(t_2,(\dots,t_n)))\\
%  \letl xrt &=(\fun xt)r\\
%  \fun{(x,y)}t &=\fun z{(\letl{(x,y)}zt)}
%\end{align*}
%
%\subsubsection*{Programas}
%El estado de un programa se representa con una tripleta $[q,\ell,t]$ donde
%\begin{itemize}
%\item $q$ es un vector normalizado de $\bigotimes_{i=1}^n\mathbb C^2$, para
%  algún $n>0$.
%\item $t$ es un término lambda.
%\item $\ell$ es una función de $W$ en $\mathbb N^{\leq n}$, donde
%  $\FV(t)\subseteq W$. A $L$ se la llama función de linkeado.
%\end{itemize}
%La función de linkeado linkea variables libres específicas de $t$ a qubits
%específicos de $q$.
%\begin{ejemplo}
%  \label{ej:tripleta}
%  La tripleta
%  \[
%    [\frac 1{\sqrt 2}(\ket{00}+\ket{11}),\{x\mapsto 2\},\fun y{Xx}]
%  \] representa el programa que comienza con el estado de Bell $\beta_{00}$ y al
%  pasarle un argumento cualquiera aplica la compuerta $X$ (not) al segundo
%  qubit, transformando dicho estado en $\beta_{01}$, pero aún no hemos dicho
%  cómo reduce un programa para poder verificar esta última afirmación.
%\end{ejemplo}
%
%\paragraph{Notación.} Para simplificar la notación, se usan $p_i$ para denotar
%las variables libres $x$ tal que $\ell(x)=i$. Es decir, $p_i$ es la variable que
%referencia al $i$-ésimo qubit. De esa manera, un programa $[q,\ell,t]$ es
%abreviado en $\prog q{t''''}$, donde
%$t'=t[p_{\ell(x_1)}/x_1]\dots[p_{\ell(x_n)}/x_n]$.
%
%\begin{ejemplo}
%  La tripleta del ejemplo \ref{ej:tripleta} se escribe abreviadamente como
%  \[
%    \prog{\frac 1{\sqrt 2}(\ket{00}+\ket{11})}{\fun y{Xp_2}}
%  \]
%\end{ejemplo}
%
%\begin{observacion}
%  El teorema de no clonado (ver~Teorema~\ref{thm:no-cloning}) en este lenguaje
%  es traducido en que cada qubit cuántico no puede ser referenciado más de una
%  vez a través de la función de linkeado. Sintácticamente esta restricción se
%  traduce en la condición de linealidad: una lambda abstracción $\fun xt$ se
%  dice lineal si la variable $x$ aparece exactamente una vez en $t$. El sistema
%  de tipos se encargará de que las variables linkeadas a través de la función
%  $\ell$ se usen linealmente, mientras al resto de las variables se permite una
%  utilización no-lineal.
%\end{observacion}
%
%\subsubsection*{Semántica operacional: preliminares}
%\paragraph{Estrategia de reducción.}
%Aún no hemos definido las reglas de reducción, sin embargo, veremos un ejemplo
%que ayuda a decidir la estrategia a utilizar.
%
%Sea \( \mathsf{xor} = \fun x{\fun y{\ifz x{(\ifz y01)}y}} \). Luego, definimos
%el siguiente término:
%\[
%  t=(\fun x{\mathsf{xor}\ xx})(\meas(H(\newq\ 0)))
%\]
%\subparagraph{Call-by-value.} En call-by-value la reducción es la siguiente
%\begin{align*}
%  \prog{\ket{}}t &\to_{CBV} \prog{\ket 0}{(\fun x{\mathsf{xor}xx})(\meas(H p_1))}\\
%                 &\to_{CBV} \prog{\frac 1{\sqrt 2}(\ket 0+\ket 1)}{(\fun x{\mathsf{xor}\ xx})(\meas\ p_1)}\\
%                 &\to_{CBV} 
%                   \left\{\begin{array}{l}
%                            \prog{\ket 0}{(\fun x{\mathsf{xor}\ xx})0}\\
%                            \prog{\ket 1}{(\fun x{\mathsf{xor}\ xx})1}
%                          \end{array}\right.
%  \\
%                 &\to_{CBV}
%                   \left\{\begin{array}{l}
%                            \prog{\ket 0}{\mathsf{xor}\ 0\ 0}\\
%                            \prog{\ket 1}{\mathsf{xor}\ 1\ 1}
%                          \end{array}\right.
%  \\
%                 &\to_{CBV}
%                   \left\{\begin{array}{l}
%                            \prog{\ket 0}{0}\\
%                            \prog{\ket 1}{0}
%                          \end{array}\right.
%\end{align*}
%Donde las dos ramas tienen probabilidad $\nicefrac 12$ cada una, por lo tanto,
%en call-by-value este programa produce el valor booleano $0$ con probabilidad
%$1$.
%
%\subparagraph{Call-by-name.} En call-by-name, en cambio, la reducción es la
%siguiente:
%\[\prog{\ket{}}t \to_{CBN} \prog{\ket{}}{\mathsf{xor}\ (\meas(H(\newq\ 0)))\ (\meas(H(\newq\ 0)))} \to_{CBN}^* \left\{\begin{array}{l} \prog{\ket{01}}1\\ \prog{\ket{10}}1\\ \prog{\ket{00}}0\\ \prog{\ket{11}}0 \end{array}\right. \]
%(No se detalla paso a paso ya que hay algunas sutilezas con la construcción $\ifz{}{}{}$ que aún no hemos mencionado).
%
%Entonces, en call-by-name este programa produce $0$ o $1$ con la misma
%probabilidad.
%
%\subparagraph{Sin estrategia.} Si no se establece una estrategia, este término
%podría incluso reducir a un término mal formado, por ejemplo:
%\begin{align*}
%  \prog{\ket{}}t &\to_{CBV} \prog{\ket 0}{(\fun x{\mathsf{xor}xx})(\meas(H p_1))}\\
%                 &\to_{CBV} \prog{\frac 1{\sqrt 2}(\ket 0+\ket 1)}{(\fun x{\mathsf{xor}\ xx})(\meas\ p_1)}\\
%                 &\to_{CBN}\prog{\frac 1{\sqrt 2}(\ket 0+\ket 1)}{\mathsf{xor}\ (\meas\ p_1)\ (\meas\ p_1)}
%\end{align*}
%Notar que el último término no es válido ya que contiene dos ocurrencias de
%$p_1$.
%
%\subparagraph{Conclusión:} Se utiliza
%la estrategia call-by-value, ya que es la más natural.
%
%\paragraph{Reescritura probabilista.}
%Como se vio en el ejemplo anterior, es necesario utilizar un sistema de
%reescritura probabilista. Esto es, un sistema de reescritura donde algunos
%términos pueden reducir en más de una forma, cada una de ellas con una
%probabilidad asociada. No nos detendremos más a formalizar ésto, basta decir que
%todas las reglas de reducción tendrán probabilidad $1$, a excepción de $\meas$
%que tendrá diferentes formas de reducirlo con una probabilidad dada por el
%vector de estado cuántico del programa.
%
%Por ejemplo,
%\[
%  \prog{\alpha\ket 0+\beta\ket 1}{\meas\ p_1}\to_{|\alpha|^2}\prog{\ket 0}0
%  \quad\textrm{ y }\quad \prog{\alpha\ket 0+\beta\ket 1}{\meas\
%    p_1}\to_{|\beta|^2}\prog{\ket 1}1
%\]
%donde $t\to_p r$ se lee ``$t$ reduce a $r$ con probabilidad $p$''.
%
%\subsubsection*{Semántica operacional: formalización}
%Dado que se ha elegido una estrategia call-by-value, definimos los valores de la
%siguiente manera:
%\[
%  v :: = x \mid \fun xt\mid 0\mid 1\mid\meas\mid\newq\mid U\mid\ast\mid (v,v)
%\]
%El conjunto de estados de valores es $\mathbb V = \{[q,\ell,v]\}$.
%
%Las reglas de reducción se dan a continuación, donde se ha utilizado la
%convención para simplificar la notación sin la función de linkeado.
%\begin{align*}
%  \prog q{(\fun xt)v} &\to_1\prog q{t[v/x]}\\
%  \prog q{\ifz 1tr} &\to_1\prog qt\\
%  \prog q{\ifz 0tr} &\to_1 \prog qr\\
%  \prog q{U(p_{j_1},\dots,p_{j_n})} &\to_1 \prog{q'}{(p_{j_1},\dots,p_{j_n})} & (*)\\
%  \prog{\alpha\ket{q_0}+\beta\ket{q_1}}{\meas\ p_i} &\to_{|\alpha|^2}\prog{\ket{q_0}}0 & (**)\\
%  \prog{\alpha\ket{q_0}+\beta\ket{q_1}}{\meas\ p_i} &\to_{|\beta|^2}\prog{\ket{q_1}}1 & (**)\\
%  \prog q{\newq\ 0} &\to_1\prog{q\otimes\ket 0}{p_{n+1}} & (***)\\
%  \prog q{\newq\ 1} &\to_1\prog{q\otimes\ket 1}{p_{n+1}} & (***)\\
%  \prog q{\letl{(x_1,x_2)}{(v_1,v_2)}t} &\to_1\prog q{t[v_1/x_1][v_2/x_2]}\\
%  \omit\rlap{Para las siguientes reglas, sea $\prog qt\to_p\prog{q'}{t'}$. Entonces:}\\
%  \prog q{rt}&\to_p\prog {q'}{rt'}\\
%  \prog q{tv}&\to_p\prog {q'}{t'v}\\
%  \prog q{(t,r)}&\to_p\prog {q'}{(t',r)}\\
%  \prog q{(v,t)}&\to_p\prog {q'}{(v,t')}\\
%  \prog q{\ifz trs}&\to_p\prog {q'}{\ifz{t'}rs}\\
%  \prog q{\letl{(x,y)}tr}&\to\prog {q'}{\letl{(x,y)}{t'}r}\\
%  \prog q{\letl{(x,y)}rt}&\to\prog {q'}{\letl{(x,y)}r{t'}}
%\end{align*}
%
%
%\begin{itemize}
%\item[$(*)$] $U$ es una compuerta cuántica de $n$-qubits y $q'$ es $q$ luego de
%  aplicar la compuerta a los qubits $j_1,\dots,j_n$.
%\item[$(**)$] $\ket{q_0}$ y $\ket{q_1}$ son qubits normalizados de la forma
%  \[
%    \ket{q_0} = \sum_j\alpha_j\ket{\phi_j^0}\otimes\ket 0\otimes\ket{\psi_j^0}
%    \qquad y \qquad \ket{q_1} = \sum_j\alpha_j\ket{\phi_j^1}\otimes\ket
%    1\otimes\ket{\psi_j^1}
%  \]
%  donde $\ket{\phi_j^0}, \ket{\phi_j^1}\in\mathbb C^{2^{i-1}}$.
%\item[$(***)$] $q\in\mathbb C^{2^n}$
%\end{itemize}
%
%\subsubsection*{Tipos}
%El sistema de tipos captura la noción de duplicabilidad, como se discutió
%anteriormente. Se utiliza la notación de la lógica lineal de \cite{GirardTCS87}.
%Un término de tipo $A$ se asume no duplicable, y a los términos duplicables se
%les asignará tipos de la forma $!A$. La gramática de los tipos se define como
%sigue:
%\[
%  A ::= \alpha\mid X\mid\ !A\mid A\multimap A\mid \top\mid A\otimes A
%\]
%donde $\alpha$ es alguno de un conjunto de constantes y $X$ es alguno de un
%conjunto de variables de tipo.
%
%Escribimos $!^nA$ en lugar de $!!\dots!!A$ con $n$ repeticiones de $!$, y $A^n$
%para el producto tensorial de $n$ $A$s: $A\otimes\dots\otimes A$. \medskip
%
%En general, un valor de tipo $!A$ podremos utilizarlo más de una vez. Pero no
%hay ningún problema si decimos que ese valor tiene también tipo $A$, y en ese
%caso debe usarse sólo una vez. Por lo tanto, se definen reglas de subtipado, que
%nos permitirán hacer que si un término tiene un tipo, también tenga cualquier
%subtipo de éste.
%\[
%  \infer[(\alpha)]{\alpha<:\alpha}{} \qquad \infer[(X)]{X<:X}{} \qquad
%  \infer[(\top)]{\top<:\top}{} \qquad \infer[(D)]{!A<:B}{A<:B} \qquad
%  \infer[(!)]{!A<:!B}{!A<:B}
%\]
%\[
%  \infer[(\otimes)]{A_1\otimes A_2<:B_1\otimes B_2} { A_1<:B_1 & A_2<:B_2 }
%  \qquad\qquad \infer[(\multimap)]{A'\multimap B<:A\multimap B'} { A<:A' & B<:B'
%  }
%\]
%
%Que un programa $[q,\ell,t]$ esté bien tipado, significa simplemente que $t$
%esté bien tipado, por lo tanto, vamos a dar las reglas de tipos sólo para $t$.
%
%Para cada constante $c$ del lenguaje se asocia un tipo fijo $A_c$:
%\[
%  \begin{array}{rcl@{\quad}rcl@{\quad}rcl}
%    A_0 &=& !\bit & A_{\newq} &=& !(\bit\multimap\qbit) & & &\\
%    A_1 &=& !\bit & A_{\meas} &=& !(\qbit\multimap\bit) & A_U &=& !(\qbit^n\multimap\qbit^n)
%  \end{array}
%\]
%
%
%\[
%  \infer[(\te{var})]{\Gamma,x:A\vdash x:B}{A<:B} \qquad
%  \infer[(\te{const})]{\Gamma\vdash c:B}{A_c<:B}
%\]
%\[
%  \infer[(\te{if})]{\Gamma_1,\Gamma_2,!\Delta\vdash\ifz trs:A} {
%    \Gamma_1,!\Delta\vdash t:\bit & \Gamma_2,!\Delta\vdash r:A &
%    \Gamma_2,!\Delta\vdash s:A }
%\]
%\[
%  \infer[(\te{app})]{\Gamma_1,\Gamma_2,!\Delta\vdash tr:B} {
%    \Gamma_1,!\Delta\vdash t:A\multimap B & \Gamma_2,!\Delta\vdash r:A }
%\]
%\[
%  \vcenter{\infer[(\lambda_1)]{\Delta\vdash\fun xt:A\multimap
%      B}{x:A,\Delta\vdash t:B}} \qquad \textrm{Si
%  }\FV(t)\cap|\Gamma|=\emptyset,\
%  \vcenter{\infer[(\lambda_2)]{\Gamma,!\Delta\vdash\fun xt:!^{n+1}(A\multimap
%      B)}{\Gamma,!\Delta,x:A\vdash t:B}}
%\]
%\[
%  \infer[(\otimes_i)]{\Gamma_1,\Gamma_2,!\Delta\vdash (t,r):!^n(A_1\otimes A_2)}
%  { \Gamma_1,!\Delta\vdash t:!^n A_1 & \Gamma_2,!\Delta\vdash r:!^n A_2 } \qquad
%  \infer[(\top)]{\Delta\vdash \ast:!^n\top}{}
%\]
%\[
%  \infer[(\otimes_e)]{\Gamma_1,\Gamma_2,!\Delta\vdash\letl{(x,y)}tr:A} {
%    \Gamma_1,!\Delta\vdash t:!^n(A_1\otimes A_2) & \Gamma_2,!\Delta,x:!^n
%    A_1,y:!^n A_2\vdash r:A }
%\]
%
%\subsubsection*{Ejemplo: Teleportación}
%La teleportación fue presentada en la Sección~\ref{sec:telep}. Reproducimos el
%circuito aquí por conveniencia.
%\begin{eqnarray*}
%  \Qcircuit @C=1em @R=1em {
%  \lstick{\ket{\psi}} 
%  & \qw & \qw & \ctrl{1} \qw & \gate{H} \qw & \meter & \controlo \cw \cwx[1] \\
%  \lstick{\ket 0}&\gate{H}\qw &\ctrl{1} \qw & \targ \qw    & \qw          & \meter & \controlo \cw \cwx \\
%  \lstick{\ket 0} &\qw & \targ\qw & \qw          & \qw          & \qw    & \gate{Z^{b_1}X^{b_2}} \cwx & \qw & \rstick{\ket{\psi}}
%                                                                                                              \gategroup{2}{2}{3}{3}{.7em}{--}
%                                                                                                              \gategroup{1}{4}{2}{6}{.7em}{--}
%                                                                                                              \gategroup{3}{7}{3}{7}{.7em}{--}
%                                                                                                              }
%\end{eqnarray*}
%Las líneas punteadas delimitan tres partes del circuito: la primera es la
%creación del estado de Bell $\beta_{00}$, y le llamaremos $\mathsf{Bell}$. La
%segunda es las operaciones que realiza Alice, y llamaremos a esta parte del
%algoritmo $\mathsf{Alice}$. Finalmente, la tercera es la que realiza Bob, por lo
%que le llamaremos $\mathsf{Bob}$. Los tipos de cada parte del programa serán:
%\begin{align*}
%  &\vdash\mathsf{Bell} :!(\top\mapsto(\qbit\otimes\qbit)\\
%  &\vdash\mathsf{Alice} :!(\qbit\multimap\qbit\multimap\bit\otimes\bit)\\
%  &\vdash\mathsf{Bob} :!(\qbit\multimap\bit\otimes\bit\multimap\qbit)
%\end{align*}
%Esas funciones se definen por
%\begin{align*}
%  \mathsf{Bell} &=\fun x{\mathit{CNOT}(\mathit H(\newq\ 0),\newq\ 0)}\\
%  \mathsf{Alice} &=\fun{q_1}{\fun{q_2}{(\letl{(p,p')}{\mathit{CNOT}(q_1,q_2)}{(\meas(\mathit H\ p),\meas\ p')})}}\\
%  \mathsf{Bob} &=\fun q{\fun{(x,y)}{\ifz x{(\ifz y{\mathit{ZX}q}{\mathit Zq})}{(\ifz y{\mathit Xq}q)}}}
%\end{align*}
%
%Luego,
%\[
%  \mathsf{Telep} = \fun q { ( \letl{(p,p')}{\mathsf{Bell}\ \ast} { (
%      \mathsf{Bob}\ p'\ (\mathsf{Alice}\ q\ p) ) } ) }
%\]
%
%\begin{ejercicios}
%\begin{enumerate}
%\item Dar la traza de $\prog{\frac 1{\sqrt 2}(\ket 0+\ket 1)}{\mathsf{Telep}\
%    p_1}$.
%\item Dar términos para Deutsch y la codificación superdensa, y tiparlos.
%\end{enumerate}
%\end{ejercicios}
%
%\section{Control y datos cuánticos}
%Antes del cálculo de Selinger y Valiron, y de la idea de control clásico y datos
%cuánticos, hubo otras extensiones al lambda cálculo. La más notoria, quizá, es
%el cálculo de \cite{vanTonderSIAM04} del cual veremos algunos detalles en la
%Sección~\ref{sec:vanTonder}. Sin embargo, luego de la introducción del paradigma
%de control clásico y datos cuánticos también hubo desarrollos en el paradigma de
%control y datos cuánticos. En particular, dicho paradigma puede ser más
%apropiado para estudiar la computación cuántica desde un punto de vista lógico a
%través del isomorfismo de Curry-Howard (ver~\citep{SorensenUrzyczyn06}). El
%cálculo lineal-algebraico (\emph{Lineal}) de \cite{ArrighiDowekRTA08,ArrighiDowekLMCS17} dio origen al estudio de los lenguajes cuánticos a nivel de su semántica operacional y lo veremos en la Sección~\ref{sec:Lineal}, así como su versión tipada, del trabajo de \cite*{ArrighiDiazcaroValironIC17}. Luego veremos una modificación a dicho cálculo, agregando medición y un sistema de tipos simples en la
%Sección~\ref{sec:Qmeas}, donde se describe el paper de \cite{DiazcaroDowekTPNC17} y
%se da una intuición de su semántica denotacional.
%\section{El cálculo de van Tonder}\label{sec:vanTonder}
%Al igual que con el cálculo de Selinger y Valiron, van Tonder también utiliza un
%sistema de tipos lineal para evitar el clonado. Sin embargo, el cálculo de van
%Tonder estudia otra propiedad interesante: la reversibilidad. Efectivamente, la
%computación cuántica, excluyendo la medición, es reversible, ya que las
%compuertas cuánticas son unitarias.
%
%Inicialmente van Tonder plantea una gramática simple, con constantes para las
%operaciones cuánticas
%\begin{align*}
%  t&::= x\mid \fun xt\mid tt\mid c\\
%  c&::= 0\mid 1\mid H\mid CNOT\mid X\mid Z\mid\dots
%\end{align*}
%donde, como puede observarse, no hay medición.
%
%La computación reversible fue estudiada en los 70s, en particular, el trabajo de
%\cite{BennettJRD73} mostró una manera simple de obtener reversibilidad: llevar
%un historial de los pasos de reducción. Por ejemplo, si $t_0\to t_1\to t_2\to
%\dots$, la reducción reversible sería
%\[
%  (t_0)\to (t_0,t_1)\to (t_0,t_1,t_2)\to\dots
%\]
%Sin embargo, en el caso cuántico, no es tan directo. Supongamos que el estado
%inicial viene dado por $H$ aplicada al qubit $\ket 0$, que van Tonder lo
%representa dentro de un ket de la siguiente manera:
%\[
%  \ket{(H\ 0)}
%\]
%Las reglas de reducción para este término deberían ser tales que $\ket{(H\ 0)}$
%reduzca a $\frac 1{\sqrt 2}(\ket 0+\ket 1)$ y $\ket{(H\ 1)}$ a $\frac 1{\sqrt
%  2}(\ket 0-\ket 1)$. Sin embargo, estas reglas no son reversibles. Usando el
%truco de Bennett podríamos tener
%\[
%  \ket{(H\ 0)}\to\frac 1{\sqrt 2}(\ket{(H\ 0);0}+\ket{(H\ 0);1}) =\ket{(H\
%    0)}\otimes\frac 1{\sqrt 2}(\ket 0+\ket 1)
%\]
%En este ejemplo, el ``historial'' se factoriza a la izquierda y el término
%reducido queda a la derecha. Sin embargo, consideremos el siguiente ejemplo:
%\begin{align*}
%  \ket{(H\ (H\ 0))}
%  &\to\frac 1{\sqrt 2}(\ket{(H\ (H\ 0));(H\ 0)}+\ket{(H\ (H\ 0));(H\ 1)})\\
%  &\to\frac 12\ket{(H\ (H\ 0))}\otimes
%    (\ket{(H\ 0);0}+\ket{(H\ 0);1}+\ket{(H\ 1);0}-\ket{(H\ 1);1})
%\end{align*}
%Aquí el término reducido no puede ser factorizado: el registro quedo enredado
%con parte del historial.
%
%Pero en este ejemplo vemos que se está guardando más información de la
%necesaria. Con guardar simplemente que subtérmino redujo y con qué operación es
%suficiente. Retomando el mismo ejemplo, tendríamos:
%\begin{align*}
%  \ket{(H\ (H\ 0))} 
%  &\to\frac 1{\sqrt 2}(\ket{(\_\ (H\ \_));(H\ 0)}+\ket{(\_\ (H\ \_));(H\ 1)})\\
%  &\to\frac 12\ket{(\_\ (H\ \_))}\otimes
%    (\ket{(H\ \_);0}+\ket{(H\ \_);1}+\ket{(H\ \_);0}-\ket{(H\ \_);1})\\
%  &=\ket{(\_\ (H\ \_));(H\ \_)}\otimes\ket 0
%\end{align*}
%
%Con estas ideas en mente se define el modelo computacional. El estado
%computacional se toma como una superposición cuántica de secuencias
%\[
%  h_1;\dots;h_n;t
%\]
%donde $h_1;\dots;h_n$ es llamado historial, y $t$ registro computacional.
%
%No vamos a discutir el cálculo de van Tonder en este curso, sólo estas
%observaciones y se recomienda al interesado el paper \citep{vanTonderSIAM04}.
%\section{El lambda cálculo lineal algebraico}\label{sec:Lineal}
%El lambda cálculo lineal algebraico (de ahora en más, Lineal) sigue un paradigma
%diferente a los dos vistos anteriormente: la idea es tener un lambda cálculo
%puro, donde no hay distinción entre datos y programas, y, por lo tanto, dado que
%los datos pueden superponerse, también pueden hacerlo los programas.
%
%Este cálculo es un primer paso hacia un cálculo cuántico. En este primer paso el
%cálculo se centra en la noción computacional de espacios vectoriales.
%
%\subsubsection*{Gramática}
%La gramática del lenguaje incluye todos los términos de lambda cálculo, y sus
%combinaciones lineales.
%\begin{align*}
%  t :: = x\mid\fun xt \mid tr\mid 0\mid\alpha.t\mid t+t
%\end{align*}
%donde $\alpha\in\mathbb C$. Aquí, la constante $0$ tiene un significado
%diferente a la de los cálculos anteriores: recordemos que Lineal no es un
%cálculo cuántico, sino un cálculo \emph{vectorial}, y por lo tanto $0$
%representa simplemente al vector nulo.
%
%Los valores de este cálculo son las variables y las abstracciones, y los
%escribimos utilizando la letra $v$.
%\subsubsection*{Semántica operacional}
%Las reglas de reducción son, además de la beta reducción, una versión orientada
%de los axiomas de espacios vectoriales. La orientación de cada regla ha sido
%elegida de manera de obtener un cálculo confluente.
%
%La idea principal es que una abstracción, sobre una combinación lineal, se
%comporte linealmente como lo hace una matriz sobre un vector. Así,
%\[
%  (\fun xt)(\alpha.r+\beta.s)\to^*\alpha.(\fun xt)r+\beta.(\fun xt)s
%\]
%De esa manera no se impone no clonado con una restricción de lógica lineal (ver
%el cálculo de Selinger y Valiron en la Sección~\ref{sec:SelVal}), sino que
%siempre que haya una superposición, la función actuará linealmente, y por lo
%tanto sólo actuará en los vectores de base. Recordemos que los vectores de base
%son clonables: por ejemplo, la compuerta CNOT clona los qubits $\ket 0$ y $\ket
%1$:
%\begin{align*}
%  \mathit{CNOT}\ket{00} &= \ket{00}\\
%  \mathit{CNOT}\ket{10} &= \ket{11}
%\end{align*}
%En cambio, la misma función aplicada a una superposición, actúa de la siguiente
%manera:
%\begin{align*}
%  \mathit{CNOT}(\alpha\ket 0+\beta\ket 1)\otimes\ket 0
%  &=\mathit{CNOT}(\alpha\ket{00}+\beta\ket{10})\\
%  &=\alpha\mathit{CNOT}\ket{00}+\beta\mathit{CNOT}\ket{10}\\
%  &=\alpha\ket{00}+\beta\ket{11}
%\end{align*}
%Notar que ese qubit difiere de $(\alpha\ket 0+\beta\ket 1)\otimes(\alpha\ket
%0+\beta\ket 1)$.
%
%Por lo tanto el cálculo deberá ser call-by-value, ya que si el argumento reduce
%a una superposición, es necesario reducirlo antes de pasarlo a una abstracción.
%
%\[
%  \begin{array}{rl@{\qquad}rl}
%    \multicolumn{2}{l}{\textrm{$\beta$-reducción}}&
%                                                    \multicolumn{2}{l}{\textrm{Reglas de factorización}}\\
%    (\fun xt)v &\to t[v/x]&
%                            \alpha.t+\beta.t &\to(\alpha+\beta).t\\
%    \multicolumn{2}{l}{\textrm{Reglas elementales}}&
%                                                     \alpha.t+t &\to(\alpha+1).t\\
%    0.t &\to 0&
%                t+t &\to 2.t\\
%    1.t &\to t&
%                t+0 &\to 0\\
%    \alpha.0 &\to 0&
%                     \multicolumn{2}{l}{\textrm{Reglas de aplicación}}\\
%    \alpha.(\beta.0) &\to (\alpha\times\beta).0&
%                                                 (t+r)s &\to ts+rs\\
%    \alpha.(t+r)&\to\alpha.t+\alpha.r&
%                                       t(r+s)  &\to tr+ts\\
%                                                  &&
%                                                     (\alpha.t)r &\to\alpha.tr\\
%                                                  &&
%                                                     t(\alpha.r) &\to\alpha.tr\\
%                                                  &&
%                                                     0t &\to 0\\
%                                                  &&
%                                                     t0 &\to 0\\
%    \multicolumn{4}{c}{\textrm{Si }t\to r\textrm{ entonces }C[t] \to C[r]\textrm{ para cualquier contexto }C[\cdot]}
%  \end{array}
%\]
%\begin{ejemplo}
%  Consideremos los términos true y false de Church, $\fun x{\fun yx}$ y $\fun
%  x{\fun yy}$ como los qubits $\ket 0$ y $\ket 1$. De esa manera podemos
%  codificar el término $\ifz trs$ simplemente como $trs$, ya que si $t$ es $\ket
%  0$, $\fun x{\fun yx}rs\to^*r$ y si $t$ es $\ket 1$, $\fun x{\fun yy}rs\to^*s$.
%
%  La compuerta Hadamard podría ser codificada en Lineal de la siguiente manera:
%  \[
%    Hx = x{(\frac 1{\sqrt 2}(\ket 0+\ket 1))}{(\frac 1{\sqrt 2}(\ket 0-\ket 1))}
%  \]
%  Sin embargo, de esta manera trivial no funciona, ya que los mecanismos
%  utilizados para no-clonado (las reglas de aplicación), harán que el término
%  reduzca de la siguiente manera:
%  \begin{align*}
%    H\ket 0 &=H\fun x{\fun yx}\\
%            &=\fun x{\fun yx}(\frac 1{\sqrt 2}(\ket 0+\ket 1))(\frac 1{\sqrt 2}(\ket 0-\ket 1))\\
%            &\to \frac 1{\sqrt 2}\fun x{\fun yx}(\ket 0+\ket 1)(\frac 1{\sqrt 2}(\ket 0-\ket 1))\\
%            &\to \frac 1{\sqrt 2}(\fun x{\fun yx}\ket 0+\fun x{\fun yx}\ket 1)(\frac 1{\sqrt 2}(\ket 0-\ket 1))\\
%            &\to^*\frac 12(\fun y{\ket 0}+\fun y{\ket 1})(\ket 0-\ket 1)\\
%            &\to^*\frac 12(\ket 0-\ket 0+\ket 1-\ket 1)\\
%            &\to^* 0
%  \end{align*}
%\end{ejemplo}
%El problema está en la linealidad. Lo que debemos hacer es que el encodaje del
%$\ifz{}{}{}$ detenga la linealidad. Por ejemplo:
%\[
%  \ifz trs = (t(\fun xr)(\fun xs))(\fun xx)
%\]
%Para hacerlo más legible, usamos la siguiente notación:
%\begin{align*}
%  [t] & = \fun xt \qquad \textrm{ con }x\notin\FV(t)\\
%  {t} & = t(\fun xx)
%\end{align*}
%Y entonces, el encodaje del $\ifz{}{}{}$ es el siguiente:
%\[
%  \ifz trs = \{t[r][s]\}
%\]
%
%Por ejemplo, a la compuerta Hadamard la codificamos correctamente como
%\[
%  \{Hx\} = {\{x[(\frac 1{\sqrt 2}(\ket 0+\ket 1))][(\frac 1{\sqrt 2}(\ket 0-\ket
%    1))]\}}
%\]
%
%\subsubsection*{Tipos}
%El sistema de tipos vectorial para Lineal~\citep*{ArrighiDiazcaroValironIC17}
%propone que los tipos lleven cuenta de las superposiciones en los términos. Así,
%si el término $t$ tiene tipo $A$ y el término $r$ tiene tipo $B$, el término
%$\alpha.t+\beta.r$ tendrá tipo $\alpha.A+\beta.B$.
%
%Si consideramos $T=X\Rightarrow Y \Rightarrow X$ y $F=X\Rightarrow Y\Rightarrow
%Y$, el término $\frac 1{\sqrt 2}(\ket 0+\ket 1)$ tendrá tipo $\frac 1{\sqrt
%  2}(T+F)$. Notar que este tipo tiene norma $1$, al igual que el término, y por
%lo tanto, un tal tipo nos permite verificar facilmente la norma del vector que
%produce un programa. \medskip
%
%Dada la estrategia call-by-value, las variables sólo pueden tener tipos no
%superpuestos, los que llamamos \emph{unit types}. Esta necesidad se comprende
%mejor con el siguiente ejemplo:
%
%Supongamos que permitimos variables con tipos escalados, como $\alpha.V$.
%Entonces, el término $\fun x{x+t}$ podría tener tipo
%$(\alpha.V)\Rightarrow\alpha.V+U$, con $t$ de tipo $U$. Luego, tomemos un
%término $v$ de tipo $V$ y tenemos $(\fun x{x+t})(\alpha.v)$ de tipo
%$\alpha.V+U$. Sin embargo
%\[
%  (\fun x{x+t})(\alpha.v)\to\alpha.(\fun
%  x{x+t})v\to\alpha.(v+t)\to\alpha.v+\alpha.t
%\]
%lo cual es problemático, ya que $\alpha.V+U$ no refleja esta superposición.
%\medskip
%
%Por el contrario, las variables de tipo no necesitan ser solo unit. Sin embargo,
%debemos distinguir variables unit de las que no lo son, ya que sólo las
%variables unit pueden aparecer a la izquierda de una flecha. Por ese motivo
%definimos dos tipos de variables: las variables $\un X$, que sólo podrán ser
%reemplazadas por tipos unit, y las variables $\gen X$, que pueden ser
%reemplazadas por cualquier tipo (escribimos simplemente $X$ cuando nos refiramos
%a cualquiera de las dos). El tipo $\un X$ es unit, mientras el tipo $\gen X$ no
%lo es. \medskip
%
%En particular, $T=\forall\un X.\forall\un Y.X\Rightarrow Y\Rightarrow X$ y
%$F=\forall\un X.\forall\un Y.X\Rightarrow Y\Rightarrow Y$. El tipo de $H$ es
%\[
%  \forall\gen X.\left([\frac 1{\sqrt 2}(T+F)]\Rightarrow[\frac 1{\sqrt
%      2}(T+F)]\Rightarrow\gen X\right)\Rightarrow\gen X
%\]
%Donde $[A]=(\forall\un X.\un X\Rightarrow\un X)\Rightarrow A$.
%
%\paragraph{Gramática de los tipos.}
%Formalizando lo anterior, la gramática de los tipos es la siguiente:
%\begin{align*}
%  A &::= U\mid\alpha.A\mid A+A\mid\gen X\\
%  U &::= \un X\mid U\Rightarrow A\mid \forall\un X.U\mid\forall\gen X.U
%\end{align*}
%
%Consideramos también las siguientes equivalencias entre tipos, la cual es una
%congruencia:
%\[
%  \begin{array}{rcl@{\qquad}rcl}
%    1.A &\equiv& A & \alpha.A+\beta.A &\equiv& (\alpha+\beta).A\\
%    \alpha.(\beta.A) &\equiv& (\alpha\times\beta).A & A+B &\equiv& B+A\\
%    \alpha.A+\alpha.B &\equiv& \alpha.(A+B) & A+(B+C) &\equiv&(A+B)+C
%  \end{array}
%\]
%\begin{observacion}
%  Por una cuestión técnica, la equivalencia $A+0.B\equiv A$ no es válida, y por
%  lo tanto la propiedad de subject reduction es más débil, en el sentido de que
%  si $\Gamma\vdash t:A$ y $t\to_p r$, sólo se puede asegurar que $\Gamma\vdash
%  r:B$ donde si la regla por la cual se redujo $t$ a $r$ no es una regla de
%  factorización, entonces $B=A$, en otro caso, $A$ puede ser $B+0.C$. De todas
%  maneras, obviaremos este detalle técnico en este apunte, y se refiere al
%  lector a \cite*[\S4.2]{ArrighiDiazcaroValironIC17} para más detalles.
%\end{observacion}
%
%\paragraph{Reglas de tipado.}
%Las reglas de tipado se detallan a continuación. Usamos $[T/X]$ para referir a
%$[U/\un X]$ o $[A/\gen X]$.
%\[
%  \infer[ax]{\Gamma, x:{U}\vdash x:{U}}{} \qquad \infer[0_I] {\Gamma\vdash 0:
%    0.A} {\Gamma\vdash {t}: A} \qquad \infer[\Rightarrow_I] {\Gamma \vdash
%    \lambda x. {t}:{U}\Rightarrow A} {\Gamma, x:{U} \vdash {t}: A}
%\]
%\[
%  \infer[\Rightarrow_E] {\Gamma \vdash tr:\sum_{i=1}^n\sum_{j=1}^m
%    \alpha_i\times\beta_j. {A_i[T_j/X]}} {\Gamma \vdash
%    t:\sum_{i=1}^n\alpha_i.\forall X.(U\Rightarrow A_i) &\Gamma\vdash
%    r:\sum_{j=1}^m\beta_j. U[T/X]}
%\]
%\[
%  \infer[\forall_{I}] {\Gamma\vdash {t}:\sum_{i=1}^{n}\alpha_i.\forall X.U_i}
%  {\Gamma\vdash {t}: \sum_{i=1}^{n}\alpha_i. U_i & {X\notin\FV(\Gamma)}} \qquad
%  \infer[\forall_{E}] {\Gamma\vdash {t}: \sum_{i=1}^{n}\alpha_i. U_i[T/X]}
%  {\Gamma\vdash {t}: \sum_{i=1}^{n}\alpha_i.\forall X.U_i}
%\]
%\[
%  \infer[\alpha_I] {\Gamma\vdash\alpha. {t}:\alpha. A} {\Gamma\vdash {t}: A}
%  \qquad \infer[+_I] {\Gamma\vdash {t}+ {r}: A+B} {\Gamma\vdash {t}: A &
%    \Gamma\vdash {r}: B} \qquad \infer[\equiv] {\Gamma\vdash {t}: B}
%  {\Gamma\vdash {t}: A & A\equiv B}
%\]
%
%\begin{ejemplo}
%  [Tipando Hadamard] Sean $\ket 0=\fun x{\fun yx}$ y $\ket 1=\fun x{\fun yy}$.
%  Es fácil verificar que
%  \begin{align*}
%    &\vdash\ket 0:\forall\un{X}\un{Y}.\un{X}\Rightarrow\un{Y}\Rightarrow\un{X},\\
%    &\vdash\ket 1:\forall\un{X}\un{Y}.\un{X}\Rightarrow\un{Y}\Rightarrow\un{Y}.
%  \end{align*}
%  \begin{observacion}
%    Usamos la notación $\forall XY.A$ para $\forall X.\forall Y.A$.
%  \end{observacion}
%  También definimos las siguientes superposiciones:
%  \[
%    \ket{+}=\frac{1}{\sqrt{2}}.(\ket 0+\ket 1) \qquad\textrm{y}\qquad
%    \ket{-}=\frac{1}{\sqrt{2}}.(\ket 0-\ket 1).
%  \]
%  De la misma manera,definimos
%  \begin{align*}
%    \boxplus&=\frac{1}{\sqrt{2}}.((\forall\un{XY}.\un{X}\Rightarrow\un{Y}\Rightarrow\un{X})+(\forall\un{XY}.\un{X}\Rightarrow\un{Y}\Rightarrow\un{Y})),
%    \\
%    \boxminus&=\frac{1}{\sqrt{2}}.((\forall\un{XY}.\un{X}\Rightarrow\un{Y}\Rightarrow\un{X})-(\forall\un{XY}.\un{X}\Rightarrow\un{Y}\Rightarrow\un{Y})).
%  \end{align*}
%  Es fácil verificar que $\vdash [\ket{+}]:[\boxplus]$ y $\vdash
%  [\ket{-}]:[\boxminus]$.
%
%  Para simplificar la notación, tomamos
%  $A=[\boxplus]\Rightarrow[\boxminus]\Rightarrow [\gen{X}]$. Entonces
%  \[
%    \infer[\forall_I]{\vdash\lambda x.\{x[\ket +][\ket -]\}:\forall\gen
%      X.([\boxplus]\Rightarrow[\boxminus]\Rightarrow[\gen X])\Rightarrow\gen X}
%    { \infer[\Rightarrow_I]{\vdash\lambda x.\{x[\ket +][\ket
%        -]\}:A\Rightarrow\gen X} { \infer[\Rightarrow_E]{x:A\vdash\{x[\ket
%          +][\ket -]\}:\gen X} { \infer[\Rightarrow_E]{x:A\vdash x[\ket +][\ket
%            -]:[\gen X]} { \infer[\Rightarrow_E]{x:A\vdash x[\ket
%              +]:[\boxminus]\Rightarrow[\gen X]} { \infer[ax]{x:A\vdash x:A}{} &
%              x:A\vdash[\ket +]:[\boxplus] } & x:A\vdash[\ket -]:[\boxminus] } }
%      } }
%  \]
%  Ahora podemos aplicar Hadamard a un qubit para obtener el tipo correcto. Sea
%  $H=\lambda x.\{x[\ket{+}][\ket{-}]\}$.
%  \[
%    \infer[\Rightarrow_E]{\vdash H\ket 0:\boxplus} { \infer[\forall_E]{\vdash
%        H:([\boxplus]\Rightarrow[\boxminus]\Rightarrow[\boxplus])\Rightarrow\boxplus}
%      { \vdash H:\forall\gen X.([\boxplus]\Rightarrow[\boxminus]\Rightarrow[\gen
%        X])\Rightarrow\gen X } & \infer[\forall_E]{\vdash\ket
%        0:[\boxplus]\Rightarrow[\boxminus]\Rightarrow[\boxplus]} {
%        \infer[\forall_E]{\vdash\ket 0:\forall\un Y.[\boxplus]\Rightarrow\un
%          Y\Rightarrow[\boxplus]} {\vdash\ket 0:\forall\un X\un Y.\un
%          X\Rightarrow\un Y\Rightarrow\un X} } }
%  \]
%
%  Un ejemplo aún más interesante es el siguiente. Sea
%  \[
%    \boxplus_I = \frac 1{\sqrt
%      2}.(([\boxplus]\Rightarrow[\boxminus]\Rightarrow[\boxplus])+([\boxplus]\Rightarrow[\boxminus]\Rightarrow[\boxminus]))
%  \]
%  Es decir, $\boxplus_I$ es $\boxplus$ donde los forall han sido instanciados.
%  Es fácil verificar que $\vdash\ket +:\boxplus_I$. Entonces,
%  \[
%    \infer[\Rightarrow_E] {\vdash H\ket +:\frac 1{\sqrt 2}.\boxplus+\frac
%      1{\sqrt 2}.\boxminus} {\vdash H:\forall
%      \gen{X}.([\boxplus]\Rightarrow[\boxminus]\Rightarrow [\gen{X}])\Rightarrow
%      \gen{X} & \vdash\ket +:\boxplus_I}
%  \]
%  Y dado que $\frac 1{\sqrt 2}.\boxplus+\frac 1{\sqrt
%    2}.\boxminus\equiv\forall\un X\un Y.\un X\to\un Y\to\un X$, podemos concluir
%  \[
%    \vdash H\ket +:\forall\un X\un Y.\un X\Rightarrow\un Y\Rightarrow\un X.
%  \]
%  Notar que $H\ket +\to^*\ket 0$.
%\end{ejemplo}
%\bigskip
%
%\begin{ejercicio}
%Escribir el algoritmo de teleportación en Lineal no tipado.
%\end{ejercicio}
%
%
%\section{Tipando superposiciones y mediciones proyectivas}\label{sec:Qmeas}
%En esta sección veremos un trabajo muy
%reciente~\citep{DiazcaroDowekTPNC17,Rinaldi18}.
%
%El principal objetivo es agregar medición a Lineal, el cálculo presentado en la
%Sección~\ref{sec:Lineal}. Como se mencionó anteriormente, para evitar el clonado
%se conocen dos técnicas:
%\begin{itemize}
%\item[(LL)] Usar términos lineales en el sentido de la lógica lineal, y entonces
%  $\lambda x.x\otimes x$ es un término mal formado.
%\item[(AL)] Usar un sistema de reescritura que defina las aplicaciones como
%  aplicaciones lineales, así $\lambda x.x\otimes x$ es permitido, pero al
%  aplicarlo a $\alpha.\ket 0+\beta.\ket 1$ producirá
%  $\alpha.\ket{00}+\beta.\ket{11}$ y no $(\alpha.\ket 0+\beta.\ket
%  1)\otimes(\alpha.\ket 0+\beta.\ket 1)$.
%\end{itemize}
%Sin embargo, definir aplicaciones lineales por medio de reescritura no funciona
%si el cálculo tiene medición, ya que sólo las compuertas cuánticas se comportan
%de esa manera. Por ejemplo, digamos que tenemos un operador de medición notado
%por $\pi$, entonces
%\[
%  (\lambda x.\pi x)(\alpha.\ket 0+\beta.\ket 1) \to^*\alpha.(\lambda x.\pi
%  x)\ket 0+\beta.(\lambda x.\pi x)\ket 1 \to^*\alpha.\ket 0+\beta.\ket 1
%\]
%lo cual claramente es un error.
%
%En este cálculo se propone usar una combinación de las dos técnicas, LL y AL, de
%esa manera, una abstracción podrá tomar una superposición, pero sólo en el caso
%de que la trate linealmente, en el sentido de LL, en otro caso, la aplicación
%sólo podrá comportarse en el sentido de AL.
%
%Claro que para eso, debemos distinguir términos superpuestos de términos que no
%lo están.
%
%\subsubsection*{Gramáticas}
%La gramática de tipos se separa en dos niveles ya que este cálculo es de primer
%orden (por razones que luego discutiremos).
%\begin{align*}
%      \Psi & := \B^n\mid S(\Psi)\mid \Psi\times\Psi & \textrm{Tipos qubit (\qtypes)} \\
%      A & := \Psi\mid \Psi\Rightarrow A\mid S(A)\mid A\times A & \textrm{Tipos generales (\types)} \\                                                    
%\end{align*}
%La gramática de términos es la siguiente:
%\begin{align*}
%      b  & := x\mid \lambda x{:}\Psi.t\mid \ket 0\mid \ket 1\mid b\times b & \textrm{T\'erminos de base (\tbasis)} \\
%      v  & := b\mid \pair vv\mid \z\mid \alpha.v\mid v\times v & \textrm{Valores (\values)} \\
%      t  & := v\mid tt\mid \pair tt\mid \pi_j t\mid \ite{}tt\mid \alpha.t\mid t\times t\mid \head~t\mid \tail~t\mid \Uparrow_r t\mid \Uparrow_\ell t & \textrm{T\'erminos ($\Lambda$)}
%\end{align*}
%con $\alpha\in\mathbb C$.
%
%Utilizamos la notación $\ifz trs$ para $(\ifz{}rs)t$. El motivo de
%considerar a $\ifz{}rs$ como una función es aprovechar la linealidad AL de las
%funciones, de esa manera,
%\[
%  \ifz{(\alpha.\ket 0+\beta.\ket 1)}rs \to^* \alpha.\ifz{\ket
%    0}rs+\beta.\ifz{\ket 1}rs
%\]
%Dentro de la gramática de términos, distinguimos dos subgramáticas, la de los
%términos de base,
%y la de los valores, que son combinaciones lineales de términos de base.
%
%\subsubsection*{Tipos}
%Dado que la semántica operacional de una aplicación será diferente si el
%argumento es una superposición o no, y esa información la sabremos utilizando
%los tipos, debemos dar primero los tipos, y luego la semántica operacional
%dependiente de ellos.
%
%La lógica de los tipos es que marcamos con una $S$ a las superposiciones, de la
%misma manera que en lógica lineal se marca con $!$ a los términos que pueden ser
%duplicados. En realidad, nuestras superposiciones son exactamente los términos
%que no pueden ser copiados, y por ese motivo no utilizamos notación de lógica
%lineal.
%
%Naturalmente existe un subtipado: si un término no es una superposición, es
%decir, puede ser copiado, también puede ser tratado como una superposición y por
%lo tanto no ser copiado. Es decir, $\Q\preceq S(\Q)$.
%
%Formalmente, la relación $\preceq$ es un preorden definido por
%    \[
%      \infer{A\preceq S(A)}{}
%      \qquad
%      \infer{S(S(A))\preceq S(A)}{}
%    \]
%    \[
%      \infer{\Psi\Rightarrow A\preceq\Psi\Rightarrow B}{A\preceq B}
%      \qquad
%      \infer{S(A)\preceq S(B)}{A\preceq B}
%      \qquad
%      \infer{A\times C\preceq B\times C}{A\preceq B}
%      \qquad
%      \infer{C\times A\preceq C\times B}{A\preceq B}
%    \]
%\begin{observacion}
%  Notar que con esta definición, $S(S(A))\equiv S(A)$.
%\end{observacion}
%
%El sistema de tipos se define a continuación:
%    \[
%      \infer[^\tax] {x:\Psi\vdash x:\Psi} {}
%      \qquad
%      \infer[^{\tax_{\vec 0}}] {\vdash \z:S(A)} {}
%      \qquad
%      \infer[^{\tax_{\ket 0}}] {\vdash\ket 0:\B} {}
%      \quad
%      \infer[^{\tax_{\ket 1}}] {\vdash\ket 1:\B} {}
%    \]
%    \[
%      \infer[^{S_I^\alpha}] {\Gamma\vdash \alpha.t:S(A)} {\Gamma\vdash t:A}
%      \qquad
%      \infer[^{S_I^+}] {\Gamma,\Delta\vdash\pair tu:S(A)} {\Gamma\vdash t:A & \Delta\vdash u:A}
%      \qquad
%      \infer[^{S_E}] {\Gamma\vdash\pi_j t:\B^j\times S(\B^{n-j})} {\Gamma\vdash t:S(\B^n)}
%    \]
%    \[
%      \infer[^\preceq] {\Gamma\vdash t:B} {\Gamma\vdash t:A \ {\scriptstyle (A\preceq B)}}
%      \qquad
%      \infer[^\tif]{\Gamma\vdash\ite{}tr:\B\Rightarrow A}{\Gamma\vdash t:A &
%      \Gamma\vdash r:A}
%      \qquad
%      \infer[^{\Rightarrow_I}] {\Gamma\vdash\lambda x{:}\Psi.t:\Psi\Rightarrow A} {\Gamma,x:\Psi\vdash t:A}
%    \]
%    \[
%      \infer[^{\Rightarrow_E}] {\Delta,\Gamma\vdash tu:A}
%      {
%	\Delta\vdash u:\Psi
%	&
%	\Gamma\vdash t:\Psi\Rightarrow A
%      }
%      \qquad
%      \infer[^{\Rightarrow_{ES}}] {\Delta,\Gamma\vdash tu:S(A)}
%      {
%	\Delta\vdash u:S(\Psi)
%	&
%	\Gamma\vdash t:S(\Psi\Rightarrow A)
%      }
%    \]
%    \[
%      \infer[^W] {\Gamma,x:\B^n\vdash t:A} {\Gamma\vdash t:A}
%      \qquad
%      \infer[^C] {\Gamma,x:\B^n\vdash (x/y)t:A} {\Gamma,x:\B^n,y:\B^n\vdash t:A}
%    \]
%    \[
%      \infer[^{\times_I}] {\Gamma,\Delta\vdash t\times u:A\times B} {\Gamma\vdash t:A & \Delta\vdash u:B}
%      \qquad
%      \infer[^{\times_{Er}}] {\Gamma\vdash \head~t:\B} {\Gamma\vdash t:\B^n &
%      {\scriptstyle n>1}}
%      \qquad
%      \infer[^{\times_{El}}] {\Gamma\vdash \tail~t:\B^{n-1}} {\Gamma\vdash t:\B^n &
%      {\scriptstyle n>1}}
%    \]
%    \[
%      \infer[^{\Uparrow_r}] {\Gamma\vdash \Uparrow_rt:S(A\times B)} {\Gamma\vdash t:S(S(A)\times B)}
%      \qquad
%      \infer[^{\Uparrow_\ell}] {\Gamma\vdash \Uparrow_\ell t:S(A\times B)} {\Gamma\vdash t:S(A\times S(B))}
%    \]
%
%\subsubsection*{Semántica operacional}
%  \[
%    \begin{array}{c|r@{\ }lr}\hline
%      \multirow{2}{*}{\titulo{12mm}{Beta}}
%      & \textrm{If $b$ has type $\B^n$ and $b\in\tbasis$, } (\lambda x{:}{\B^n}.t)b &\lra (b/x)t & \rbetab\\
%      & \textrm{If $u$ has type $S(\Psi)$, } (\lambda x{:}{S(\Psi)}.t)u &\lra (u/x)t & \rbetan\\\hline
%      \multirow{2}{*}{\titulo{12mm}{If}}
%      &\ite{\ket 1}tr &\lra t &\riftrue\\
%      &\ite{\ket 0}tr &\lra r &\riffalse\\\hline
%      \multirow{6}{*}{\titulo{25.2mm}{Distribuciones lineales}}
%      &\textrm{If $t$ has type $\B^n\Rightarrow A$, } t\pair uv &\lra \pair{tu}{tv} & \rlinr\\
%      &\textrm{If $t$ has type $\B^n\Rightarrow A$, } (\alpha.u) &\lra\alpha.tu & \rlinscalr\\
%      &\textrm{If $t$ has type $\B^n\Rightarrow A$, } t\z[\B^n] &\lra\z &\rlinzr\\
%      &\pair tuv &\lra\pair{tv}{uv} & \rlinl\\
%      &(\alpha.t)u &\lra\alpha.tu &\rlinscall\\
%      &\z[\B^n\Rightarrow A]t &\lra\z[A] &\rlinzl\\\hline
%      \multirow{10}{*}{\titulo{47.8mm}{Axiomas de espacios vectoriales}}
%      &\pair\z t &\lra t &\rneut\\
%      &1.t &\lra t &\runit\\
%      &\textrm{If $t$ has type $A$, }0.t &\lra\z &\rzeros\\
%      &\alpha.\z &\lra\z &\rzero\\
%      &\alpha.(\beta.t) &\lra (\alpha\beta).t &\rprod\\
%      &\alpha.\pair tu &\lra\pair{\alpha.t}{\alpha.u} &\rdists\\
%      &\pair{\alpha.t}{\beta.t} &\lra(\alpha+\beta).t &\rfact\\
%      &\pair{\alpha.t}t &\lra (\alpha+1).t &\rfacto\\
%      &\pair tt &\lra 2.t &\rfactt\\
%      &\z[S(A)] &\lra\z & \rzeroS \\\hline
%      \multirow{2}{*}{\titulo{8.4mm}{Listas}}
%      &\textrm{If $h\neq u\times v$ and $h\in\tbasis$, }\head\ h\times t &\lra h & \rehead\\
%      &\textrm{If $h\neq u\times v$ and $h\in\tbasis$, }\tail\ h\times t &\lra t & \rtail\\\hline
%      \multirow{10}{*}{\titulo{62mm}{\mbox{\hspace{1.3cm}Casteo}}}
%      & \Uparrow_r \pair rs\times u &\lra\pair{\Uparrow_r r\times u}{\Uparrow_r s\times u} &\rdistsumr\\
%      &\Uparrow_\ell u\times\pair rs &\lra\pair{\Uparrow_\ell u\times r}{\Uparrow_\ell u\times s} &\rdistsuml\\
%      &\Uparrow_r (\alpha.r)\times u &\lra \alpha.\Uparrow_r r\times u &\rdistscalr\\
%      &\Uparrow_\ell u\times(\alpha.r) &\lra \alpha.\Uparrow_r u\times r &\rdistscall\\
%      &\textrm{If $u$ has type $B$, }\Uparrow_r \z\times u &\lra\z[A\times B] &\rdistzr\\
%      &\textrm{If $u$ has type $A$, }\Uparrow_\ell u\times\z[B] &\lra\z[A\times B] &\rdistzl\\
%      &\Uparrow\pair tu&\lra\pair{\Uparrow t}{\Uparrow u} &\rdistcasum\\
%      &\Uparrow(\alpha.t)&\lra\alpha.\Uparrow t &\rdistcascal\\
%      &\textrm{If $u\in\tbasis$, }\Uparrow_r u\times v&\lra u\times v &\rcaneutr\\
%      &\textrm{If $v\in\tbasis$, }\Uparrow_\ell u\times v&\lra u\times v &\rcaneutl \\\hline
%      \multirow{1}{*}{\titulo{5mm}{Proy.}}
%      &\multicolumn{2}{c}{
%	\pi_j(\sum\limits_{i=1}^n\may[\alpha_i]\prod\limits_{h=1}^m\ket{b_{hi}})
%	\lra
%	\bigparallel\limits_{k=0}^{2^j-1} p_k (\ket k\times\ket{\phi_k})
%      }
%      & \rproj\\\hline
%      \multirow{2}{*}{\titulo{25mm}{Reglas contextuales}}
%      &\multicolumn{3}{l}{\textrm{ If $t\lra u$, then}}\\
%      &\multicolumn{3}{c}{ 
%	\begin{array}{c@{}cc}
%	  tv \lra uv & (\lambda x^{\B^n}.v)t\lra(\lambda x^{\B^n}.v)u & \pair tv\lra\pair uv \\
%	  \alpha.t\lra\alpha.u& \pi_j t\lra\pi_j u& t\times v\lra u\times v \\
%	  v\times t\lra v\times u& \Uparrow_r t\lra\Uparrow_r u& \Uparrow_\ell t\lra\Uparrow_\ell u \\
%	  \head\ t\lra\head\ u& \tail\ t\lra\tail\ u& \ite trs\lra\ite urs \\\hline
%	\end{array}
%      }
%    \end{array}
%  \]
%donde, en la regla \rproj, se tiene:
%\begin{align*}
%  & j\leq m\\
%  &\ket k=\ket{b_1}\times\dots\times\ket{b_j}\textrm{ donde }b_1\dots b_j\textrm{ es la representación binaria de }k\\
%  & \ket{\phi_k} = \sum_{i\in T_k}\left(\frac{\alpha_i}{\sqrt{\sum_{r\in T_k}|\alpha_r|^2}}\right)\prod_{h=j+1}^m \ket{b_{hi}}\\
%  &p_k=\sum_{i\in T_k}\left(\frac{|\alpha_i|^2}{\sum_{r=1}^n|\alpha_r|^2}\right)\\
%  & T_k=\{i\leq n\mid\ket{b_{1i}}\times\dots\times\ket{b_{ji}}=\ket k\}
%\end{align*}
%
%\paragraph{Primer orden.} El motivo de utilizar primer orden es que en este
%cálculo hemos mezclado los dos enfoques precedentes: LL y AL, y por lo tanto
%ahora es posible construir una máquina de clonado si se permite alto orden. El
%truco es esconder dentro de una abstracción una superposición, por ejemplo
%$\lambda x{:}\Q.\alpha.\ket 0+\beta.\ket 1$. Éste es un término duplicable, y no
%hay problema en ello (no es una superposición, es un programa que produce una).
%Sin embargo, dado que ahora tenemos términos LL, podríamos también producir
%$\lambda y{:}S(\Q).\lambda x{:}\Q.y$, el cual nos permite generar superposiciones
%duplicables. La solución es impedir tomar una abstracción como argumento, y por
%lo tanto este término no podrá ser duplicado.
%\subsubsection*{Multiple qubits: casteo}
%Consideremos el siguiente ejemplo:
%\begin{equation}
%  \ket 0\times(\ket 0+\ket 1)\to\ket 0\times\ket 0+\ket 0\times\ket 1
%  \label{eq:red}
%\end{equation}
%El primer término podría ser tipado con $\Q\otimes S(\Q)$, en cambio el segundo
%debería ser tipado con $S(\Q\otimes\Q)$. Naturalmente el subipado va en sentido
%contrario al necesario: $\Q\otimes S(\Q)\preceq S(\Q\otimes\Q)$, y por lo tanto
%este ejemplo rompe la propiedad de preservación de tipos.
%
%Es normal, en matemática, que al desarrollar un término perdamos información.
%Por ejemplo, $(x-1)(x-2) = x^2-3x+2$. La información del término izquierdo, que
%da sus raíces y una factorización, es perdida al desplegar el término. Por este
%motivo, no permitimos la reducción \eqref{eq:red}. En cambio, para poder
%realizar esa reducción, se debe castear el término, y entonces el tipo es
%preservado:
%\[
%  \Uparrow_\ell\ket 0\times(\ket 0+\ket 1)
%  \to\ket 0\times\ket 0+\ket 0\times\ket 1
%\]
%
%\newcommand\citaSR{\citep[Teorema~2]{DiazcaroDowekTPNC17}}
%\newcommand\citaSN{\citep[Teorema~5.16]{Rinaldi18}}
%\begin{teorema}[Preservación de tipos en términos cerrados, \citaSR]
%  Si $t\lra[p_i] u_i$
%  and $\vdash t:A$, entonces $\vdash u_i:A$.
%  \qed
%\end{teorema}
%\begin{teorema}[Normalización fuerte, \citaSN]
%  Si $\vdash t:A$ entonces $t$ es fuertemente normalizante.
%  \qed
%\end{teorema}
%
%\subsubsection*{Algoritmo de Deutsch}\label{ex:deut}
%La compuerta Hadamard puede ser implementada de la siguiente manera:
%\[
%  \s H =\lambda x{:}\Q.\nicefrac 1{\sqrt 2}.\pair{\ket 0}{(\ifz x{(-\ket 1)}{\ket
%      1})}
%\]
%
%Notar que la variable es un tipo de base, y por lo tanto, si $H$ se aplica a una
%superposición, por ejemplo $\pair{\alpha.\ket 0}{\beta.\ket 1}$, reduce de la
%siguiente manera:
%\[
%  \s H\pair{\alpha.\ket 0}{\beta.\ket 1} \red\rlinr \pair{\s H\alpha.\ket 0}{\s
%    H\beta.\ket 1} \red{\rlinscalr^2} \pair{\alpha.\s H\ket 0}{\beta.\s H\ket 1}
%\]
%y por lo tanto, finalmente se aplica a términos de base.
%
%Definimos $\s H_1$ como la función que toma dos qubits y aplica $H$ al primero:
%\[
%  \s H_1 =\lambda x{:}\Q\times\Q~((\s H~(\head~x))\times(\tail~x))
%\]
%
%Similarmente, $\s H_{\textsl{both}}$ aplica $\s H$ a ambos qubits:
%\[
%  \s H_{\textsl{both}} = \lambda x{:}\Q\times\Q~((\s H~(\head~x))\times(\s
%  H~(\tail~x)))
%\]
%
%El oráculo $U_f$ está definido por:
%\[
%  U_f\ket{xy} = \ket{x,y\oplus f(x)}
%\]
%donde $\oplus$ es la suma modulo $2$. Para implementarlo, necesitamos la
%compuerta $\mathit{not}$, la que puede ser implementada como sigue:
%\[
%  \s{not}=\lambda x{:}\Q~(\ifz x{\ket 0}{\ket 1})
%\]
%Entonces, $U_f$ es:
%\[
%  \s U_f = \lambda x{:}\Q\times\Q~
%  ((\head~x)\times(\ifz{(\tail~x)}{(\s{not}~(f~(\head~x)))}{(f~(\head~x))}))
%\]
%donde $f$ es una función dada de tipo $\Q\Rightarrow\Q$.
%
%Finalmente, el algoritmo de Deutsch combina todas las definiciones previas:
%\[
%  \s{Deutsch}_f = \pi_1~(\Uparrow_r  \s H_1~(\s
%  U_f\Uparrow_\ell\Uparrow_r \s H_{\textsl{both}}~(\ket 0\times\ket 1)))
%\]
%
%Los casteos luego de las compuertas Hadamard se necesitan para desarrollar el
%término por completo para luego poder pasarlo a una abstracción que espera
%términos de base.
%
%El término $\s{Deutsch}_f$ se tipa como sigue:
%\[
%  \vdash \s{Deutsch}_f:\Q\times S(\Q)
%\]
%
%Este término, en la función identidad, reduce así:
%\[
%  \s{Deutsch}_{id} \lra^* \pi_1\npair{\nicefrac 1{\sqrt 2}.\ket{10}}{\nicefrac
%    1{\sqrt 2}.\ket{11}} \red{\rproj} \ket 1\times\npair{\nicefrac 1{\sqrt
%      2}.\ket{0}}{\nicefrac 1{\sqrt 2}.\ket{1}}
%\]

%\paragraph{Algoritmo de teleportación}\label{ex:telep}
%En el ejemplo precedente la aplicación de la medición tenía sólo un resultado
%posible: el primer qubit ya estaba en un estado de base antes de medirlo. Por lo
%tanto, introducimos un ejemplo un poco más complejo, la teleportación, donde la
%medición es usada como un operador que cambia el estado.
%
%La compuerta $\mathit{cnot}$ la implementamos como sigue:
%\[
%  \s{cnot} = \lambda
%  x:\Q\times\Q~((\head~x)\times(\ifz{(\head~x)}{(\s{not}~(\tail~x))}{(\tail~x)}))
%\]
%
%Definimos $\s H^3_1$ que aplica $H$ al primer qubit en un sistema de tres
%qubits:
%\[
%  \s H^3_1 = \lambda x{:}\Q\times\Q\times\Q~ ((\s H~(\head~x))\times(\tail~x))
%\]
%
%Notar que la única diferencia con $\s H_1$ es el tipo de la variable. También,
%necesitamos aplicar $cnot$ a los dos primeros qubits de un sistema de tres
%qubits, por lo que definimos $\s{cnot}^3_{12}$:
%\[
%  \lambda x{:}\Q\times\Q\times\Q~
%  ((\s{cnot}~(\head~x\times(\head~\tail~x)))\times (\tail~\tail~x))
%\]
%
%La compuerta $Z$ se implementa de manera similar a $\s{not}$:
%\[
%  \s Z = \lambda x{:}\Q~(\ifz x{(-\ket 1)}{\ket 0})
%\]
%
%A la parte de Alice la definimos así:
%\[
%  \s{Alice} = \lambda x{:}S(\Q)\times S(\Q\times
%  \Q)~(\pi_2(\Uparrow_r\s
%  H^3_1~(\s{cnot}^3_{12}(\Uparrow_\ell\Uparrow_r x))))
%\]
%Notar que antes de pasar a $\s{cnot}_{12}^3$ el parámetro de tipo \(
%S(\Q)\times S(\Q\times\Q), \) necesitamos desarrollar el término por completo
%usando dos casteos, y de nuevo luego de la compuerta Hadamard.
%
%El lado de Bob del algoritmo aplicará ciertas compuertas basado en los bits que
%reciba de Alice. Por lo tanto, para cualquier $\vdash \s U:\Q\Rightarrow S(\Q)$
%o $\vdash \s U:\Q\Rightarrow\Q$, definimos $\s U^{(b)}$ como la función que
%aplica $\s U$ o no dependiendo del bit $b$:
%\[
%  \s U^{(b)} =(\lambda x{:}\Q~\lambda y{:}\Q~(\ifz x{\s Uy}y))\ b
%\]
%
%Bob se implementa como sigue:
%\[
%  \s{Bob} = \lambda x{:}\Q\times\Q\times\Q~(\s Z^{(\head~x)}(\s{not}^{(\head\
%    \tail\ x)}~(\tail~\tail~x)))
%\]
%
%El estado de Bell lo definimos directamente:
%\[
%  \beta_{00} = \pair{\nicefrac 1{\sqrt 2}.\ket{00}}{\nicefrac 1{\sqrt
%      2}.\ket{11}}
%\]
%
%Finalmente, la teleportación se define por:
%\[
%  \s{Telep} = \lambda q{:}S(\Q)~(\s{Bob}(\Uparrow_\ell \s{Alice}~(q\times\beta_{00})))
%\]
%Este tipo tiene el tipo esperado $S(\Q)\Rightarrow S(\Q)$, y aplicado a
%cualquier superposición \( \pair{\alpha.\ket 0}{\beta.\ket 1} \) reduce, como es
%de esperarse, a $\pair{\alpha.\ket 0}{\beta.\ket 1}$.
%

%\chapter{Un nuevo conectivo de Deducción Natural}

\renewcommand\bibname{Bibliografía citada}
\bibliographystyle{hispanat} \bibliography{biblioES}
\end{document}